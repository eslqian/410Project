{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 410 Final Project: **Study Guide Helper**\n",
        "\n",
        "### **Project Overview**\n",
        "\n",
        "The **Study Guide Helper** is a project designed to transform the study process through state-of-the-art technology. It integrates three key elements:\n",
        "\n",
        "1. **Retrieval System**  \n",
        "   A cornerstone of this project is its advanced retrieval system. This system is uniquely configured to incorporate a comprehensive corpus of information, which includes every lecture transcript and each chapter of the course textbook, converted into text files. By integrating these diverse and rich resources, the system is exceptionally equipped to identify and present the most relevant documents in response to specific user queries. This approach ensures that the information retrieved is not only pertinent but also encompasses a broad spectrum of educational materials, facilitating focused and effective learning.\n",
        "\n",
        "2. **Generative AI (Powered by OpenAI APIs)**  \n",
        "   Utilizing OpenAI's APIs, this segment of the project generates coherent and contextually accurate answers based on the documents retrieved. The use of OpenAI's advanced AI technology guarantees that the responses are relevant, reliable, and grounded in the substantial database of lecture transcripts and textbook content, thereby elevating the quality and accuracy of the information provided.\n",
        "\n",
        "3. **Interactive User Interface**  \n",
        "   The user interface is the heart of the Study Guide Helper. It is crafted to be engaging and user-friendly, enabling users to seamlessly pose questions and receive AI-generated answers. This interactive platform is the gateway to the sophisticated capabilities of both the Retrieval System and Generative AI, making it a dynamic and accessible educational tool.\n",
        "\n",
        "### **Project Goals**\n",
        "\n",
        "- To deliver efficient and targeted learning by providing access to a rich database of lecture transcripts and textbook content.\n",
        "- To enhance understanding and retention through AI-generated answers, leveraging the comprehensive corpus of educational materials and the power of OpenAI's APIs.\n",
        "- To provide an intuitive and interactive tool that caters to the diverse needs of learners and educators."
      ],
      "metadata": {
        "id": "xY7Y-gIBFluw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Conversion of Textbook PDF into Chapter-wise Text Files\n",
        "\n",
        "This initial step involves the meticulous conversion of the textbook from its PDF format into individual text files, each corresponding to a separate chapter. This process is designed to ensure that each chapter is distinctly segmented, facilitating ease of access and reference in the subsequent stages of the project.\n"
      ],
      "metadata": {
        "id": "5N7fUYkYKKOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pdfplumber\n",
        "%pip install pytesseract\n",
        "%pip install PyPDF2\n",
        "%pip install openai\n",
        "import pdfplumber\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import PyPDF2\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "RTm5LiwxNErN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4741eaac-5aa1-4c45-a311-7bded8ebb058"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.10.3)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20221105)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.25.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIeXwSpXU5g_",
        "outputId": "c892c2c7-2543-439f-acce-6dfe0d5e8035"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_spaces(text):\n",
        "    # Pattern to identify places where a lowercase letter is followed by an uppercase letter\n",
        "    pattern = re.compile(r'(?<=[a-z])(?=[A-Z])')\n",
        "\n",
        "    # Insert a space at each identified position\n",
        "    return pattern.sub(' ', text)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "            # Check for encryption and try to decrypt\n",
        "            if reader.is_encrypted:\n",
        "                try:\n",
        "                    reader.decrypt('')\n",
        "                except Exception as e:\n",
        "                    print(f\"Unable to decrypt PDF: {e}\")\n",
        "                    return None\n",
        "\n",
        "            text = ''\n",
        "\n",
        "            # Extract text from each page\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    # Add logic here if you need to clean or format the text\n",
        "                    text += page_text + '\\n'\n",
        "            return text\n",
        "    except FileNotFoundError:\n",
        "        print(\"File not found. Please check the file path.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def chunk_text_by_chapter_and_save(text, chapter_titles, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    current_chapter = None\n",
        "    chapter_contents = []\n",
        "\n",
        "    for line in text.split('\\n'):\n",
        "        # Check if the line matches or partially matches any chapter title\n",
        "        for title in chapter_titles:\n",
        "            if title.startswith(line) or line.startswith(title):\n",
        "                if current_chapter:\n",
        "                    save_chapter_to_file(current_chapter, '\\n'.join(chapter_contents), output_folder)\n",
        "                current_chapter = title\n",
        "                chapter_contents = []\n",
        "                break\n",
        "        else:  # This else corresponds to the for-loop\n",
        "            chapter_contents.append(line)\n",
        "\n",
        "    if current_chapter:\n",
        "        save_chapter_to_file(current_chapter, '\\n'.join(chapter_contents), output_folder)\n",
        "\n",
        "def save_chapter_to_file(chapter_title, content, output_folder):\n",
        "    filename = f\"{chapter_title}.txt\".replace(' ', '_').replace('/', '_')\n",
        "    file_path = os.path.join(output_folder, filename)\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n"
      ],
      "metadata": {
        "id": "6OoPRIoYJS_D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_titles = [\n",
        "    \"1Introduction\",\n",
        "    \"2Background\",\n",
        "    \"3Text Data Understanding\",\n",
        "    \"4META: A Unified Toolkit\",\n",
        "    \"5Overview of Text\",\n",
        "    \"6Retrieval Models\",\n",
        "    \"7Feedback\",\n",
        "    \"8Search Engine\",\n",
        "    \"9Search Engine Evaluation\",\n",
        "    \"10Web Search\",\n",
        "    \"11Recommender Systems\",\n",
        "    \"12Overview of Text\",\n",
        "    \"13Word Association Mining\",\n",
        "    \"14Text Clustering\",\n",
        "    \"15Text Categorization\",\n",
        "    \"16Text Summarization\",\n",
        "    \"17Topic Analysis\",\n",
        "    \"18Opinion Mining and\",\n",
        "    \"19Joint Analysis of Text\",\n",
        "    \"20Toward A Unified System for Text Management and Analysis\"\n",
        "]\n",
        "\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/CS_410/textbook_410.pdf'\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if text:\n",
        "    chunk_text_by_chapter_and_save(text, chapter_titles, \"files/\")"
      ],
      "metadata": {
        "id": "gCOhTiltMB8Q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: File Verification and Inventory\n",
        "\n",
        "This step is crucial in ensuring the completeness and readiness of our resources. It involves a thorough verification process to confirm the presence of all necessary files. The key components to be verified are:\n",
        "\n",
        "- **Overview Files:** These files should contain guiding questions and key concepts. It's essential to ensure that each overview file is complete and accurately reflects the course material.\n",
        "\n",
        "- **Lecture Transcripts:** We need to have transcripts for each week's lecture. This step includes checking that each transcript is available, legible, and correctly corresponds to the respective week's content.\n",
        "\n",
        "- **Textbook Chapters:** Different chapters from our textbook should be available as individual text files. The verification process here involves confirming that each chapter is properly extracted, correctly labeled, and includes all the relevant content.\n",
        "\n",
        "This comprehensive verification ensures that all the critical educational resources are in place and correctly organized for the subsequent stages of our project.\n"
      ],
      "metadata": {
        "id": "Xd4AsiEXQYmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "# Function to extract content between two headings\n",
        "def extract_content(heading, text, next_heading=None):\n",
        "    if next_heading:\n",
        "        pattern = re.compile(rf\"{heading}\\n(.*?)\\n{next_heading}\", re.DOTALL)\n",
        "    else:\n",
        "        pattern = re.compile(rf\"{heading}\\n(.*?)(?=\\n[A-Z][a-z])\", re.DOTALL)\n",
        "    match = pattern.search(text)\n",
        "    content = match.group(1).strip() if match else \"\"\n",
        "    return [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "\n",
        "# Function to process overview files in a specified directory\n",
        "def process_overviews(directory):\n",
        "    files = sorted(os.listdir(directory))\n",
        "    for file in files:\n",
        "        file_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            goals_and_objectives = extract_content(\"Goals and Objectives\", text, \"Guiding Questions\")\n",
        "            guiding_questions = extract_content(\"Guiding Questions\", text, \"Key Phrases and Concepts\")\n",
        "            key_phrases_and_concepts = extract_content(\"Key Phrases and Concepts\", text)\n",
        "\n",
        "            print(f\"File: {file}\")\n",
        "            print(f\"Length: {len(text)} characters\")\n",
        "            print(\"Goals and Objectives:\")\n",
        "            for item in goals_and_objectives:\n",
        "                print(\"-\", item)\n",
        "            print(\"\\nGuiding Questions:\")\n",
        "            for item in guiding_questions:\n",
        "                print(\"-\", item)\n",
        "            print(\"\\nKey Phrases and Concepts:\")\n",
        "            for item in key_phrases_and_concepts:\n",
        "                print(\"-\", item)\n",
        "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Function to list textbook chapters or lecture transcripts\n",
        "def list_files(directory, title):\n",
        "    files = sorted(os.listdir(directory))\n",
        "    print(f\"{title}:\")\n",
        "    for file in files:\n",
        "        file_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "            print(f\"{file} - Length: {len(text)} characters\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Function to process lecture transcripts in weekly folders\n",
        "def process_lecture_transcripts(directory):\n",
        "    for week in sorted(os.listdir(directory)):\n",
        "        week_path = os.path.join(directory, week)\n",
        "        if os.path.isdir(week_path):\n",
        "            print(f\"Processing {week} Transcripts:\")\n",
        "            list_files(week_path, f\"{week} Transcripts\")\n",
        "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Directory paths\n",
        "overview_directory = \"/content/drive/MyDrive/CS_410/Text Files/Overview\"\n",
        "chapter_directory = \"/content/drive/MyDrive/CS_410/Text Files/Textbook\"\n",
        "lecture_directory = \"/content/drive/MyDrive/CS_410/Text Files/Lectures\"\n",
        "\n",
        "# Check if directories exist and process files\n",
        "if os.path.exists(overview_directory):\n",
        "    process_overviews(overview_directory)\n",
        "else:\n",
        "    print(f\"The overview directory {overview_directory} does not exist.\")\n",
        "\n",
        "if os.path.exists(chapter_directory):\n",
        "    list_files(chapter_directory, \"Textbook Chapters\")\n",
        "else:\n",
        "    print(f\"The textbook chapters directory {chapter_directory} does not exist.\")\n",
        "\n",
        "if os.path.exists(lecture_directory):\n",
        "    process_lecture_transcripts(lecture_directory)\n",
        "else:\n",
        "    print(f\"The lecture transcripts directory {lecture_directory} does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Xt8b6WSdDN",
        "outputId": "ee6d6ef5-931d-40f9-a9a3-e1e3165046bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: Week1.txt\n",
            "Length: 2000 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain some basic concepts in natural language processing, text information access.\n",
            "- Explain why text retrieval is often defined as a ranking problem.\n",
            "- Explain the basic idea of the vector space retrieval model and how to instantiate it with the simplest bit-vector representation.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "- What does a computer have to do in order to understand a natural language sentence?\n",
            "- What is ambiguity?\n",
            "- Why is natural language processing (NLP) difficult for computers?\n",
            "- What is bag-of-words representation? Why do modern search engines use this simple representation of text?\n",
            "- What are the two modes of text information access? Which mode does a web search engine such as Google support?\n",
            "- When is browsing more useful than querying to help a user find relevant information?\n",
            "- Why is a text retrieval task defined as a ranking task?\n",
            "- What is a retrieval model?\n",
            "- What are the two assumptions made by the Probability Ranking Principle?\n",
            "- What is the Vector Space Retrieval Model? How does it work?\n",
            "- How do we define the dimensions of the Vector Space Model? What does “bag of words” representation mean?\n",
            "- What does the retrieval function intuitively capture when we instantiate a vector space model with bag of words representation and bit representation for documents and queries?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week10.txt\n",
            "Length: 1753 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain the concept of text clustering and why it is useful.\n",
            "- Explain how Hierarchical Agglomerative Clustering and k-Means clustering work.\n",
            "- Explain how to evaluate text clustering.\n",
            "- Explain the concept of text categorization and why it is useful.\n",
            "- Explain how Naïve Bayes classifier works.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "- What is clustering? What are some applications of clustering in text mining and analysis?\n",
            "- How does hierarchical agglomerative clustering work? How do single-link, complete-link, and average-link work for computing group similarity? Which of these three ways of computing group similarity is least sensitive to outliers in the data?\n",
            "- How do we evaluate clustering results?\n",
            "- What is text categorization? What are some applications of text categorization?\n",
            "- What does the training data for categorization look like?\n",
            "- How does the Naïve Bayes classifier work?\n",
            "- Why do we often use logarithm in the scoring function for Naïve Bayes?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week11.txt\n",
            "Length: 2652 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain the basic ideas of Logistic Regression, K-Nearest Neighbors (k-NN), and how K-NN works.\n",
            "- Explain how to evaluate categorization results.\n",
            "- Explain the tasks of opinion mining and sentiment analysis and why they are important tasks from an application perspective.\n",
            "- Explain how sentiment analysis can be done using text categorization techniques and why a straightforward application of regular text categorization techniques may not be adequate.\n",
            "- Give examples of both simple and complex features that are used for characterizing text data and explain how NLP can enable complex features to be generated from text.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "- What’s the general idea of the logistic regression classifier? How is it related to Naïve Bayes? Under what conditions would logistic regression cover Naïve Bayes as a special case for two-category categorization?\n",
            "- What’s the general idea of the k-Nearest Neighbor classifier? How does it work?\n",
            "- How do we evaluate categorization results?\n",
            "- How do we compute classification accuracy, precision, recall, and F score?\n",
            "- Why is harmonic mean as used in F better than the arithmetic mean of precision and recall?\n",
            "- What’s the difference between macro and micro averaging?\n",
            "- Why is it sometimes interesting to frame a categorization problem as a ranking problem?\n",
            "- What is an opinion? How is it different from a factual statement?\n",
            "- What’s an opinion holder? What’s an opinion target?\n",
            "- What’s the goal of opinion mining?\n",
            "- What is sentiment analysis? How is it similar to and different from a text categorization task such as topic categorization?\n",
            "- Why are unigram features generally insufficient for accurate sentiment classification?\n",
            "- What’s the concern of using too many complex features such as frequent substructures of parse trees?\n",
            "- What are some commonly used features to represent text data?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week12.txt\n",
            "Length: 2840 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain why it is necessary and useful to perform joint analysis and mining for text and non-text data.\n",
            "- Explain the general idea of Contextual Probabilistic Latent Semantic Analysis (CPLSA) and the main difference between CPLSA and PLSA.\n",
            "- Give multiple application examples of CPLSA for contextual text mining.\n",
            "- Explain the general idea of using the social network of authors as context to analyze topics in text data and its potential benefit from an application perspective.\n",
            "- Explain how a time series (such as stock prices) can be used as context to analyze topics in text data that have time stamps using topic models\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "- Why is text-based prediction interesting from an application perspective? Why are humans playing an important role in text-based prediction? What is the “data mining loop”?\n",
            "- Why is it necessary and useful to jointly mine and analyze text and non-text data? How can non-text data potentially help in analyzing text data? How can text data potentially help in mining non-text data?\n",
            "- Can you give some examples of context of a text article? How can we partition text data using context information? Can you give some examples where we can leverage context information to perform interesting comparative analysis of topics in text data?\n",
            "- What’s the general idea of Contextual Probabilistic Latent Semantic Analysis (CPLSA)? How is it different from PLSA?\n",
            "- Can you give some examples of interesting topic patterns that can be found by CPLSA? What’s the general idea of using CPLSA for analyzing the impact of an event? Can you think of an interesting application of this kind?\n",
            "- What’s the general idea of using the social network of authors of text data as a complex context to improve topic analysis for text data? Can you give an example of an interesting application of this kind?\n",
            "- What’s the general idea of using a time series like stock prices over time to supervise the discovery of topics from text data? Can you give an example of an interesting application of this kind?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week2.txt\n",
            "Length: 1971 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain what TF-IDF weighting is and why TF transformation and document length normalization are necessary for the design of an effective ranking function.\n",
            "- Explain what an inverted index is and how to construct it for a large set of text documents that do not fit into the memory.\n",
            "- Explain how variable-length encoding can be used to compress integers and how unary coding and gamma-coding work.\n",
            "- Explain how scoring of documents in response to a query can be done quickly by using an inverted index.\n",
            "- Explain Zipf’s law.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "- What are some different ways to place a document as a vector in the vector space?\n",
            "- What is term frequency (TF)?\n",
            "- What is TF transformation?\n",
            "- What is document frequency (DF)?\n",
            "- What is inverse document frequency (IDF)?\n",
            "- What is TF-IDF weighting?\n",
            "- Why do we need to penalize long documents in text retrieval?\n",
            "- What is pivoted document length normalization?\n",
            "- What are the main ideas behind the retrieval function BM25?\n",
            "- What is the typical architecture of a text retrieval system?\n",
            "- What is an inverted index?\n",
            "- Why is it desirable to compress an inverted index?\n",
            "- How can we create an inverted index when the collection of documents does not fit into the memory?\n",
            "- How can we leverage an inverted index to score documents quickly?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week3.txt\n",
            "Length: 2411 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system.\n",
            "- Explain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1.\n",
            "- Explain how to evaluate a ranked list of documents.\n",
            "- Explain how to compute and plot a precision-recall curve.\n",
            "- Explain how to compute average precision and mean average precision (MAP).\n",
            "- Explain how to evaluate a ranked list with multi-level relevance judgments.\n",
            "- Explain how to compute normalized discounted cumulative gain.\n",
            "- Explain why it is important to perform statistical significance tests.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "- Why is evaluation so critical for research and application development in text retrieval?\n",
            "- How does the Cranfield evaluation methodology work?\n",
            "- How do we evaluate a set of retrieved documents?\n",
            "- How do you compute precision, recall, and F1?\n",
            "- How do we evaluate a ranked list of search results?\n",
            "- How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)?\n",
            "- What is mean reciprocal rank?\n",
            "- Why is MAP more appropriate than precision at k documents when comparing two retrieval methods?\n",
            "- Why is precision at k documents more meaningful than average precision from a user’s perspective?\n",
            "- How can we evaluate a ranked list of search results using multi-level relevance judgments?\n",
            "- How do you compute normalized discounted cumulative gain (nDCG)?\n",
            "- Why is normalization necessary in nDCG? Does MAP need a similar normalization?  Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week4.txt\n",
            "Length: 3065 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain how to interpret p(R=1|q,d) and estimate it based on a large set of collected relevance judgments (or clickthrough information) about query q and document d.\n",
            "- Explain how to interpret the conditional probability p(q|d) used for scoring documents in the query likelihood retrieval function.\n",
            "- Explain what a statistical language model and a unigram language model are.\n",
            "- Explain how to compute the maximum likelihood estimate of a unigram language model.\n",
            "- Explain how to use unigram language models to discover semantically related words.\n",
            "- Compute p(q|d) based on a given document language model p(w|d).\n",
            "- Explain what smoothing does.\n",
            "- Show that query likelihood retrieval function implements TF-IDF weighting if we smooth the document language model p(w|d) using the collection language model p(w|C) as a reference language model.\n",
            "- Compute the estimate of p(w|d) using Jelinek-Mercer (JM) smoothing and Dirichlet Prior smoothing, respectively.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "- Given a table of relevance judgments in the form of three columns (query, document, and binary relevance judgments), how can we estimate p(R=1|q,d)?\n",
            "- How should we interpret the query likelihood conditional probability p(q|d)?\n",
            "- What is a statistical language model? What is a unigram language model? How many parameters are there in a unigram language model?\n",
            "- How do we compute the maximum likelihood estimate of the unigram language model (based on a text sample)?\n",
            "- What is a background language model? What is a collection language model? What is a document language model?\n",
            "- Why do we need to smooth a document language model in the query likelihood retrieval model? What would happen if we don’t do smoothing?\n",
            "- When we smooth a document language model using a collection language model as a reference language model, what is the probability assigned to an unseen word in a document?\n",
            "- How can we prove that the query likelihood retrieval function implements TF-IDF weighting if we use a collection language model smoothing?\n",
            "- How does linear interpolation (Jelinek-Mercer) smoothing work? What is the formula?\n",
            "- How does Dirichlet prior smoothing work? What is the formula?\n",
            "- What are the similarities and differences between Jelinek-Mercer smoothing and Dirichlet prior smoothing?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "- p(R=1|q,d) ; query likelihood, p(q|d)\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week5.txt\n",
            "Length: 3064 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain the similarity and differences in the three different kinds of feedback, i.e., relevance feedback, pseudo-relevance feedback, and implicit feedback.\n",
            "- Explain how the Rocchio feedback algorithm works.\n",
            "- Explain how the Kullback-Leibler (KL) divergence retrieval function generalizes the query likelihood retrieval function.\n",
            "- Explain the basic idea of using a mixture model for feedback.\n",
            "- Explain some of the main general challenges in creating a web search engine.\n",
            "- Explain what a web crawler is and what factors have to be considered when designing a web crawler.\n",
            "- Explain the basic idea of Google File System (GFS).\n",
            "- Explain the basic idea of MapReduce and how we can use it to build an inverted index in parallel.\n",
            "- Explain how links on the web can be leveraged to improve search results.\n",
            "- Explain how PageRank algorithm works.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "- What is relevance feedback? What is pseudo-relevance feedback? What is implicit feedback?\n",
            "- How does Rocchio work? Why do we need to ensure that the original query terms have sufficiently large weights in feedback?\n",
            "- What is the KL-divergence retrieval function? How is it related to the query likelihood retrieval function?\n",
            "- What is the basic idea of the two-component mixture model for feedback?\n",
            "- What are some of the general challenges in building a web search engine?\n",
            "- What is a crawler? How can we implement a simple crawler?\n",
            "- What is focused crawling? What is incremental crawling?\n",
            "- What kind of pages should have a higher priority for recrawling in incremental crawling?\n",
            "- What can we do if the inverted index doesn’t fit in any single machine?\n",
            "- What’s the basic idea of the Google File System (GFS)?\n",
            "- How does MapReduce work? What are the two key functions that a programmer needs to implement when programming with a MapReduce framework?\n",
            "- How can we use MapReduce to build an inverted index in parallel?\n",
            "- What is anchor text? Why is it useful for improving search accuracy?\n",
            "- What is a hub page? What is an authority page?\n",
            "- What kind of web pages tend to receive high scores from PageRank?\n",
            "- How can we interpret PageRank from the perspective of a random surfer “walking” on the Web?\n",
            "- How exactly do you compute PageRank scores?\n",
            "- How does the HITS algorithm work?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week6.txt\n",
            "Length: 1595 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain how we can extend a retrieval system to perform content-based information filtering (recommendation).\n",
            "- Explain how we can use a linear utility function to evaluate an information filtering system.\n",
            "- Explain the basic idea of collaborative filtering.\n",
            "- Explain how the memory-based collaborative filtering algorithm works.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "- What is content-based information filtering?\n",
            "- How can we use a linear utility function to evaluate a filtering system? How should we set the coefficients in such a linear utility function?\n",
            "- How can we extend a retrieval system to perform content-based information filtering?\n",
            "- What is the exploration-exploitation tradeoff?\n",
            "- How does the beta-gamma threshold learning algorithm work?\n",
            "- What is the basic idea of collaborative filtering?\n",
            "- How does the memory-based collaborative filtering algorithm work?\n",
            "- What is the “cold start” problem in collaborative filtering?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week7.txt\n",
            "Length: 1706 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain some basic concepts in natural language processing.\n",
            "- Explain different ways to represent text data.\n",
            "- Explain the two basic types of word associations and how to mine paradigmatic relations from text data.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "- What does a computer have to do in order to understand a natural language sentence?\n",
            "- What is ambiguity?\n",
            "- Why is natural language processing (NLP) difficult for computers?\n",
            "- What is bag-of-words representation?\n",
            "- Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "- What is a paradigmatic relation?\n",
            "- What is a syntagmatic relation?\n",
            "- What is the general idea for discovering paradigmatic relations from text?\n",
            "- What is the general idea for discovering syntagmatic relations from text?\n",
            "- Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "- How does BM25 Term Frequency transformation work?\n",
            "- Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week8.txt\n",
            "Length: 2621 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain some basic concepts of probability including entropy, conditional entropy and mutual information.\n",
            "- Explain some ways of discovering syntagmatic and paradigmatic relations.\n",
            "- Explain the basic idea of Bayesian estimation theory.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "- What is entropy? For what kind of random variables does the entropy function reach its minimum and maximum, respectively?\n",
            "- What is conditional entropy?\n",
            "- What is the relation between conditional entropy H(X|Y) and entropy H(X)? Which is larger?\n",
            "- How can conditional entropy be used for discovering syntagmatic relations?\n",
            "- What is mutual information I(X;Y)? How is it related to entropy H(X) and conditional entropy H(X|Y)?\n",
            "- What’s the minimum value of I(X;Y)? Is it symmetric?\n",
            "- For what kind of X and Y, does mutual information I(X;Y) reach its minimum? For a given X, for what Y does I(X;Y) reach its maximum?\n",
            "- Why is mutual information sometimes more useful for discovering syntagmatic relations than conditional entropy?\n",
            "- What is a topic?\n",
            "- How can we define the task of topic mining and analysis computationally? What’s the input? What’s the output?\n",
            "- How can we heuristically solve the problem of topic mining and analysis by treating a term as a topic? What are the main problems of such an approach?\n",
            "- What are the benefits of representing a topic by a word distribution?\n",
            "- What is a statistical language model? What is a unigram language model? How can we compute the probability of a sequence of words given a unigram language model?\n",
            "- What is Maximum Likelihood estimate of a unigram language model given a text article?\n",
            "- What is the basic idea of Bayesian estimation? What is a prior distribution? What is a posterior distribution? How are they related with each other? What is Bayes rule?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "File: Week9.txt\n",
            "Length: 2250 characters\n",
            "Goals and Objectives:\n",
            "- After you actively engage in the learning experiences in this module, you should be able to:\n",
            "- Explain what a mixture of unigram language model is and why using a background language in a mixture can help “absorb” common words in English.\n",
            "- Explain what PLSA is and how it can be used to mine and analyze topics in text.\n",
            "- Explain the general idea of using a generative model for text mining.\n",
            "- Explain how to compute the probability of observing a word from a mixture model like PLSA.\n",
            "- Explain the basic idea of the EM algorithm and how it works.\n",
            "- Explain the main difference between LDA and PLSA.\n",
            "\n",
            "Guiding Questions:\n",
            "- Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "- What is a mixture model? In general, how do you compute the probability of observing a particular word from a mixture model? What is the general form of the expression for this probability?\n",
            "- What does the maximum likelihood estimate of the component word distributions of a mixture model behave like? In what sense do they “collaborate” and/or “compete”? Why can we use a fixed background word distribution to force a discovered topic word distribution to reduce its probability on the common (often non-content) words?\n",
            "- What is the basic idea of the EM algorithm? What does the E-step typically do? What does the M-step typically do? In which of the two steps do we typically apply the Bayes rule? Does EM converge to a global maximum?\n",
            "- What is PLSA? How many parameters does a PLSA model have? How is this number affected by the size of our data set to be mined? How can we adjust the standard PLSA to incorporate a prior on a topic word distribution?\n",
            "- How is LDA different from PLSA? What is shared by the two models?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Textbook Chapters:\n",
            "10Web_Search.txt - Length: 67279 characters\n",
            "11Recommender_Systems.txt - Length: 37084 characters\n",
            "12Overview_of_Text.txt - Length: 31305 characters\n",
            "13Word_Association_Mining.txt - Length: 55296 characters\n",
            "14Text_Clustering.txt - Length: 52217 characters\n",
            "15Text_Categorization.txt - Length: 40943 characters\n",
            "16Text_Summarization.txt - Length: 26091 characters\n",
            "17Topic_Analysis.txt - Length: 148741 characters\n",
            "18Opinion_Mining_and.txt - Length: 61664 characters\n",
            "19Joint_Analysis_of_Text.txt - Length: 194448 characters\n",
            "1Introduction.txt - Length: 41063 characters\n",
            "20Toward_A_Unified_System_for_Text_Management_and_Analysis.txt - Length: 21824 characters\n",
            "2Background.txt - Length: 38897 characters\n",
            "3Text_Data_Understanding.txt - Length: 39724 characters\n",
            "4META:_A_Unified_Toolkit.txt - Length: 26994 characters\n",
            "5Overview_of_Text.txt - Length: 32677 characters\n",
            "6Retrieval_Models.txt - Length: 99407 characters\n",
            "7Feedback.txt - Length: 30073 characters\n",
            "8Search_Engine.txt - Length: 38671 characters\n",
            "9Search_Engine_Evaluation.txt - Length: 53404 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week1 Transcripts:\n",
            "Week1 Transcripts:\n",
            "1_1_Natural_Language_Content_Analysis.txt - Length: 17689 characters\n",
            "1_2_Test_Access.txt - Length: 7571 characters\n",
            "1_3_Text_Retrieval_Problem.txt - Length: 18243 characters\n",
            "1_4_Overview_of_Text_Retrieval_Methods.txt - Length: 7128 characters\n",
            "1_5_Vector_Space_Model_Basic_Idea.txt - Length: 6812 characters\n",
            "1_6_Vector_Space_Retrieval_Model_Simplest_Instantiation.txt - Length: 12108 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week10 Transcripts:\n",
            "Week10 Transcripts:\n",
            "10_1_Text_Clustering_Motivation.txt - Length: 7830 characters\n",
            "10_2_Text_Clustering_Generative_Probabilistic_Models_Part_1.txt - Length: 12484 characters\n",
            "10_3_Text_Clustering_Generative_Probabilistic_Models_Part_2.txt - Length: 6291 characters\n",
            "10_4_Text_Clustering_Generative_Probabilistic_Models_Part_3.txt - Length: 11573 characters\n",
            "10_5_Text_Clustering_Similarity_based_Approaches.txt - Length: 14092 characters\n",
            "10_6_Text_Clustering_Evaluation.txt - Length: 8017 characters\n",
            "10_7_Text_Categorization_Motivation.txt - Length: 11195 characters\n",
            "10_8_Text_Categorization_Methods.txt - Length: 8876 characters\n",
            "10_9_Text_Categorization_Generative_Probabilistic_Models.txt - Length: 25044 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week11 Transcripts:\n",
            "Week11 Transcripts:\n",
            "11_1_Text_Categorization_Discriminative_Classifier_Part_1.txt - Length: 15810 characters\n",
            "11_2_Text_Categorization_Discriminative_Classifier_Part_2.txt - Length: 24814 characters\n",
            "11_3_Text_Categorization_Evaluation_Part_1.txt - Length: 11590 characters\n",
            "11_4_Text_Categorization_Evaluation_Part_2.txt - Length: 8838 characters\n",
            "11_5_Opinion_Mining_and_Sentiment_Analysis_Motivation.txt - Length: 13704 characters\n",
            "11_6_Opinion_Mining_and_Sentiment_Analysis_Sentiment_Classification.txt - Length: 9606 characters\n",
            "11_7_Opinion_Mining_and_Sentiment_Analysis_Ordinal_Logistic_Regression.txt - Length: 9908 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week12 Transcripts:\n",
            "Week12 Transcripts:\n",
            "12_1_Opinion_Mining_and_Sentiment_Analysis_Latent_Aspect_Rating_Analysis_Part_1.txt - Length: 11636 characters\n",
            "12_2_Opinion_Mining_and_Sentiment_Analysis_Latent_Aspect_Rating_Analysis_Part_2.txt - Length: 12390 characters\n",
            "12_3_Text_Based_Prediction.txt - Length: 9698 characters\n",
            "12_4_Contextual_Text_Mining_Motivation.txt - Length: 5356 characters\n",
            "12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt - Length: 13693 characters\n",
            "12_6_Contextual_Text_Mining_Mining_Topics_with_Social_Network_Context.txt - Length: 10712 characters\n",
            "12_7_Contextual_Text_Mining_Mining_Causal_Topics_with_Time_Series_Supervision.txt - Length: 15926 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week2 Transcripts:\n",
            "Week2 Transcripts:\n",
            "2_1_Vector_Space_Model_Improved_Instantiation.txt - Length: 11821 characters\n",
            "2_2_TF_Transformation.txt - Length: 6700 characters\n",
            "2_3_Doc_Length_Normalization.txt - Length: 13958 characters\n",
            "2_4_Implementation_of_TR_Systems.txt - Length: 16096 characters\n",
            "2_5_System_Implementation_Inverted_Index_Construction.txt - Length: 12988 characters\n",
            "2_6_System_Implementation_Fast_Search.txt - Length: 12661 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week3 Transcripts:\n",
            "Week3 Transcripts:\n",
            "3_1_Evaluation_of_TR_Systems.txt - Length: 8089 characters\n",
            "3_2_Evaluation_of_TR_Systems_Basic_Measures.txt - Length: 9493 characters\n",
            "3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt - Length: 12679 characters\n",
            "3_4_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt - Length: 8028 characters\n",
            "3_5_Evalution_of_TR_Systems_Multi_Level_Judgements.txt - Length: 8139 characters\n",
            "3_6_Evalution_of_TR_Systems_Practical_Issues.txt - Length: 12011 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week4 Transcripts:\n",
            "Week4 Transcripts:\n",
            "4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt - Length: 9552 characters\n",
            "4_2_Statistical_Language_Model.txt - Length: 14782 characters\n",
            "4_3_Query_Likelihood_Retrieval_Function.txt - Length: 9289 characters\n",
            "4_4_Statistical_Language_Model_Part_1.txt - Length: 8670 characters\n",
            "4_5_Statistical_Language_Model_Part_2.txt - Length: 7395 characters\n",
            "4_6_Smoothing_Methods_Part_1.txt - Length: 6988 characters\n",
            "4_7_Smoothing_Methods_Part_2.txt - Length: 9946 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week5 Transcripts:\n",
            "Week5 Transcripts:\n",
            "5_1_Feedback_in_Text_Retrieval.txt - Length: 5307 characters\n",
            "5_2_Feedback_in_Vector_Space_Model_Rocchio.txt - Length: 9077 characters\n",
            "5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt - Length: 15146 characters\n",
            "5_4_Web_Search_Introduction_&_Web_Crawler.txt - Length: 8891 characters\n",
            "5_5_Web_Indexing.txt - Length: 12478 characters\n",
            "5_6_Link_Analysis_Part_1.txt - Length: 6965 characters\n",
            "5_7_Link_Analysis_Part_2.txt - Length: 12429 characters\n",
            "5_8_Link_Analysis_Part_3_OPTIONAL.txt - Length: 4606 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week6 Transcripts:\n",
            "Week6 Transcripts:\n",
            "6_10_Summary_for_Exam_1.txt - Length: 7039 characters\n",
            "6_1_Learning_to_Rank_Part_1_OPTIONAL.txt - Length: 4264 characters\n",
            "6_2_Learning_to_Rank_Part_2_OPTIONAL.txt - Length: 6797 characters\n",
            "6_3_Learning_to_Rank_Part_3_OPTIONAL.txt - Length: 3243 characters\n",
            "6_4_Future_of_Web_Search.txt - Length: 10627 characters\n",
            "6_5_Recommender_Systems_Content_Based_Filtering_Part_1.txt - Length: 10163 characters\n",
            "6_6_Recommender_Systems_Content_Based_Filtering_Part_2.txt - Length: 8158 characters\n",
            "6_7_Recommender_Systems_Collaborative_Filtering_Part_1.txt - Length: 4601 characters\n",
            "6_8_Recommender_Systems_Collaborative_Filtering_Part_2.txt - Length: 8340 characters\n",
            "6_9_Recommender_Systems_Collaborative_Filtering_Part_3.txt - Length: 3496 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week7 Transcripts:\n",
            "Week7 Transcripts:\n",
            "7_1_Overview_Text_Mining_And_Analytics_Part_1.txt - Length: 7908 characters\n",
            "7_2_Overview_Text_Mining_And_Analytics_Part_2.txt - Length: 8200 characters\n",
            "7_3_Natural_Language_Content_Analysis_Part_1.txt - Length: 9006 characters\n",
            "7_4_Natural_Language_Content_Analysis_Part_2.txt - Length: 2969 characters\n",
            "7_5_Text_Representation_Part_1.txt - Length: 7432 characters\n",
            "7_6_Text_Representation_Part_2.txt - Length: 6897 characters\n",
            "7_8_Paradigmatic_Relation_Discovery_Part_1.txt - Length: 10257 characters\n",
            "7_9_Paradigmatic_Relation_Discovery_Part_2.txt - Length: 12089 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week8 Transcripts:\n",
            "Week8 Transcripts:\n",
            "8_10_Probabilistic_Topic_Models_Mining_One_Topic.txt - Length: 8822 characters\n",
            "8_1_Syntagmatic_Relation_Discovery_Entropy.txt - Length: 7597 characters\n",
            "8_2_Syntagmatic_Relation_Discovery_Conditional_Entropy.txt - Length: 8292 characters\n",
            "8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt - Length: 10435 characters\n",
            "8_4_Syntagmatic_Relation_Discovery_Mutual_Information_Part_2.txt - Length: 7470 characters\n",
            "8_5_Topic_Mining_and_Analysis_Motivation_and_Task_Definition.txt - Length: 5740 characters\n",
            "8_6_Topic_Mining_and_Analysis_Term_as_Topic.txt - Length: 8726 characters\n",
            "8_7_Topic_Mining_and_Analysis_Probabilistic_Topic_Models.txt - Length: 11569 characters\n",
            "8_8_Probabilistic_Topic_Models_Overview_of_Statistical_Language_Models_Part_1.txt - Length: 7886 characters\n",
            "8_9_Probabilistic_Topic_Models_Overview_of_Statistical_Language_Models_Part_2.txt - Length: 9146 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing Week9 Transcripts:\n",
            "Week9 Transcripts:\n",
            "9_10_Latent_Dirichlet_Allocation_LDA_Part_2.txt - Length: 9938 characters\n",
            "9_1_Probabilistic_Topic_Models_Mixture_of_Unigram_Language_Models.txt - Length: 10071 characters\n",
            "9_2_Probabilistic_Topic_Models_Mixture_Model_Estimation_Part_1.txt - Length: 8138 characters\n",
            "9_3_Probabilistic_Topic_Models_Mixture_Model_Estimation_Part_2.txt - Length: 6693 characters\n",
            "9_4_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_1.txt - Length: 7995 characters\n",
            "9_5_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_2.txt - Length: 8046 characters\n",
            "9_6_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_3.txt - Length: 4898 characters\n",
            "9_7_Probabilistic_Latent_Semantic_Analysis_PLSA_Part_1.txt - Length: 8794 characters\n",
            "9_8_Probabilistic_Latent_Semantic_Analysis_PLSA_Part_2.txt - Length: 7823 characters\n",
            "9_9_Latent_Dirichlet_Allocation_LDA_Part_1.txt - Length: 8199 characters\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Advanced Text Analysis and Retrieval System\n",
        "\n",
        "This section of the project is dedicated to the development of an advanced text analysis and retrieval system. It encompasses several key functionalities designed to streamline the process of querying and extracting valuable insights from a comprehensive set of educational resources. The main components and their functionalities include:\n",
        "\n",
        "- **Content Extraction from Overview Files:** Utilizing a custom function to parse and extract guiding questions from overview files. This ensures a focused approach in identifying key areas of study and topics of interest.\n",
        "\n",
        "- **Preprocessing of Textual Data:** Implementation of a preprocessing routine involving lemmatization and removal of stopwords. This step is crucial for standardizing the text data, enhancing the effectiveness of subsequent analysis.\n",
        "\n",
        "- **Lecture Transcripts and Textbook Chapters Processing:** Systematic processing of lecture transcripts and textbook chapters, converting them into a format suitable for advanced text analysis. This includes organizing lectures by weeks and chapters by their specific content.\n",
        "\n",
        "- **TF-IDF Vectorization:** Application of the Term Frequency-Inverse Document Frequency (TF-IDF) vectorization to transform the textual data into a numerical format. This transformation is vital for enabling sophisticated similarity comparisons.\n",
        "\n",
        "- **Cosine Similarity-Based Document Retrieval:** Utilization of cosine similarity measures to identify the most relevant documents in response to a query. This component is adept at retrieving the top matching lecture transcripts and textbook chapters, tailored to the specifics of each query.\n",
        "\n",
        "- **Results Presentation:** Displaying the top matching documents, including lecture transcripts and the most relevant textbook chapter for each query. This provides users with immediate access to the most pertinent information, fostering an efficient and targeted learning experience.\n",
        "\n",
        "This comprehensive system ensures that students and educators can swiftly locate the most relevant information, thereby enhancing the overall effectiveness of the learning process."
      ],
      "metadata": {
        "id": "OyU2PrxzeltR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_content(heading, text, next_heading=None):\n",
        "    if next_heading:\n",
        "        pattern = re.compile(rf\"{heading}\\n(.*?)\\n{next_heading}\", re.DOTALL)\n",
        "    else:\n",
        "        pattern = re.compile(rf\"{heading}\\n(.*?)(?=\\n[A-Z][a-z])\", re.DOTALL)\n",
        "    match = pattern.search(text)\n",
        "    content = match.group(1).strip() if match else \"\"\n",
        "    return [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "\n",
        "# Function to process overview files and create a list of all guiding questions\n",
        "def process_overviews_and_extract_questions(directory):\n",
        "    all_guiding_questions = []\n",
        "    files = sorted(os.listdir(directory))\n",
        "    for file in files:\n",
        "        file_path = os.path.join(directory, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            guiding_questions = extract_content(\"Guiding Questions\", text, \"Key Phrases and Concepts\")\n",
        "            all_guiding_questions.extend(guiding_questions)\n",
        "\n",
        "    return all_guiding_questions\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.lower().split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Directories\n",
        "lecture_directory = \"/content/drive/MyDrive/CS_410/Text Files/Lectures\"\n",
        "chapter_directory = \"/content/drive/MyDrive/CS_410/Text Files/Textbook\"\n",
        "overview_directory = \"/content/drive/MyDrive/CS_410/Text Files/Overview\"\n",
        "\n",
        "# Extract and preprocess overview questions\n",
        "overview_questions = process_overviews_and_extract_questions(overview_directory)\n",
        "\n",
        "# Read and preprocess transcripts and textbook chapters\n",
        "documents = []\n",
        "document_names = []\n",
        "\n",
        "# Process lecture transcripts\n",
        "for week_folder in sorted(os.listdir(lecture_directory)):\n",
        "    week_path = os.path.join(lecture_directory, week_folder)\n",
        "    if os.path.isdir(week_path):\n",
        "        for filename in os.listdir(week_path):\n",
        "            file_path = os.path.join(week_path, filename)\n",
        "            if filename.endswith(\".txt\"):\n",
        "                with open(file_path, 'r') as file:\n",
        "                    documents.append(preprocess(file.read()))\n",
        "                    document_names.append(f\"{week_folder}/{filename}\")\n",
        "\n",
        "# Process textbook chapters\n",
        "for filename in os.listdir(chapter_directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(chapter_directory, filename), 'r') as file:\n",
        "            documents.append(preprocess(file.read()))\n",
        "            document_names.append(f\"Textbook/{filename}\")\n",
        "\n",
        "# TfidfVectorizer with preprocessing\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Function to find top matching documents\n",
        "def find_top_documents(query, top_n=3, is_textbook=False):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "    if is_textbook:\n",
        "        # Filter to include only textbook chapters\n",
        "        textbook_indices = [i for i, doc_name in enumerate(document_names) if \"Textbook/\" in doc_name]\n",
        "        textbook_similarities = [cosine_similarities[i] for i in textbook_indices]\n",
        "        top_indices = sorted(range(len(textbook_similarities)), key=lambda i: textbook_similarities[i], reverse=True)[:top_n]\n",
        "        top_indices = [textbook_indices[i] for i in top_indices]  # Map back to original indices\n",
        "    else:\n",
        "        top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
        "\n",
        "    results = [(document_names[i], cosine_similarities[i]) for i in top_indices]\n",
        "    return results[0] if is_textbook else results\n",
        "\n",
        "\n",
        "# Find and print top documents for each query\n",
        "for query in overview_questions:\n",
        "    top_lectures = find_top_documents(query)\n",
        "    top_chapter = find_top_documents(query, top_n=1, is_textbook=True)\n",
        "\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"Top Lecture Transcripts:\")\n",
        "    for doc, score in top_lectures:\n",
        "        if \"Textbook\" not in doc:\n",
        "            print(f\"Matching document: {doc} with score {float(score):.4f}\")\n",
        "\n",
        "    print(\"\\nTop Textbook Chapter:\")\n",
        "    if top_chapter:\n",
        "        chapter_doc, chapter_score = top_chapter\n",
        "        print(f\"Matching chapter: {chapter_doc} with score {float(chapter_score):.4f}\")\n",
        "    else:\n",
        "        print(\"No matching chapter found.\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFsmO6DtZgn5",
        "outputId": "6f04416a-8900-4bc3-f0e7-58919eeb2eb0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0284\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.0271\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0112\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What does a computer have to do in order to understand a natural language sentence?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.3625\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.3421\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.2186\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is ambiguity?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.1432\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.1355\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0337\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is natural language processing (NLP) difficult for computers?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.3464\n",
            "Matching document: Week7/7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.2307\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.2660\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is bag-of-words representation? Why do modern search engines use this simple representation of text?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_6_Text_Representation_Part_2.txt with score 0.2921\n",
            "Matching document: Week7/7_5_Text_Representation_Part_1.txt with score 0.2427\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.1895\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.1554\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are the two modes of text information access? Which mode does a web search engine such as Google support?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_4_Future_of_Web_Search.txt with score 0.3212\n",
            "Matching document: Week1/1_2_Test_Access.txt with score 0.2678\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/5Overview_of_Text.txt with score 0.2902\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: When is browsing more useful than querying to help a user find relevant information?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_2_Test_Access.txt with score 0.4758\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.2582\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/5Overview_of_Text.txt with score 0.3950\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is a text retrieval task defined as a ranking task?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.2187\n",
            "Matching document: Week7/7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.1614\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.1619\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a retrieval model?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.3698\n",
            "Matching document: Week1/1_4_Overview_of_Text_Retrieval_Methods.txt with score 0.3105\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.2923\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are the two assumptions made by the Probability Ranking Principle?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.1514\n",
            "Matching document: Week9/9_2_Probabilistic_Topic_Models_Mixture_Model_Estimation_Part_1.txt with score 0.1413\n",
            "Matching document: Week9/9_3_Probabilistic_Topic_Models_Mixture_Model_Estimation_Part_2.txt with score 0.1307\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/2Background.txt with score 0.0972\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the Vector Space Retrieval Model? How does it work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_6_Vector_Space_Retrieval_Model_Simplest_Instantiation.txt with score 0.2975\n",
            "Matching document: Week1/1_5_Vector_Space_Model_Basic_Idea.txt with score 0.2884\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.2378\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do we define the dimensions of the Vector Space Model? What does “bag of words” representation mean?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_6_Vector_Space_Retrieval_Model_Simplest_Instantiation.txt with score 0.2689\n",
            "Matching document: Week1/1_5_Vector_Space_Model_Basic_Idea.txt with score 0.2491\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1864\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What does the retrieval function intuitively capture when we instantiate a vector space model with bag of words representation and bit representation for documents and queries?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_6_Vector_Space_Retrieval_Model_Simplest_Instantiation.txt with score 0.2667\n",
            "Matching document: Week2/2_1_Vector_Space_Model_Improved_Instantiation.txt with score 0.2076\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.2154\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0284\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.0271\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0112\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is clustering? What are some applications of clustering in text mining and analysis?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_6_Text_Clustering_Evaluation.txt with score 0.4840\n",
            "Matching document: Week10/10_1_Text_Clustering_Motivation.txt with score 0.4576\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/14Text_Clustering.txt with score 0.4971\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does hierarchical agglomerative clustering work? How do single-link, complete-link, and average-link work for computing group similarity? Which of these three ways of computing group similarity is least sensitive to outliers in the data?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_5_Text_Clustering_Similarity_based_Approaches.txt with score 0.4847\n",
            "Matching document: Week5/5_6_Link_Analysis_Part_1.txt with score 0.2034\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/14Text_Clustering.txt with score 0.2420\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do we evaluate clustering results?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_6_Text_Clustering_Evaluation.txt with score 0.3925\n",
            "Matching document: Week10/10_1_Text_Clustering_Motivation.txt with score 0.3130\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/14Text_Clustering.txt with score 0.3597\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is text categorization? What are some applications of text categorization?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_7_Text_Categorization_Motivation.txt with score 0.6549\n",
            "Matching document: Week11/11_4_Text_Categorization_Evaluation_Part_2.txt with score 0.2914\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.2631\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What does the training data for categorization look like?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_7_Text_Categorization_Motivation.txt with score 0.3063\n",
            "Matching document: Week10/10_8_Text_Categorization_Methods.txt with score 0.2629\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.2151\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does the Naïve Bayes classifier work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_7_Opinion_Mining_and_Sentiment_Analysis_Ordinal_Logistic_Regression.txt with score 0.1403\n",
            "Matching document: Week10/10_8_Text_Categorization_Methods.txt with score 0.1280\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.1757\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why do we often use logarithm in the scoring function for Naïve Bayes?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.1413\n",
            "Matching document: Week11/11_1_Text_Categorization_Discriminative_Classifier_Part_1.txt with score 0.1368\n",
            "Matching document: Week10/10_9_Text_Categorization_Generative_Probabilistic_Models.txt with score 0.1254\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.0640\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0284\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.0271\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0112\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the general idea of the logistic regression classifier? How is it related to Naïve Bayes? Under what conditions would logistic regression cover Naïve Bayes as a special case for two-category categorization?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_7_Opinion_Mining_and_Sentiment_Analysis_Ordinal_Logistic_Regression.txt with score 0.2552\n",
            "Matching document: Week11/11_1_Text_Categorization_Discriminative_Classifier_Part_1.txt with score 0.2541\n",
            "Matching document: Week10/10_9_Text_Categorization_Generative_Probabilistic_Models.txt with score 0.2058\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.1668\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the general idea of the k-Nearest Neighbor classifier? How does it work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_1_Text_Categorization_Discriminative_Classifier_Part_1.txt with score 0.2653\n",
            "Matching document: Week11/11_7_Opinion_Mining_and_Sentiment_Analysis_Ordinal_Logistic_Regression.txt with score 0.1569\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.1727\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do we evaluate categorization results?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_7_Text_Categorization_Motivation.txt with score 0.4014\n",
            "Matching document: Week11/11_4_Text_Categorization_Evaluation_Part_2.txt with score 0.2581\n",
            "Matching document: Week10/10_8_Text_Categorization_Methods.txt with score 0.1615\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.1557\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do we compute classification accuracy, precision, recall, and F score?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.4653\n",
            "Matching document: Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt with score 0.3936\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.3429\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is harmonic mean as used in F better than the arithmetic mean of precision and recall?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.3600\n",
            "Matching document: Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt with score 0.3382\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.3231\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the difference between macro and micro averaging?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_4_Text_Categorization_Evaluation_Part_2.txt with score 0.3330\n",
            "Matching document: Week3/3_4_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.0461\n",
            "Matching document: Week3/3_6_Evalution_of_TR_Systems_Practical_Issues.txt with score 0.0261\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.0124\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is it sometimes interesting to frame a categorization problem as a ranking problem?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_7_Text_Categorization_Motivation.txt with score 0.3016\n",
            "Matching document: Week11/11_4_Text_Categorization_Evaluation_Part_2.txt with score 0.2637\n",
            "Matching document: Week10/10_8_Text_Categorization_Methods.txt with score 0.1654\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.1244\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is an opinion? How is it different from a factual statement?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_5_Opinion_Mining_and_Sentiment_Analysis_Motivation.txt with score 0.4254\n",
            "Matching document: Week12/12_2_Opinion_Mining_and_Sentiment_Analysis_Latent_Aspect_Rating_Analysis_Part_2.txt with score 0.0615\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/18Opinion_Mining_and.txt with score 0.1468\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s an opinion holder? What’s an opinion target?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_5_Opinion_Mining_and_Sentiment_Analysis_Motivation.txt with score 0.7487\n",
            "Matching document: Week11/11_6_Opinion_Mining_and_Sentiment_Analysis_Sentiment_Classification.txt with score 0.1356\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/18Opinion_Mining_and.txt with score 0.2609\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the goal of opinion mining?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_5_Opinion_Mining_and_Sentiment_Analysis_Motivation.txt with score 0.5757\n",
            "Matching document: Week12/12_3_Text_Based_Prediction.txt with score 0.2002\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/18Opinion_Mining_and.txt with score 0.2171\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is sentiment analysis? How is it similar to and different from a text categorization task such as topic categorization?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_7_Text_Categorization_Motivation.txt with score 0.5684\n",
            "Matching document: Week11/11_4_Text_Categorization_Evaluation_Part_2.txt with score 0.2950\n",
            "Matching document: Week11/11_6_Opinion_Mining_and_Sentiment_Analysis_Sentiment_Classification.txt with score 0.2772\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.2413\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why are unigram features generally insufficient for accurate sentiment classification?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_6_Opinion_Mining_and_Sentiment_Analysis_Sentiment_Classification.txt with score 0.2274\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/18Opinion_Mining_and.txt with score 0.1001\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the concern of using too many complex features such as frequent substructures of parse trees?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week11/11_6_Opinion_Mining_and_Sentiment_Analysis_Sentiment_Classification.txt with score 0.1405\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.0482\n",
            "Matching document: Week11/11_2_Text_Categorization_Discriminative_Classifier_Part_2.txt with score 0.0357\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/15Text_Categorization.txt with score 0.0324\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are some commonly used features to represent text data?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_3_Text_Based_Prediction.txt with score 0.2546\n",
            "Matching document: Week7/7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.2401\n",
            "Matching document: Week7/7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.2303\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.1992\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0284\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.0271\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0112\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is text-based prediction interesting from an application perspective? Why are humans playing an important role in text-based prediction? What is the “data mining loop”?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_3_Text_Based_Prediction.txt with score 0.4155\n",
            "Matching document: Week7/7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.3032\n",
            "Matching document: Week7/7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.2280\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.2198\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is it necessary and useful to jointly mine and analyze text and non-text data? How can non-text data potentially help in analyzing text data? How can text data potentially help in mining non-text data?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_3_Text_Based_Prediction.txt with score 0.5719\n",
            "Matching document: Week7/7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.5372\n",
            "Matching document: Week7/7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.5346\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.4308\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Can you give some examples of context of a text article? How can we partition text data using context information? Can you give some examples where we can leverage context information to perform interesting comparative analysis of topics in text data?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_4_Contextual_Text_Mining_Motivation.txt with score 0.4679\n",
            "Matching document: Week7/7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.2831\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.2814\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.2771\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the general idea of Contextual Probabilistic Latent Semantic Analysis (CPLSA)? How is it different from PLSA?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.2200\n",
            "Matching document: Week9/9_10_Latent_Dirichlet_Allocation_LDA_Part_2.txt with score 0.1828\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/19Joint_Analysis_of_Text.txt with score 0.1874\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Can you give some examples of interesting topic patterns that can be found by CPLSA? What’s the general idea of using CPLSA for analyzing the impact of an event? Can you think of an interesting application of this kind?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.1965\n",
            "Matching document: Week8/8_5_Topic_Mining_and_Analysis_Motivation_and_Task_Definition.txt with score 0.0941\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/19Joint_Analysis_of_Text.txt with score 0.1346\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the general idea of using the social network of authors of text data as a complex context to improve topic analysis for text data? Can you give an example of an interesting application of this kind?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_6_Contextual_Text_Mining_Mining_Topics_with_Social_Network_Context.txt with score 0.4368\n",
            "Matching document: Week12/12_4_Contextual_Text_Mining_Motivation.txt with score 0.3970\n",
            "Matching document: Week12/12_3_Text_Based_Prediction.txt with score 0.3269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/19Joint_Analysis_of_Text.txt with score 0.2917\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the general idea of using a time series like stock prices over time to supervise the discovery of topics from text data? Can you give an example of an interesting application of this kind?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_7_Contextual_Text_Mining_Mining_Causal_Topics_with_Time_Series_Supervision.txt with score 0.3793\n",
            "Matching document: Week12/12_4_Contextual_Text_Mining_Motivation.txt with score 0.1862\n",
            "Matching document: Week7/7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.1789\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/19Joint_Analysis_of_Text.txt with score 0.1722\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0338\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0160\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.0151\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/4META:_A_Unified_Toolkit.txt with score 0.0128\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are some different ways to place a document as a vector in the vector space?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_6_Vector_Space_Retrieval_Model_Simplest_Instantiation.txt with score 0.5328\n",
            "Matching document: Week1/1_5_Vector_Space_Model_Basic_Idea.txt with score 0.5242\n",
            "Matching document: Week5/5_2_Feedback_in_Vector_Space_Model_Rocchio.txt with score 0.3704\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.3057\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is term frequency (TF)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_2_TF_Transformation.txt with score 0.3119\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.2911\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.2654\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1985\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is TF transformation?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_2_TF_Transformation.txt with score 0.5730\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.1902\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.1816\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1446\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is document frequency (DF)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_1_Vector_Space_Model_Improved_Instantiation.txt with score 0.2090\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.1684\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1738\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is inverse document frequency (IDF)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_1_Vector_Space_Model_Improved_Instantiation.txt with score 0.3437\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.2663\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.2622\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1971\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is TF-IDF weighting?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.4814\n",
            "Matching document: Week2/2_1_Vector_Space_Model_Improved_Instantiation.txt with score 0.2968\n",
            "Matching document: Week2/2_2_TF_Transformation.txt with score 0.2493\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.2130\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why do we need to penalize long documents in text retrieval?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.2546\n",
            "Matching document: Week7/7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.1582\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.1580\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.1460\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is pivoted document length normalization?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.4166\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.1777\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.2129\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are the main ideas behind the retrieval function BM25?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.2208\n",
            "Matching document: Week1/1_4_Overview_of_Text_Retrieval_Methods.txt with score 0.1970\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1385\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the typical architecture of a text retrieval system?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.1440\n",
            "Matching document: Week7/7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.1000\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.1239\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is an inverted index?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.3167\n",
            "Matching document: Week2/2_6_System_Implementation_Fast_Search.txt with score 0.3056\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/8Search_Engine.txt with score 0.2911\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is it desirable to compress an inverted index?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.2029\n",
            "Matching document: Week2/2_6_System_Implementation_Fast_Search.txt with score 0.1909\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/8Search_Engine.txt with score 0.2088\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we create an inverted index when the collection of documents does not fit into the memory?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.2224\n",
            "Matching document: Week2/2_6_System_Implementation_Fast_Search.txt with score 0.1938\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/8Search_Engine.txt with score 0.2014\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we leverage an inverted index to score documents quickly?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_6_System_Implementation_Fast_Search.txt with score 0.3105\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.2981\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/8Search_Engine.txt with score 0.2526\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0338\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0160\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.0151\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/4META:_A_Unified_Toolkit.txt with score 0.0128\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is evaluation so critical for research and application development in text retrieval?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week10/10_6_Text_Clustering_Evaluation.txt with score 0.1566\n",
            "Matching document: Week6/6_3_Learning_to_Rank_Part_3_OPTIONAL.txt with score 0.1460\n",
            "Matching document: Week3/3_1_Evaluation_of_TR_Systems.txt with score 0.1382\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.1345\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does the Cranfield evaluation methodology work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_1_Evaluation_of_TR_Systems.txt with score 0.2111\n",
            "Matching document: Week3/3_6_Evalution_of_TR_Systems_Practical_Issues.txt with score 0.0955\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.1133\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do we evaluate a set of retrieved documents?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt with score 0.2533\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.1202\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.1271\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do you compute precision, recall, and F1?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.4916\n",
            "Matching document: Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt with score 0.4914\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.3571\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do we evaluate a ranked list of search results?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.2181\n",
            "Matching document: Week3/3_1_Evaluation_of_TR_Systems.txt with score 0.1920\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.2179\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_4_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.4648\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.4467\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.4156\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is mean reciprocal rank?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_4_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.3757\n",
            "Matching document: Week6/6_1_Learning_to_Rank_Part_1_OPTIONAL.txt with score 0.0600\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.0954\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is MAP more appropriate than precision at k documents when comparing two retrieval methods?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.3429\n",
            "Matching document: Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt with score 0.2611\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.2882\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is precision at k documents more meaningful than average precision from a user’s perspective?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.5732\n",
            "Matching document: Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt with score 0.4125\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.4668\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we evaluate a ranked list of search results using multi-level relevance judgments?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_1_Evaluation_of_TR_Systems.txt with score 0.1863\n",
            "Matching document: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt with score 0.1662\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.2014\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do you compute normalized discounted cumulative gain (nDCG)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week3/3_5_Evalution_of_TR_Systems_Multi_Level_Judgements.txt with score 0.3518\n",
            "Matching document: Week9/9_8_Probabilistic_Latent_Semantic_Analysis_PLSA_Part_2.txt with score 0.0384\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.1175\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is normalization necessary in nDCG? Does MAP need a similar normalization?  Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.1465\n",
            "Matching document: Week3/3_6_Evalution_of_TR_Systems_Practical_Issues.txt with score 0.1374\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/9Search_Engine_Evaluation.txt with score 0.1292\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0338\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0160\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.0151\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/4META:_A_Unified_Toolkit.txt with score 0.0128\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Given a table of relevance judgments in the form of three columns (query, document, and binary relevance judgments), how can we estimate p(R=1|q,d)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_2_Learning_to_Rank_Part_2_OPTIONAL.txt with score 0.2161\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.1958\n",
            "Matching document: Week6/6_1_Learning_to_Rank_Part_1_OPTIONAL.txt with score 0.1445\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1218\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How should we interpret the query likelihood conditional probability p(q|d)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.3536\n",
            "Matching document: Week4/4_3_Query_Likelihood_Retrieval_Function.txt with score 0.2906\n",
            "Matching document: Week4/4_4_Statistical_Language_Model_Part_1.txt with score 0.2596\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1864\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a statistical language model? What is a unigram language model? How many parameters are there in a unigram language model?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_2_Statistical_Language_Model.txt with score 0.4202\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.3409\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.3469\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How do we compute the maximum likelihood estimate of the unigram language model (based on a text sample)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_2_Statistical_Language_Model.txt with score 0.3222\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.2994\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.2810\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a background language model? What is a collection language model? What is a document language model?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_2_Statistical_Language_Model.txt with score 0.4937\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.4690\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.3573\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why do we need to smooth a document language model in the query likelihood retrieval model? What would happen if we don’t do smoothing?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_6_Smoothing_Methods_Part_1.txt with score 0.3731\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.3582\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.3659\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: When we smooth a document language model using a collection language model as a reference language model, what is the probability assigned to an unseen word in a document?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_2_Statistical_Language_Model.txt with score 0.4718\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.4009\n",
            "Matching document: Week4/4_4_Statistical_Language_Model_Part_1.txt with score 0.3945\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.3291\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we prove that the query likelihood retrieval function implements TF-IDF weighting if we use a collection language model smoothing?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.4897\n",
            "Matching document: Week4/4_7_Smoothing_Methods_Part_2.txt with score 0.4234\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.3931\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does linear interpolation (Jelinek-Mercer) smoothing work? What is the formula?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_6_Smoothing_Methods_Part_1.txt with score 0.2982\n",
            "Matching document: Week4/4_7_Smoothing_Methods_Part_2.txt with score 0.2166\n",
            "Matching document: Week4/4_5_Statistical_Language_Model_Part_2.txt with score 0.1254\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1109\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does Dirichlet prior smoothing work? What is the formula?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week9/9_9_Latent_Dirichlet_Allocation_LDA_Part_1.txt with score 0.2669\n",
            "Matching document: Week4/4_6_Smoothing_Methods_Part_1.txt with score 0.2464\n",
            "Matching document: Week4/4_7_Smoothing_Methods_Part_2.txt with score 0.2419\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1039\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are the similarities and differences between Jelinek-Mercer smoothing and Dirichlet prior smoothing?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_6_Smoothing_Methods_Part_1.txt with score 0.3120\n",
            "Matching document: Week4/4_7_Smoothing_Methods_Part_2.txt with score 0.2601\n",
            "Matching document: Week9/9_9_Latent_Dirichlet_Allocation_LDA_Part_1.txt with score 0.1711\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1182\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0338\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0160\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.0151\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/4META:_A_Unified_Toolkit.txt with score 0.0128\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is relevance feedback? What is pseudo-relevance feedback? What is implicit feedback?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_1_Feedback_in_Text_Retrieval.txt with score 0.4693\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.3914\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/7Feedback.txt with score 0.5752\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does Rocchio work? Why do we need to ensure that the original query terms have sufficiently large weights in feedback?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_2_Feedback_in_Vector_Space_Model_Rocchio.txt with score 0.2547\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.2177\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/7Feedback.txt with score 0.3041\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the KL-divergence retrieval function? How is it related to the query likelihood retrieval function?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.3048\n",
            "Matching document: Week1/1_4_Overview_of_Text_Retrieval_Methods.txt with score 0.2419\n",
            "Matching document: Week4/4_3_Query_Likelihood_Retrieval_Function.txt with score 0.2170\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.2124\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the basic idea of the two-component mixture model for feedback?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.4072\n",
            "Matching document: Week9/9_1_Probabilistic_Topic_Models_Mixture_of_Unigram_Language_Models.txt with score 0.3003\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/7Feedback.txt with score 0.4455\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are some of the general challenges in building a web search engine?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_4_Web_Search_Introduction_&_Web_Crawler.txt with score 0.2324\n",
            "Matching document: Week6/6_4_Future_of_Web_Search.txt with score 0.2037\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.1830\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a crawler? How can we implement a simple crawler?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_4_Web_Search_Introduction_&_Web_Crawler.txt with score 0.1477\n",
            "Matching document: Week1/1_5_Vector_Space_Model_Basic_Idea.txt with score 0.0278\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.1037\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is focused crawling? What is incremental crawling?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_4_Web_Search_Introduction_&_Web_Crawler.txt with score 0.3951\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.1129\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What kind of pages should have a higher priority for recrawling in incremental crawling?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_4_Web_Search_Introduction_&_Web_Crawler.txt with score 0.3379\n",
            "Matching document: Week5/5_6_Link_Analysis_Part_1.txt with score 0.0529\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.0995\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What can we do if the inverted index doesn’t fit in any single machine?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.2178\n",
            "Matching document: Week2/2_6_System_Implementation_Fast_Search.txt with score 0.1998\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/8Search_Engine.txt with score 0.2094\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the basic idea of the Google File System (GFS)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_5_Web_Indexing.txt with score 0.2246\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.1139\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does MapReduce work? What are the two key functions that a programmer needs to implement when programming with a MapReduce framework?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_5_Web_Indexing.txt with score 0.3466\n",
            "Matching document: Week1/1_5_Vector_Space_Model_Basic_Idea.txt with score 0.0691\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.1327\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we use MapReduce to build an inverted index in parallel?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_5_Web_Indexing.txt with score 0.3684\n",
            "Matching document: Week2/2_6_System_Implementation_Fast_Search.txt with score 0.2025\n",
            "Matching document: Week2/2_4_Implementation_of_TR_Systems.txt with score 0.1898\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/8Search_Engine.txt with score 0.1611\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is anchor text? Why is it useful for improving search accuracy?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_10_Summary_for_Exam_1.txt with score 0.1461\n",
            "Matching document: Week5/5_6_Link_Analysis_Part_1.txt with score 0.1334\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/1Introduction.txt with score 0.1504\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a hub page? What is an authority page?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_8_Link_Analysis_Part_3_OPTIONAL.txt with score 0.7375\n",
            "Matching document: Week5/5_6_Link_Analysis_Part_1.txt with score 0.5222\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.4501\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What kind of web pages tend to receive high scores from PageRank?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_4_Web_Search_Introduction_&_Web_Crawler.txt with score 0.1813\n",
            "Matching document: Week5/5_7_Link_Analysis_Part_2.txt with score 0.1510\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.1977\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we interpret PageRank from the perspective of a random surfer “walking” on the Web?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_7_Link_Analysis_Part_2.txt with score 0.3922\n",
            "Matching document: Week5/5_6_Link_Analysis_Part_1.txt with score 0.1112\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.2298\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How exactly do you compute PageRank scores?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_7_Link_Analysis_Part_2.txt with score 0.1621\n",
            "Matching document: Week5/5_8_Link_Analysis_Part_3_OPTIONAL.txt with score 0.1453\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/10Web_Search.txt with score 0.1376\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does the HITS algorithm work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week5/5_8_Link_Analysis_Part_3_OPTIONAL.txt with score 0.0959\n",
            "Matching document: Week9/9_6_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_3.txt with score 0.0485\n",
            "Matching document: Week3/3_1_Evaluation_of_TR_Systems.txt with score 0.0485\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/14Text_Clustering.txt with score 0.0392\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0338\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0160\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.0151\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/4META:_A_Unified_Toolkit.txt with score 0.0128\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is content-based information filtering?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_7_Recommender_Systems_Collaborative_Filtering_Part_1.txt with score 0.3249\n",
            "Matching document: Week6/6_9_Recommender_Systems_Collaborative_Filtering_Part_3.txt with score 0.3029\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.3712\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we use a linear utility function to evaluate a filtering system? How should we set the coefficients in such a linear utility function?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_6_Recommender_Systems_Content_Based_Filtering_Part_2.txt with score 0.2868\n",
            "Matching document: Week6/6_5_Recommender_Systems_Content_Based_Filtering_Part_1.txt with score 0.2668\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.2156\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we extend a retrieval system to perform content-based information filtering?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_5_Recommender_Systems_Content_Based_Filtering_Part_1.txt with score 0.2381\n",
            "Matching document: Week6/6_7_Recommender_Systems_Collaborative_Filtering_Part_1.txt with score 0.2250\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.2518\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the exploration-exploitation tradeoff?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_6_Recommender_Systems_Content_Based_Filtering_Part_2.txt with score 0.2314\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.0514\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does the beta-gamma threshold learning algorithm work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_6_Recommender_Systems_Content_Based_Filtering_Part_2.txt with score 0.3119\n",
            "Matching document: Week6/6_2_Learning_to_Rank_Part_2_OPTIONAL.txt with score 0.1352\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.1375\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the basic idea of collaborative filtering?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_7_Recommender_Systems_Collaborative_Filtering_Part_1.txt with score 0.4087\n",
            "Matching document: Week6/6_9_Recommender_Systems_Collaborative_Filtering_Part_3.txt with score 0.2478\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.3351\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does the memory-based collaborative filtering algorithm work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_7_Recommender_Systems_Collaborative_Filtering_Part_1.txt with score 0.2847\n",
            "Matching document: Week6/6_9_Recommender_Systems_Collaborative_Filtering_Part_3.txt with score 0.1892\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.2422\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the “cold start” problem in collaborative filtering?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week6/6_7_Recommender_Systems_Collaborative_Filtering_Part_1.txt with score 0.3596\n",
            "Matching document: Week6/6_9_Recommender_Systems_Collaborative_Filtering_Part_3.txt with score 0.2609\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/11Recommender_Systems.txt with score 0.2670\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0284\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.0271\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0112\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What does a computer have to do in order to understand a natural language sentence?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.3625\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.3421\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.2186\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is ambiguity?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.1432\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.1355\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0337\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is natural language processing (NLP) difficult for computers?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.3464\n",
            "Matching document: Week7/7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.2307\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.2660\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is bag-of-words representation?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_6_Text_Representation_Part_2.txt with score 0.2537\n",
            "Matching document: Week7/7_5_Text_Representation_Part_1.txt with score 0.1962\n",
            "Matching document: Week1/1_1_Natural_Language_Content_Analysis.txt with score 0.1340\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.1088\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_6_Text_Representation_Part_2.txt with score 0.3393\n",
            "Matching document: Week7/7_5_Text_Representation_Part_1.txt with score 0.3004\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.2911\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a paradigmatic relation?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_4_Syntagmatic_Relation_Discovery_Mutual_Information_Part_2.txt with score 0.1773\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.1616\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.2249\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a syntagmatic relation?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_4_Syntagmatic_Relation_Discovery_Mutual_Information_Part_2.txt with score 0.3329\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.1205\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.2411\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the general idea for discovering paradigmatic relations from text?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_4_Syntagmatic_Relation_Discovery_Mutual_Information_Part_2.txt with score 0.2072\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.1961\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.1693\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the general idea for discovering syntagmatic relations from text?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_4_Syntagmatic_Relation_Discovery_Mutual_Information_Part_2.txt with score 0.3200\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.1656\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.1781\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.3805\n",
            "Matching document: Week7/7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.3530\n",
            "Matching document: Week2/2_2_TF_Transformation.txt with score 0.3469\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/14Text_Clustering.txt with score 0.1600\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How does BM25 Term Frequency transformation work?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week2/2_2_TF_Transformation.txt with score 0.3377\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.2756\n",
            "Matching document: Week2/2_3_Doc_Length_Normalization.txt with score 0.2347\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1175\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week7/7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.3089\n",
            "Matching document: Week2/2_1_Vector_Space_Model_Improved_Instantiation.txt with score 0.2969\n",
            "Matching document: Week7/7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.2860\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/6Retrieval_Models.txt with score 0.1950\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0284\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.0271\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0112\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is entropy? For what kind of random variables does the entropy function reach its minimum and maximum, respectively?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_2_Syntagmatic_Relation_Discovery_Conditional_Entropy.txt with score 0.4427\n",
            "Matching document: Week8/8_1_Syntagmatic_Relation_Discovery_Entropy.txt with score 0.3915\n",
            "Matching document: Week8/8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt with score 0.3078\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/2Background.txt with score 0.1860\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is conditional entropy?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_2_Syntagmatic_Relation_Discovery_Conditional_Entropy.txt with score 0.7089\n",
            "Matching document: Week8/8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt with score 0.4194\n",
            "Matching document: Week8/8_1_Syntagmatic_Relation_Discovery_Entropy.txt with score 0.3512\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.2150\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the relation between conditional entropy H(X|Y) and entropy H(X)? Which is larger?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_2_Syntagmatic_Relation_Discovery_Conditional_Entropy.txt with score 0.6752\n",
            "Matching document: Week8/8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt with score 0.4347\n",
            "Matching document: Week8/8_1_Syntagmatic_Relation_Discovery_Entropy.txt with score 0.3983\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.2620\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can conditional entropy be used for discovering syntagmatic relations?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_2_Syntagmatic_Relation_Discovery_Conditional_Entropy.txt with score 0.5092\n",
            "Matching document: Week8/8_4_Syntagmatic_Relation_Discovery_Mutual_Information_Part_2.txt with score 0.3361\n",
            "Matching document: Week8/8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt with score 0.2980\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.2712\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is mutual information I(X;Y)? How is it related to entropy H(X) and conditional entropy H(X|Y)?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt with score 0.6396\n",
            "Matching document: Week8/8_2_Syntagmatic_Relation_Discovery_Conditional_Entropy.txt with score 0.6352\n",
            "Matching document: Week8/8_1_Syntagmatic_Relation_Discovery_Entropy.txt with score 0.3712\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.2833\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What’s the minimum value of I(X;Y)? Is it symmetric?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_1_Syntagmatic_Relation_Discovery_Entropy.txt with score 0.0804\n",
            "Matching document: Week8/8_9_Probabilistic_Topic_Models_Overview_of_Statistical_Language_Models_Part_2.txt with score 0.0604\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/2Background.txt with score 0.0547\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: For what kind of X and Y, does mutual information I(X;Y) reach its minimum? For a given X, for what Y does I(X;Y) reach its maximum?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt with score 0.2160\n",
            "Matching document: Week6/6_4_Future_of_Web_Search.txt with score 0.0627\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.0680\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Why is mutual information sometimes more useful for discovering syntagmatic relations than conditional entropy?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_3_Syntagmatic_Relation_Discovery_Mutual_Information_Part_1.txt with score 0.5340\n",
            "Matching document: Week8/8_2_Syntagmatic_Relation_Discovery_Conditional_Entropy.txt with score 0.4602\n",
            "Matching document: Week8/8_4_Syntagmatic_Relation_Discovery_Mutual_Information_Part_2.txt with score 0.3730\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/13Word_Association_Mining.txt with score 0.3243\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a topic?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_5_Topic_Mining_and_Analysis_Motivation_and_Task_Definition.txt with score 0.6149\n",
            "Matching document: Week12/12_7_Contextual_Text_Mining_Mining_Causal_Topics_with_Time_Series_Supervision.txt with score 0.3909\n",
            "Matching document: Week8/8_7_Topic_Mining_and_Analysis_Probabilistic_Topic_Models.txt with score 0.3778\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/17Topic_Analysis.txt with score 0.2560\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we define the task of topic mining and analysis computationally? What’s the input? What’s the output?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_5_Topic_Mining_and_Analysis_Motivation_and_Task_Definition.txt with score 0.2738\n",
            "Matching document: Week8/8_7_Topic_Mining_and_Analysis_Probabilistic_Topic_Models.txt with score 0.1634\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/20Toward_A_Unified_System_for_Text_Management_and_Analysis.txt with score 0.1359\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How can we heuristically solve the problem of topic mining and analysis by treating a term as a topic? What are the main problems of such an approach?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_5_Topic_Mining_and_Analysis_Motivation_and_Task_Definition.txt with score 0.2705\n",
            "Matching document: Week8/8_6_Topic_Mining_and_Analysis_Term_as_Topic.txt with score 0.2687\n",
            "Matching document: Week8/8_7_Topic_Mining_and_Analysis_Probabilistic_Topic_Models.txt with score 0.2033\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/19Joint_Analysis_of_Text.txt with score 0.1504\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What are the benefits of representing a topic by a word distribution?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_7_Topic_Mining_and_Analysis_Probabilistic_Topic_Models.txt with score 0.2728\n",
            "Matching document: Week10/10_2_Text_Clustering_Generative_Probabilistic_Models_Part_1.txt with score 0.2320\n",
            "Matching document: Week9/9_1_Probabilistic_Topic_Models_Mixture_of_Unigram_Language_Models.txt with score 0.2227\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/17Topic_Analysis.txt with score 0.1796\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a statistical language model? What is a unigram language model? How can we compute the probability of a sequence of words given a unigram language model?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_2_Statistical_Language_Model.txt with score 0.4959\n",
            "Matching document: Week8/8_8_Probabilistic_Topic_Models_Overview_of_Statistical_Language_Models_Part_1.txt with score 0.3951\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.3819\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is Maximum Likelihood estimate of a unigram language model given a text article?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week4/4_2_Statistical_Language_Model.txt with score 0.3293\n",
            "Matching document: Week5/5_3_Feedback_in_Text_Retrieval_Feedback_in_LM.txt with score 0.2886\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.2929\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the basic idea of Bayesian estimation? What is a prior distribution? What is a posterior distribution? How are they related with each other? What is Bayes rule?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week8/8_9_Probabilistic_Topic_Models_Overview_of_Statistical_Language_Models_Part_2.txt with score 0.4307\n",
            "Matching document: Week9/9_9_Latent_Dirichlet_Allocation_LDA_Part_1.txt with score 0.2860\n",
            "Matching document: Week9/9_4_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_1.txt with score 0.2754\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/2Background.txt with score 0.2104\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt with score 0.0284\n",
            "Matching document: Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt with score 0.0271\n",
            "Matching document: Week1/1_3_Text_Retrieval_Problem.txt with score 0.0269\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/3Text_Data_Understanding.txt with score 0.0112\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is a mixture model? In general, how do you compute the probability of observing a particular word from a mixture model? What is the general form of the expression for this probability?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week9/9_1_Probabilistic_Topic_Models_Mixture_of_Unigram_Language_Models.txt with score 0.5302\n",
            "Matching document: Week9/9_2_Probabilistic_Topic_Models_Mixture_Model_Estimation_Part_1.txt with score 0.4772\n",
            "Matching document: Week9/9_3_Probabilistic_Topic_Models_Mixture_Model_Estimation_Part_2.txt with score 0.3457\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/17Topic_Analysis.txt with score 0.2489\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What does the maximum likelihood estimate of the component word distributions of a mixture model behave like? In what sense do they “collaborate” and/or “compete”? Why can we use a fixed background word distribution to force a discovered topic word distribution to reduce its probability on the common (often non-content) words?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week9/9_1_Probabilistic_Topic_Models_Mixture_of_Unigram_Language_Models.txt with score 0.4492\n",
            "Matching document: Week9/9_2_Probabilistic_Topic_Models_Mixture_Model_Estimation_Part_1.txt with score 0.3942\n",
            "Matching document: Week9/9_4_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_1.txt with score 0.3526\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/17Topic_Analysis.txt with score 0.2905\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is the basic idea of the EM algorithm? What does the E-step typically do? What does the M-step typically do? In which of the two steps do we typically apply the Bayes rule? Does EM converge to a global maximum?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week9/9_6_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_3.txt with score 0.1614\n",
            "Matching document: Week9/9_8_Probabilistic_Latent_Semantic_Analysis_PLSA_Part_2.txt with score 0.1430\n",
            "Matching document: Week9/9_5_Probabilistic_Topic_Models_Expectation_Maximization_Algorithm_Part_2.txt with score 0.1427\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/17Topic_Analysis.txt with score 0.0561\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: What is PLSA? How many parameters does a PLSA model have? How is this number affected by the size of our data set to be mined? How can we adjust the standard PLSA to incorporate a prior on a topic word distribution?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week9/9_10_Latent_Dirichlet_Allocation_LDA_Part_2.txt with score 0.4002\n",
            "Matching document: Week9/9_9_Latent_Dirichlet_Allocation_LDA_Part_1.txt with score 0.3252\n",
            "Matching document: Week9/9_7_Probabilistic_Latent_Semantic_Analysis_PLSA_Part_1.txt with score 0.2163\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/17Topic_Analysis.txt with score 0.1814\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: How is LDA different from PLSA? What is shared by the two models?\n",
            "Top Lecture Transcripts:\n",
            "Matching document: Week9/9_10_Latent_Dirichlet_Allocation_LDA_Part_2.txt with score 0.5800\n",
            "Matching document: Week12/12_6_Contextual_Text_Mining_Mining_Topics_with_Social_Network_Context.txt with score 0.1473\n",
            "Matching document: Week9/9_9_Latent_Dirichlet_Allocation_LDA_Part_1.txt with score 0.1460\n",
            "\n",
            "Top Textbook Chapter:\n",
            "Matching chapter: Textbook/17Topic_Analysis.txt with score 0.0887\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Parallelized Query Processing with GPT-4 and Contextual Document Retrieval\n",
        "\n",
        "This section of the project focuses on efficiently answering guiding questions by leveraging the advanced capabilities of GPT-4, in conjunction with a context-based retrieval system. Utilizing parallel processing and the integration of contextual documents, this system aims to provide comprehensive and relevant answers to key questions derived from educational materials. Key features of this section include:\n",
        "\n",
        "- **GPT-4 Query Function**: Utilizes OpenAI's GPT-4 to generate answers for the provided queries. This function forms the core of the query-answering mechanism, leveraging the advanced language understanding capabilities of GPT-4.\n",
        "\n",
        "- **Contextual Information Gathering**: Before querying GPT-4, the system gathers relevant contextual information from a set of pre-identified documents. This includes the top lecture transcripts and the most relevant textbook chapters related to each query, ensuring that the responses are well-informed and pertinent.\n",
        "\n",
        "- **Preprocessing and TF-IDF Vectorization**: Implements preprocessing routines and TF-IDF vectorization to transform textual data into a suitable format for analysis, enhancing the effectiveness of document retrieval based on query relevance.\n",
        "\n",
        "- **Parallelized Processing**: Employs Python's `concurrent.futures.ThreadPoolExecutor` for parallel processing of multiple queries. This approach significantly improves efficiency, especially when dealing with multiple queries and large volumes of data.\n",
        "\n",
        "- **Dynamic Response Generation**: For each guiding question, the system dynamically generates a prompt that includes the question and its associated contextual documents. This prompt is then used to query GPT-4, ensuring that the AI's response is informed by the most relevant and recent academic content.\n",
        "\n",
        "- **Comprehensive Output**: The output for each query includes the guiding question, the documents used for context (both lecture transcripts and textbook chapters), and the answer generated by GPT-4. This structure provides a clear and thorough understanding of how each response was derived.\n",
        "\n",
        "This system represents a sophisticated approach to automated query answering in educational settings, combining state-of-the-art AI with contextually rich academic resources to deliver insightful and accurate responses."
      ],
      "metadata": {
        "id": "GPGZcxf6es5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import openai\n",
        "\n",
        "OPENAI_KEY = \"INSERT YOUR KEY HERE\"\n",
        "def query_gpt4(prompt):\n",
        "    client = openai.OpenAI(api_key=OPENAI_KEY)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=4096\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def ask_gpt_with_context(question, context_documents, document_names):\n",
        "    prompt = f\"Question: {question}\\n\\nContext:\\n\"\n",
        "    for doc, name in zip(context_documents, document_names):\n",
        "        prompt += f\"Document: {name}\\n{doc}\\n\\n\"\n",
        "\n",
        "    max_length = 128000\n",
        "    if len(prompt) > max_length:\n",
        "        prompt = prompt[-max_length:]\n",
        "\n",
        "    return query_gpt4(prompt)\n",
        "\n",
        "def process_question(query):\n",
        "    try:\n",
        "        top_documents = find_top_documents(query)\n",
        "        top_chapter_result = find_top_documents(query, top_n=1, is_textbook=True)\n",
        "\n",
        "        context_documents = []\n",
        "        context_document_names = []\n",
        "\n",
        "        # Process top lecture transcripts\n",
        "        for doc_name, _ in top_documents:\n",
        "            if doc_name in document_names:\n",
        "                context_documents.append(documents[document_names.index(doc_name)])\n",
        "                context_document_names.append(doc_name)\n",
        "            else:\n",
        "                print(f\"Document '{doc_name}' not found in document_names.\")\n",
        "\n",
        "        # Process top textbook chapter\n",
        "        if top_chapter_result:\n",
        "            chapter_name, _ = top_chapter_result  # Unpack the result\n",
        "            if chapter_name in document_names:\n",
        "                chapter_content = documents[document_names.index(chapter_name)]\n",
        "                context_documents.append(chapter_content)\n",
        "                context_document_names.append(chapter_name)\n",
        "            else:\n",
        "                print(f\"Chapter '{chapter_name}' not found in document_names.\")\n",
        "\n",
        "        answer = ask_gpt_with_context(query, context_documents, context_document_names)\n",
        "\n",
        "        output = {\n",
        "            \"question\": query,\n",
        "            \"documents_used\": context_document_names,\n",
        "            \"answer\": answer\n",
        "        }\n",
        "\n",
        "        return output\n",
        "    except Exception as exc:\n",
        "        print(f\"An error occurred while processing the question '{query}': {exc}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Using ThreadPoolExecutor for parallel processing\n",
        "outputs2 = []\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Submit each query (first three questions) to the executor\n",
        "    future_to_query = {executor.submit(process_question, query): query for query in overview_questions[:1]}\n",
        "\n",
        "    # As each future completes, process its result\n",
        "    for future in concurrent.futures.as_completed(future_to_query):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            output = future.result()\n",
        "            outputs2.append(output)\n",
        "            print(f\"Question: {query}\")\n",
        "            print(f\"Documents used: {', '.join(output['documents_used'])}\")\n",
        "            print(\"Answer:\", output['answer'])\n",
        "            print(\"-------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
        "        except Exception as exc:\n",
        "            print(f\"{query} generated an exception: {exc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfyNoyYPgNsp",
        "outputId": "c43107a2-fcdb-4201-e1fc-e840ea509b3c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Develop your answers to the following guiding questions while watching the video lectures throughout the week.\n",
            "Documents used: Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt, Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt, Week1/1_3_Text_Retrieval_Problem.txt, Textbook/3Text_Data_Understanding.txt\n",
            "Answer: These documents provide a detailed insight into several concepts essential to understanding and working with text data and retrieval systems. While there's a substantial amount of information, I will focus on summarizing the key points from each document relevant to the guiding questions.\n",
            "\n",
            "1. **Week12/12_5_Contextual_Text_Mining_Contextual_Probabilistic_Latent_Semantic_Analysis.txt**:\n",
            "   - This document discusses Contextual Probabilistic Latent Semantic Analysis (CPLSA), which incorporates context variables (like time periods or locations) into topic modeling.\n",
            "   - CPLSA aims to discover how topics and their coverage in text change depending on the context.\n",
            "   - The approach involves modeling conditional likelihoods of text given context and the dependency of topics on context, which can be utilized to extract context-specific variations of topics.\n",
            "   - EM algorithm is used for parameter estimation in this model.\n",
            "\n",
            "2. **Week4/4_1_Probabilistic_Retrieval_Model_Basic_Idea.txt**:\n",
            "   - The probabilistic retrieval model is a different approach from the vector space model, and it formulates ranking functions based on the probability of a document being relevant to a query.\n",
            "   - It uses a binary random variable for relevance and deals with random variables instead of vectors.\n",
            "   - Various sub-classes of the probabilistic model, like the language modeling approach, are discussed for effective text retrieval.\n",
            "\n",
            "3. **Week1/1_3_Text_Retrieval_Problem.txt**:\n",
            "   - Text retrieval is the task of a system responding to a user's query with relevant documents.\n",
            "   - The document discusses the difference between text retrieval and database retrieval, highlighting issues like the ambiguity of queries in text retrieval and the structured nature of database queries.\n",
            "   - Two strategies for text retrieval are discussed: document selection (classification) and document ranking. The latter is preferred due to its flexibility and the ability to deal with relative relevancies.\n",
            "\n",
            "4. **Textbook/3Text_Data_Understanding.txt**:\n",
            "   - This chapter from a textbook introduces basic concepts of text data understanding and natural language processing (NLP), including lexical, syntactic, semantic, pragmatic, and discourse analysis.\n",
            "   - Challenges faced in NLP, such as ambiguity and the need for large-scale knowledge representation, are highlighted.\n",
            "   - The utility of statistical language models in text analysis, such as unigram models for simplicity and robustness, is discussed.\n",
            "   - There's an emphasis on how the quality of the text information system is dependent on NLP capability, though some workarounds bypass the need for deep linguistic analysis.\n",
            "\n",
            "To develop answers to the guiding questions, it is crucial to grasp the fundamentals outlined in these documents, especially the concepts of probabilistic models and language models, as well as the challenges and techniques employed in text mining and retrieval systems. Understanding these concepts will enable the viewer to have informed insights into the use of context in text mining, the workings of retrieval models, and the interplay of different NLP tasks in text analysis and understanding.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 (Optional): Generating Guiding Questions and Key Concepts\n",
        "This step addresses the need for creating study aids when guiding questions and key concepts are not readily available. It automates the extraction of essential educational elements from lecture transcripts, ensuring comprehensive support for learning"
      ],
      "metadata": {
        "id": "SfV-zlo2zKts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_weekly_lectures(base_directory, weeks_to_process):\n",
        "    all_responses = []\n",
        "\n",
        "    # Function to generate the prompt for GPT\n",
        "    def create_gpt_prompt(week_number, lectures_text):\n",
        "        return (f\"Week {week_number} Lectures for a Master-Level Text Information Systems Course:\\n\\n\" +\n",
        "                \"FORMAT YOUR RESPONSE LIKE\" +\n",
        "                \"Guiding Questions:\\n\" +\n",
        "                \"Q1: [Question 1]\\nA1: [Answer to Question 1]\\n\" +\n",
        "                \"Q2: [Question 2]\\nA2: [Answer to Question 2]\\n\\n\" +\n",
        "                \"Key Concepts:\\n\" +\n",
        "                \"Identify Key Concepts mentioned in the lectures and provide a brief definition for each. Format the response as a numbered list, for example:\\n\" +\n",
        "                \"1. [Term1] - [Definition1]\\n\" +\n",
        "                \"2. [Term2] - [Definition2]\\n\"+\n",
        "                \"______________________________\" +\n",
        "                \"Here are my lecture for the week: \"+\n",
        "                lectures_text +\n",
        "                \"\\nBased on the above lectures, please generate Guiding Questions and Key Concepts as follows:\\n\\n\" +\n",
        "                \"AGAIN, please format the response in this EXACT same format\" +\n",
        "                \"Guiding Questions:\\n\" +\n",
        "                \"Q1: [Question 1]\\nA1: [Answer to Question 1]\\n\" +\n",
        "                \"Q2: [Question 2]\\nA2: [Answer to Question 2]\\n\\n\" +\n",
        "                \"Key Concepts:\\n\" +\n",
        "                \"Identify Key Concepts mentioned in the lectures and provide a brief definition for each. Format the response as a numbered list, for example:\\n\" +\n",
        "                \"1. [Term1] - [Definition1]\\n\" +\n",
        "                \"2. [Term2] - [Definition2]\\n\"+\n",
        "                \"______________________________\"\n",
        "            )\n",
        "\n",
        "    # Iterate over the specified weeks\n",
        "    for week in weeks_to_process:\n",
        "        week_folder = f\"Week{week}\"\n",
        "        week_path = os.path.join(base_directory, week_folder)\n",
        "        if os.path.isdir(week_path):\n",
        "            week_lectures = \"\"\n",
        "            for filename in sorted(os.listdir(week_path)):\n",
        "                file_path = os.path.join(week_path, filename)\n",
        "                if filename.endswith(\".txt\"):\n",
        "                    with open(file_path, 'r') as file:\n",
        "                        week_lectures += file.read() + \"\\n\\n\"\n",
        "\n",
        "            prompt = create_gpt_prompt(week, week_lectures)\n",
        "            response_text = \"Week \" + str(week) + \" Overview: \\n\" + query_gpt4(prompt)\n",
        "            all_responses.append(response_text)\n",
        "\n",
        "    return all_responses\n",
        "\n",
        "# Example usage\n",
        "base_directory = \"/content/drive/MyDrive/CS_410/Text Files/Lectures\"\n",
        "selected_weeks = [1, 2]  # Example: process only weeks 1, 2, and 3\n",
        "responses = process_weekly_lectures(base_directory, selected_weeks)\n"
      ],
      "metadata": {
        "id": "33KeyNc_kMlZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_guiding_questions(responses):\n",
        "    all_parsed_questions = []\n",
        "    question_pattern = re.compile(r\"(?:Q\\d*|[-•]|\\d+\\))\\:? ([^\\n]+)\\n(?:A\\d*|[-•]|\\d+\\))\\:? ([^\\n]+)\")\n",
        "\n",
        "    for response in responses:\n",
        "        questions = question_pattern.findall(response)\n",
        "        for question, answer in questions:\n",
        "            all_parsed_questions.append({'Question': question.strip(), 'Answer': answer.strip()})\n",
        "\n",
        "    return all_parsed_questions\n",
        "\n",
        "def parse_key_concepts(responses):\n",
        "    all_parsed_concepts = []\n",
        "    concept_pattern = re.compile(r\"(\\d+\\.|[-•]) ([^\\-•\\n]+) - ([^\\n]+)\")\n",
        "\n",
        "    for response in responses:\n",
        "        concepts = concept_pattern.findall(response)\n",
        "        for _, term, definition in concepts:\n",
        "            all_parsed_concepts.append({'Term': term.strip(), 'Definition': definition.strip()})\n",
        "\n",
        "    return all_parsed_concepts\n",
        "\n",
        "\n",
        "parsed_questions = parse_guiding_questions(responses)\n",
        "parsed_concepts = parse_key_concepts(responses)\n",
        "\n",
        "# Printing the parsed questions and concepts\n",
        "print(\"Guiding Questions:\")\n",
        "for q in parsed_questions:\n",
        "    print(f\"Q: {q['Question']}\")\n",
        "    print(f\"A: {q['Answer']}\\n\")\n",
        "\n",
        "print(\"Key Concepts:\")\n",
        "for c in parsed_concepts:\n",
        "    print(f\"{c['Term']} - {c['Definition']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la0zbjZKv6gw",
        "outputId": "0df25405-898a-4283-b83a-b27adf22177e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guiding Questions:\n",
            "Q: What is Natural Language Processing (NLP) and why is it important in text retrieval?\n",
            "A: Natural Language Processing (NLP) is the main technique for processing natural languages to help computers understand the text data they process. It involves lexical analysis, semantic parsing, and inference among other tasks. It is important in text retrieval because understanding the structure and meaning of text is necessary for effectively finding and organizing relevant information.\n",
            "\n",
            "Q: What are some of the main challenges in natural language processing that affect text retrieval?\n",
            "A: Some main challenges in natural language processing that impact text retrieval include word-level ambiguity, where a word has multiple meanings or syntactic categories (like \"design\" functioning as a noun or a verb); syntactical ambiguities, where a sentence could have multiple interpretations; anaphora resolution, deciding what a pronoun or reference word stands for; and presuppositions, where certain implied information needs to be understood by the system.\n",
            "\n",
            "Q: Why do we need to perform sublinear term frequency (TF) transformation in the vector space model?\n",
            "A: We need to perform sublinear TF transformation to capture the diminishing return of higher term counts in a document and avoid the dominance of a single term over all others. The sublinear transformation, such as the BM25 transformation, helps to ensure that the first occurrence of a term is given more importance than subsequent occurrences, as the initial match is more informative about the document’s relevance to the query.\n",
            "\n",
            "Q: What is document length normalization and why is it important in the vector space model?\n",
            "A: Document length normalization is a process used to penalize long documents that have a higher chance of matching any given query by random chance while avoiding over penalization of documents that are long due to more content as opposed to verbose language. This is important in the vector space model to ensure that document length doesn’t unfairly influence the relevance score of a document. Pivoted length normalization is a common approach where average document length is used as a pivot to adjust the document weight accordingly.\n",
            "\n",
            "Key Concepts:\n",
            "Natural Language Processing (NLP) - A field of study focused on the interaction between computers and human (natural) languages, tasked with enabling computers to understand, interpret, and respond to human language in a valuable way.\n",
            "Lexical Analysis - The process of analyzing text to identify and understand individual words, including their part of speech (nouns, verbs, etc.), often as a first step in the parsing process.\n",
            "Semantic Analysis - A deeper level analysis after syntactical parsing that attempts to deduce the meanings of words, phrases, and sentences in their specific contexts to understand their significance.\n",
            "Bag of Words Representation - A text representation model that ignores the order of words but maintains multiplicity, turning text data into a collection of individual words for analysis purposes.\n",
            "Ranking Function - In text retrieval systems, this function orders documents based on their relevance to a given query, often determined by calculating the similarity between documents' features and the query.\n",
            "Vector Space Model - A model used to represent text documents and queries as vectors of identifiers (like words), where the relevance between documents and queries is calculated based on vector similarity measures such as the cosine similarity.\n",
            "Term Frequency (TF) - A count of how frequently a term appears in a document, often used in scoring the relevance of documents during text retrieval.\n",
            "Document Frequency (DF) - The number of documents in a collection that contain a specific term, used to measure term specificity and weight terms accordingly in relevance calculations.\n",
            "Inference - The logical process of deriving new statements based on the known relationship between existing statements, often part of understanding and reasoning in NLP for text retrieval.\n",
            "Probabilistic Models - Retrieval models that treat queries and documents as observations from random variables and score the relevance as the probability of a document being relevant to a query.\n",
            "Dot Product - A mathematical operation that takes two equal-length sequences of numbers (vectors) and returns a single number, often used to measure vector similarity in vector space models.\n",
            "Query Likelihood Models - A class of probabilistic models in text retrieval where the likelihood of generating the query from a document model is used to score its relevance.\n",
            "Anaphora Resolution - The process of determining which entity a pronoun or a noun phrase refers to within text, which is a challenging aspect of NLP and critical for accurate text interpretation.\n",
            "Term Weight - The relevance or importance of a term in a document or query, which is used in vector space models and other retrieval strategies to score and rank documents.\n",
            "Syntactic Ambiguity - Occurs when a sentence or phrase can be parsed in more than one way, leading to different possible structures and meanings.\n",
            "Vector Space Model (VSM) - A model used to represent text in a multi-dimensional space where each dimension corresponds to a term from the document collection, with the goal of quantifying the relevance of documents to a given query based on the geometric relationship between document and query vectors.\n",
            "Term Frequency (TF) - A count of how many times a term appears in a document, which reflects the importance of the term within that particular document.\n",
            "Inverse Document Frequency (IDF) - A logarithmically scaled ratio used to diminish the weight of terms that appear frequently across multiple documents and boost the weight of terms that are rare in the document collection.\n",
            "BM25 - An advanced ranking function used in text retrieval that incorporates term frequency, inverse document frequency, and document length normalization to calculate the relevance score of a document for a given query.\n",
            "Inverted Index - A data structure used in search engines to store a mapping from content, such as words or phrases, to its locations in a database file, a document, or a set of documents, enabling fast full-text searches.\n",
            "Tokenization - The process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens to enable further text processing like indexing or search.\n",
            "Document Length Normalization - An adjustment made to a term's significance in a document based on the length of the document, ensuring that longer documents do not have an undue advantage in matching queries.\n",
            "Pivoted Length Normalization - A document length normalization technique that uses the average document length as a pivot point for adjusting a term's weight based on document length.\n",
            "Indexer - A system component that processes text documents to create an index that allows efficient retrieval of documents by content.\n",
            "Scorer - Part of a retrieval system that takes a user's query and the index, calculates the relevance of documents in the collection, and ranks them according to their scores.\n",
            "Feedback Mechanism - A component of a retrieval system that utilizes user interactions, such as clicks and views, to adjust and improve the relevance of search results over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def save_week_overviews(responses, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)  # Create the folder if it doesn't exist\n",
        "\n",
        "    for response in responses:\n",
        "        # Use regular expression to find the week number\n",
        "        match = re.search(r\"Week (\\d+)\", response)\n",
        "        if match:\n",
        "            week_number = match.group(1)\n",
        "        else:\n",
        "            # Fallback to find any number if \"Week X\" format isn't found\n",
        "            match = re.search(r\"\\d+\", response)\n",
        "            week_number = match.group() if match else \"Unknown\"\n",
        "\n",
        "        file_name = f\"Week{week_number}.txt\"\n",
        "        file_path = os.path.join(output_folder, file_name)\n",
        "\n",
        "        # Write the response to the file\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.write(response)\n",
        "\n",
        "\n",
        "output_folder = \"/content/drive/MyDrive/CS_410/Text Files/Generated_Overviews\"  # Replace with your desired output folder path\n",
        "save_week_overviews(responses, output_folder)\n"
      ],
      "metadata": {
        "id": "DsIvYeFg4-MN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: User Interaction and Query Processing\n",
        "\n",
        "In this step, we focus on the user interface and the core functionality of our educational resource retrieval system. The following functions and processes are implemented to facilitate user interaction:\n",
        "\n",
        "- **Display Menu:** We provide a menu with options for the user to choose from, including getting guiding questions for a specific week, asking a question, or quitting the program.\n",
        "\n",
        "- **Get Guiding Questions:** Users can input a week number, and the system retrieves and displays guiding questions from the corresponding text file. This feature assists learners in accessing relevant course materials.\n",
        "\n",
        "- **Ask a Question:** Users can input their questions, and the system processes these queries by concurrently retrieving documents and querying GPT-4 for answers. The results are presented to the user, including the top documents used and the generated answer.\n",
        "\n",
        "- **Main Program Loop:** The main program loop ensures continuous user interaction, allowing them to navigate through the available options until they choose to exit the program.\n",
        "\n",
        "This step integrates the user interface with the information retrieval and AI-driven question-answering capabilities, making our educational resource system accessible and efficient.\n"
      ],
      "metadata": {
        "id": "3OQhWHl0lS3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "\n",
        "# Function to display the main menu and get the user's choice\n",
        "def display_menu():\n",
        "    print(\"1. Get Guiding Questions\")\n",
        "    print(\"2. Ask a Question\")\n",
        "    print(\"3. Quit\")\n",
        "    choice = input(\"Enter your choice (1-3): \")\n",
        "    return choice\n",
        "\n",
        "# Function to choose the type of guiding questions (provided or generated)\n",
        "def choose_question_type():\n",
        "    print(\"1. Provided Guiding Questions\")\n",
        "    print(\"2. Generated Guiding Questions\")\n",
        "    choice = input(\"Choose the type of guiding questions (1-2): \")\n",
        "    return choice\n",
        "\n",
        "# Function to get and display guiding questions for a specified week\n",
        "def get_week_options(base_path, subfolder):\n",
        "    \"\"\"Scan the directory to get available week options, sorted numerically.\"\"\"\n",
        "    week_options = []\n",
        "    full_path = os.path.join(base_path, subfolder)\n",
        "    if os.path.exists(full_path):\n",
        "        for filename in os.listdir(full_path):\n",
        "            if filename.startswith(\"Week\") and filename.endswith(\".txt\"):\n",
        "                # Extract the week number from filename and convert to integer\n",
        "                week_number = int(filename[4:-4])\n",
        "                week_options.append(week_number)\n",
        "\n",
        "    # Sort the week numbers in ascending numerical order\n",
        "    week_options = sorted(week_options)\n",
        "\n",
        "    # Convert back to strings\n",
        "    week_options = [str(week) for week in week_options]\n",
        "    return week_options\n",
        "\n",
        "def get_guiding_questions(question_type):\n",
        "    base_path = \"/content/drive/MyDrive/CS_410/Text Files/\"\n",
        "    subfolder = \"Overview\" if question_type == '1' else \"Generated_Overviews\"\n",
        "    week_options = get_week_options(base_path, subfolder)\n",
        "\n",
        "    if week_options:\n",
        "        print(f\"Available weeks: {', '.join(week_options)}\")\n",
        "        week_number = input(\"Enter one of the available week numbers: \")\n",
        "        file_path = os.path.join(base_path, subfolder, f\"Week{week_number}.txt\")\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r') as file:\n",
        "                questions = file.read()\n",
        "                print(questions + \"\\n\\n\")\n",
        "        else:\n",
        "            print(f\"No guiding questions found for week {week_number}.\\n\\n\\n\")\n",
        "    else:\n",
        "        print(f\"No guiding questions available in the {subfolder} folder.\\n\\n\\n\")\n",
        "\n",
        "# Function to ask a question, retrieve documents, and query GPT for an answer\n",
        "def ask_question():\n",
        "    user_question = input(\"Enter your question: \")\n",
        "\n",
        "    # Process the question using concurrent futures\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future = executor.submit(process_question, user_question)\n",
        "        try:\n",
        "            output = future.result()\n",
        "            print(f\"Top Documents: {', '.join(output['documents_used'])}\")\n",
        "            print(f\"Answer: {output['answer']}\\n\\n\")\n",
        "        except Exception as exc:\n",
        "            print(f\"An error occurred: {exc}\")\n",
        "\n",
        "# Main program function\n",
        "def main():\n",
        "    while True:\n",
        "        choice = display_menu()\n",
        "        if choice == '1':\n",
        "            question_type = choose_question_type()\n",
        "            get_guiding_questions(question_type)\n",
        "        elif choice == '2':\n",
        "            ask_question()\n",
        "        elif choice == '3':\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuVZtpdywQr_",
        "outputId": "cf62733b-ca61-4443-c866-f44a98ced83d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 1\n",
            "1. Provided Guiding Questions\n",
            "2. Generated Guiding Questions\n",
            "Choose the type of guiding questions (1-2): 1\n",
            "Available weeks: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n",
            "Enter one of the available week numbers: 3\n",
            "Goals and Objectives\n",
            "After you actively engage in the learning experiences in this module, you should be able to:\n",
            "\n",
            "Explain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system.\n",
            "\n",
            "Explain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1.\n",
            "\n",
            "Explain how to evaluate a ranked list of documents.\n",
            "\n",
            "Explain how to compute and plot a precision-recall curve.\n",
            "\n",
            "Explain how to compute average precision and mean average precision (MAP).\n",
            "\n",
            "Explain how to evaluate a ranked list with multi-level relevance judgments.\n",
            "\n",
            "Explain how to compute normalized discounted cumulative gain.\n",
            "\n",
            "Explain why it is important to perform statistical significance tests.\n",
            "\n",
            "Guiding Questions\n",
            "Develop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\n",
            "\n",
            "Why is evaluation so critical for research and application development in text retrieval? \n",
            "\n",
            "How does the Cranfield evaluation methodology work? \n",
            "\n",
            "How do we evaluate a set of retrieved documents? \n",
            "\n",
            "How do you compute precision, recall, and F1? \n",
            "\n",
            "How do we evaluate a ranked list of search results? \n",
            "\n",
            "How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)? \n",
            "\n",
            "What is mean reciprocal rank? \n",
            "\n",
            "Why is MAP more appropriate than precision at k documents when comparing two retrieval methods?\n",
            "\n",
            "Why is precision at k documents more meaningful than average precision from a user’s perspective? \n",
            "\n",
            "How can we evaluate a ranked list of search results using multi-level relevance judgments? \n",
            "\n",
            "How do you compute normalized discounted cumulative gain (nDCG)? \n",
            "\n",
            "Why is normalization necessary in nDCG? Does MAP need a similar normalization?  Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?\n",
            "\n",
            "Key Phrases and Concepts\n",
            "Keep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\n",
            "\n",
            "Cranfield evaluation methodology\n",
            "\n",
            "Precision and recall\n",
            "\n",
            "Average precision, mean average precision (MAP), and geometric mean average precision (gMAP) \n",
            "\n",
            "Reciprocal rank and mean reciprocal rank \n",
            "\n",
            "F-measure \n",
            "\n",
            "Normalized discounted cumulative Gain (nDCG) \n",
            "\n",
            "Statistical significance test  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 2\n",
            "Enter your question: How do you compute precision, recall, and F1? \n",
            "Top Documents: Week3/3_3_Evalution_of_TR_Systems_Evaluating_Ranked_Lists_Part_1.txt, Week3/3_2_Evaluation_of_TR_Systems_Basic_Measures.txt, Textbook/9Search_Engine_Evaluation.txt, Textbook/9Search_Engine_Evaluation.txt\n",
            "Answer: Precision, recall, and F1 score are standard evaluation metrics used to assess the performance of information retrieval systems, classification models, and other predictive algorithms where the output is binary. Here's how to compute them:\n",
            "\n",
            "1. **Precision** (also known as Positive Predictive Value):\n",
            "   This measures the accuracy of the positive predictions. It is defined as the number of true positives divided by the number of true positives plus false positives.\n",
            "   \\[\n",
            "   Precision = \\frac{TP}{TP + FP}\n",
            "   \\]\n",
            "   where \\( TP \\) is the number of true positive results and \\( FP \\) is the number of false positive results.\n",
            "\n",
            "2. **Recall** (also known as Sensitivity or True Positive Rate):\n",
            "   This measures the ability of the model to identify all relevant instances. It is defined as the number of true positives divided by the number of true positives plus false negatives.\n",
            "   \\[\n",
            "   Recall = \\frac{TP}{TP + FN}\n",
            "   \\]\n",
            "   where \\( FN \\) is the number of false negative results.\n",
            "\n",
            "3. **F1 Score**:\n",
            "   The F1 score is the harmonic mean of precision and recall and therefore balances the two metrics. It is useful when you want to seek a balance between precision and recall.\n",
            "   \\[\n",
            "   F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
            "   \\]\n",
            "   or equivalently,\n",
            "   \\[\n",
            "   F1 = \\frac{2TP}{2TP + FP + FN}\n",
            "   \\]\n",
            "   where \\( TP \\), \\( FP \\), and \\( FN \\) are true positives, false positives, and false negatives respectively.\n",
            "\n",
            "The **harmonic mean** is used for combining precision and recall instead of the arithmetic mean because it gives a more significant weight to lower values, which ensures that a good F1 score is obtained only if both precision and recall are high. A high arithmetic mean can be achieved even if one of the two is low and the other is high, which the harmonic mean counteracts. \n",
            "\n",
            "The specific values used to calculate these metrics are usually derived from a confusion matrix, which tabulates the actual vs. predicted classifications in a two-by-two table: true positive, false positive, false negative, and true negative.\n",
            "\n",
            "\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 1\n",
            "1. Provided Guiding Questions\n",
            "2. Generated Guiding Questions\n",
            "Choose the type of guiding questions (1-2): 2\n",
            "Available weeks: 1, 2\n",
            "Enter one of the available week numbers: 2\n",
            "Week 2 Overview: \n",
            "Guiding Questions:\n",
            "Q1: Why do we need to perform sublinear term frequency (TF) transformation in the vector space model?\n",
            "A1: We need to perform sublinear TF transformation to capture the diminishing return of higher term counts in a document and avoid the dominance of a single term over all others. The sublinear transformation, such as the BM25 transformation, helps to ensure that the first occurrence of a term is given more importance than subsequent occurrences, as the initial match is more informative about the document’s relevance to the query.\n",
            "\n",
            "Q2: What is document length normalization and why is it important in the vector space model?\n",
            "A2: Document length normalization is a process used to penalize long documents that have a higher chance of matching any given query by random chance while avoiding over penalization of documents that are long due to more content as opposed to verbose language. This is important in the vector space model to ensure that document length doesn’t unfairly influence the relevance score of a document. Pivoted length normalization is a common approach where average document length is used as a pivot to adjust the document weight accordingly.\n",
            "\n",
            "Key Concepts:\n",
            "1. Vector Space Model (VSM) - A model used to represent text in a multi-dimensional space where each dimension corresponds to a term from the document collection, with the goal of quantifying the relevance of documents to a given query based on the geometric relationship between document and query vectors.\n",
            "   \n",
            "2. Term Frequency (TF) - A count of how many times a term appears in a document, which reflects the importance of the term within that particular document.\n",
            "\n",
            "3. Inverse Document Frequency (IDF) - A logarithmically scaled ratio used to diminish the weight of terms that appear frequently across multiple documents and boost the weight of terms that are rare in the document collection.\n",
            "\n",
            "4. TF-IDF Weighting - A method to evaluate how important a word is to a document in a collection, combining term frequency (TF) and inverse document frequency (IDF).\n",
            "\n",
            "5. BM25 - An advanced ranking function used in text retrieval that incorporates term frequency, inverse document frequency, and document length normalization to calculate the relevance score of a document for a given query.\n",
            "\n",
            "6. Inverted Index - A data structure used in search engines to store a mapping from content, such as words or phrases, to its locations in a database file, a document, or a set of documents, enabling fast full-text searches.\n",
            "\n",
            "7. Tokenization - The process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens to enable further text processing like indexing or search.\n",
            "\n",
            "8. Document Length Normalization - An adjustment made to a term's significance in a document based on the length of the document, ensuring that longer documents do not have an undue advantage in matching queries.\n",
            "\n",
            "9. Pivoted Length Normalization - A document length normalization technique that uses the average document length as a pivot point for adjusting a term's weight based on document length.\n",
            "\n",
            "10. Indexer - A system component that processes text documents to create an index that allows efficient retrieval of documents by content.\n",
            "\n",
            "11. Scorer - Part of a retrieval system that takes a user's query and the index, calculates the relevance of documents in the collection, and ranks them according to their scores.\n",
            "\n",
            "12. Feedback Mechanism - A component of a retrieval system that utilizes user interactions, such as clicks and views, to adjust and improve the relevance of search results over time.\n",
            "\n",
            "\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): Why do we need to perform sublinear term frequency (TF) transformation in the vector space model?\n",
            "Invalid choice. Please enter 1, 2, or 3.\n",
            "1. Get Guiding Questions\n",
            "2. Ask a Question\n",
            "3. Quit\n",
            "Enter your choice (1-3): 3\n",
            "Exiting the program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VQyzJaq1hCAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}