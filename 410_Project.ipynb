{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM91fB/l7bpVpcBkBJkCted",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eslqian/410Project/blob/task%2Frm%2FstudyGuideHelperPhase1/410_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 410 Project: Part 1 - Finding best document to answer guiding question"
      ],
      "metadata": {
        "id": "rbpqJzSiJ4Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 Unzip transcript files to use"
      ],
      "metadata": {
        "id": "e8KPe43cJ8-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Replace 'example.zip' with your actual zip file path\n",
        "zip_path = '/content/Week7.zip'\n",
        "extract_path = '/content/'\n",
        "\n",
        "# Create a directory to extract to\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Extracted to {extract_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IskPtkxjJhQJ",
        "outputId": "2a514682-ab1d-4d8a-8dd2-8847acc57a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted to /content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View the overview questions"
      ],
      "metadata": {
        "id": "PtaHWPcKKISW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDhyA0ocIzco",
        "outputId": "832db622-efb1-4d35-d9dd-e8e381e0d820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goals and Objectives:\n",
            "- Explain some basic concepts in natural language processing.\n",
            "- Explain different ways to represent text data.\n",
            "- Explain the two basic types of word associations and how to mine paradigmatic relations from text data.\n",
            "\n",
            "Guiding Questions:\n",
            "- What does a computer have to do in order to understand a natural language sentence?\n",
            "- What is ambiguity?\n",
            "- Why is natural language processing (NLP) difficult for computers?\n",
            "- What is bag-of-words representation?\n",
            "- Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "- What is a paradigmatic relation?\n",
            "- What is a syntagmatic relation?\n",
            "- What is the general idea for discovering paradigmatic relations from text?\n",
            "- What is the general idea for discovering syntagmatic relations from text?\n",
            "- Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "- How does BM25 Term Frequency transformation work?\n",
            "- Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "\n",
            "Key Phrases and Concepts:\n",
            "- Part of speech tagging\n",
            "- Syntactic analysis\n",
            "- Semantic analysis\n",
            "- Ambiguity\n",
            "- Text representation, especially bag-of-words representation\n",
            "- Context of a word; context similarity\n",
            "- Paradigmatic relation\n",
            "- Syntagmatic relation\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "# Define the path to your overview file\n",
        "overview_file = \"/content/week7_overview.txt\"\n",
        "\n",
        "# Check if the file exists before proceeding\n",
        "if os.path.exists(overview_file):\n",
        "    with open(overview_file, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Helper function to extract content between two headings\n",
        "    def extract_content(heading, text, next_heading=None):\n",
        "        # Define pattern to capture all text after the heading until the next heading\n",
        "        if next_heading:\n",
        "            pattern = re.compile(rf\"{heading}\\n\\n(.*?)\\n{next_heading}\", re.DOTALL)\n",
        "        else:  # if it is the last section\n",
        "            pattern = re.compile(rf\"{heading}\\n\\n(.*?)(?=\\n[A-Z]\\n|$)\", re.DOTALL)\n",
        "        match = pattern.search(text)\n",
        "        content = match.group(1).strip() if match else \"\"\n",
        "        # Split content into a list by newlines and ignore empty lines\n",
        "        return [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "\n",
        "    # Extract each section\n",
        "    goals_and_objectives = extract_content(\"Goals and Objectives\", text, \"Guiding Questions\")\n",
        "    guiding_questions = extract_content(\"Guiding Questions\", text, \"Key Phrases and Concepts\")\n",
        "    key_phrases_and_concepts = extract_content(\"Key Phrases and Concepts\", text)\n",
        "\n",
        "    # Print the extracted sections\n",
        "    print(\"Goals and Objectives:\")\n",
        "    for item in goals_and_objectives:\n",
        "        print(\"-\", item)\n",
        "    print(\"\\nGuiding Questions:\")\n",
        "    for item in guiding_questions:\n",
        "        print(\"-\", item)\n",
        "    print(\"\\nKey Phrases and Concepts:\")\n",
        "    for item in key_phrases_and_concepts:\n",
        "        print(\"-\", item)\n",
        "else:\n",
        "    print(f\"The file at {overview_file} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For each question, determine the documents that are most likely to contain an answer to one of the questions from the overview"
      ],
      "metadata": {
        "id": "sjpoGmh5KOYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Assume extract_content is defined elsewhere\n",
        "\n",
        "transcripts_folder = \"/content/Week7\"\n",
        "overview_file = \"/content/week7_overview.txt\"\n",
        "\n",
        "# Load the content from the overview file\n",
        "with open(overview_file, 'r') as file:\n",
        "    overview_text = file.read()\n",
        "\n",
        "# Extract questions and key concepts\n",
        "guiding_questions = extract_content(\"Guiding Questions\", overview_text, \"Key Phrases and Concepts\")\n",
        "key_phrases_and_concepts = extract_content(\"Key Phrases and Concepts\", overview_text)\n",
        "\n",
        "# Combine the questions and key concepts\n",
        "queries = guiding_questions + key_phrases_and_concepts\n",
        "\n",
        "# Read all transcripts and store them in a list\n",
        "documents = []\n",
        "document_names = []\n",
        "for filename in os.listdir(transcripts_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(transcripts_folder, filename), 'r') as file:\n",
        "            documents.append(file.read())\n",
        "            document_names.append(filename)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Convert the documents to a matrix of TF-IDF features\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Define a function to find the top three matching documents for each query\n",
        "def find_top_documents(query, top_n=3):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]  # Get the indices of top matches\n",
        "    return [(document_names[i], cosine_similarities[i]) for i in top_indices]\n",
        "\n",
        "# Find and print the top three documents for each query\n",
        "for query in queries:\n",
        "    top_documents = find_top_documents(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    for doc, score in top_documents:\n",
        "        print(f\"Matching document: {doc} with score {score:.4f}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfMOtPb7J3WG",
        "outputId": "7796b5f7-04eb-4964-bd98-aa51dfa5e0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What does a computer have to do in order to understand a natural language sentence?\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.4608\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.1435\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.1224\n",
            "\n",
            "\n",
            "Query: What is ambiguity?\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.1418\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.0000\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.0000\n",
            "\n",
            "\n",
            "Query: Why is natural language processing (NLP) difficult for computers?\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.3344\n",
            "Matching document: 7_4_Natural_Language_Content_Analysis_Part_2.txt with score 0.2958\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.1379\n",
            "\n",
            "\n",
            "Query: What is bag-of-words representation?\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.2683\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.2201\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.1865\n",
            "\n",
            "\n",
            "Query: Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.3520\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.2775\n",
            "Matching document: 7_4_Natural_Language_Content_Analysis_Part_2.txt with score 0.1438\n",
            "\n",
            "\n",
            "Query: What is a paradigmatic relation?\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.3068\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.0868\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.0365\n",
            "\n",
            "\n",
            "Query: What is a syntagmatic relation?\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.3004\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.0710\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.0221\n",
            "\n",
            "\n",
            "Query: What is the general idea for discovering paradigmatic relations from text?\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.2688\n",
            "Matching document: 7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.1538\n",
            "Matching document: 7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.1524\n",
            "\n",
            "\n",
            "Query: What is the general idea for discovering syntagmatic relations from text?\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.2734\n",
            "Matching document: 7_2_Overview_Text_Mining_And_Analytics_Part_2.txt with score 0.1477\n",
            "Matching document: 7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.1464\n",
            "\n",
            "\n",
            "Query: Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.3544\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.2697\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.0980\n",
            "\n",
            "\n",
            "Query: How does BM25 Term Frequency transformation work?\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.3513\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.0450\n",
            "Matching document: 7_1_Overview_Text_Mining_And_Analytics_Part_1.txt with score 0.0114\n",
            "\n",
            "\n",
            "Query: Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.3053\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.2290\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.0851\n",
            "\n",
            "\n",
            "Query: Part of speech tagging\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.1432\n",
            "Matching document: 7_4_Natural_Language_Content_Analysis_Part_2.txt with score 0.0662\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.0489\n",
            "\n",
            "\n",
            "Query: Syntactic analysis\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.2430\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.1766\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.1394\n",
            "\n",
            "\n",
            "Query: Semantic analysis\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.2080\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.1749\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.1537\n",
            "\n",
            "\n",
            "Query: Ambiguity\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.1418\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.0000\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.0000\n",
            "\n",
            "\n",
            "Query: Text representation, especially bag-of-words representation\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.3429\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.3160\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.1369\n",
            "\n",
            "\n",
            "Query: Context of a word; context similarity\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.6101\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.3079\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.2559\n",
            "\n",
            "\n",
            "Query: Paradigmatic relation\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.3068\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.0868\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.0365\n",
            "\n",
            "\n",
            "Query: Syntagmatic relation\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.3004\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.0710\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.0221\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Question Generation"
      ],
      "metadata": {
        "id": "_IsaY7UFLjnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Have GPT generate a list of guiding questions and key cncepts based of the video transcripts"
      ],
      "metadata": {
        "id": "m44_5a1bVSQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = # USE YOUR OWN API KEY PLS"
      ],
      "metadata": {
        "id": "SuBTZJsO0G9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Path to the directory with your transcript files\n",
        "transcripts_folder = \"/content/Week7\"\n",
        "\n",
        "pre_prompt = \"The following text is a part of a lecture transcript from a master-level text information course. Please provide a response in two sections. In the first section, called Guiding Questions, list down potential Guiding Questions and provide an answer to each as a numbered list in the format of Q1: {question} A1: {answer to Q1}. In the second section, called Key Concepts, identify Key Concepts and provide a definition for each as numbered list in the format of 1. {Term} - {Definition}: \"\n",
        "\n",
        "def query_chat_gpt(prompt):\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",  # Use the specific GPT-4 variant\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=4096  # Maximum output token limit\n",
        "    )\n",
        "    # Accessing the response content correctly\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def parse_questions_answers(text):\n",
        "    pattern = re.compile(r\"Q(\\d+): (.*?)\\nA\\1: (.+?)(?=\\nQ\\d+:|\\Z)\", re.DOTALL)\n",
        "    matches = pattern.findall(text)\n",
        "    questions_answers = [{\"Q\": match[1], \"A\": match[2]} for match in matches]\n",
        "    return questions_answers\n",
        "\n",
        "def parse_key_concepts(text):\n",
        "    pattern = re.compile(r\"(\\d+)\\. (.*?) - (.*?)(?=\\n\\d+\\. |$)\", re.DOTALL)\n",
        "    matches = pattern.findall(text)\n",
        "    key_concepts = [{\"id\": match[0], \"term\": match[1], \"definition\": match[2]} for match in matches]\n",
        "    return key_concepts\n",
        "\n",
        "# Store all guiding questions and key concepts\n",
        "all_guiding_questions = []\n",
        "all_key_concepts = []\n",
        "all_responses = []\n",
        "\n",
        "max_chunk_size = 10000\n",
        "\n",
        "# Process each transcript file in the directory\n",
        "for filename in os.listdir(transcripts_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(transcripts_folder, filename)\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()\n",
        "\n",
        "            # Break up into chunks that fit into the GPT-3 prompt size limit\n",
        "            chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "            for chunk in chunks:\n",
        "                prompt = pre_prompt + chunk\n",
        "\n",
        "                # Query GPT-3\n",
        "                response_text = query_chat_gpt(prompt)\n",
        "                all_responses.append(response_text)\n",
        "\n",
        "\n",
        "# Output the accumulated results\n",
        "print(\"Accumulated Guiding Questions and Answers:\")\n",
        "for qa in all_guiding_questions:\n",
        "    print(f\"Q{qa['Q']}:\\nA{qa['A']}\")\n",
        "\n",
        "print(\"\\nAccumulated Key Concepts:\")\n",
        "for concept in all_key_concepts:\n",
        "    print(f\"{concept['id']}. {concept['term']} - {concept['definition']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7OFfKMVKdWx",
        "outputId": "dcefef82-949c-4fb4-e062-0c07b2e7a13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated Guiding Questions and Answers:\n",
            "\n",
            "Accumulated Key Concepts:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse the guiding questions and answers"
      ],
      "metadata": {
        "id": "B71gx43xVYW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_key_concepts(text):\n",
        "    pattern = re.compile(r\"(\\d+)\\.\\s*(.*?)\\s*-\\s*(.+?)(?=\\n\\d+\\. |$)\", re.DOTALL)\n",
        "    matches = pattern.findall(text)\n",
        "    key_concepts = [{\"id\": match[0], \"term\": match[1].strip(), \"definition\": match[2].strip()} for match in matches]\n",
        "    return key_concepts\n",
        "def parse_questions_answers(text):\n",
        "    # This pattern will match questions in the format \"Q1:\" or \"Q:\" and answers in the format \"A1:\" or \"A:\"\n",
        "    pattern = re.compile(r\"Q(\\d+)?:\\s*(.*?)\\s*A\\1?:\\s*(.+?)(?=\\nQ(\\d+)?:|\\Z)\", re.DOTALL)\n",
        "    matches = pattern.findall(text)\n",
        "    questions_answers = [{\"Q\": match[1].strip(), \"A\": match[2].strip()} for match in matches]\n",
        "    return questions_answers\n",
        "\n",
        "# I'll assume 'all_responses' is a list of strings, each string being a response from the model.\n",
        "all_guiding_questions = []\n",
        "all_key_concepts = []\n",
        "\n",
        "for response_text in all_responses:\n",
        "    # Split the response into the 'Guiding Questions' and 'Key Concepts' sections\n",
        "    guiding_questions_section = re.search(r\"Guiding Questions:(.*?)\\n\\n\", response_text, re.DOTALL)\n",
        "    key_concepts_section = re.search(r\"Key Concepts:(.*)\", response_text, re.DOTALL)\n",
        "\n",
        "    # Extract the questions and answers\n",
        "    questions_answers = parse_questions_answers(guiding_questions_section.group(1)) if guiding_questions_section else []\n",
        "    # Extract the key concepts\n",
        "    key_concepts = parse_key_concepts(key_concepts_section.group(1)) if key_concepts_section else []\n",
        "\n",
        "    # Accumulate the questions and concepts\n",
        "    all_guiding_questions.extend(questions_answers)\n",
        "    all_key_concepts.extend(key_concepts)\n",
        "\n",
        "# Output the accumulated results\n",
        "print(\"Accumulated Guiding Questions and Answers:\")\n",
        "for qa in all_guiding_questions:\n",
        "    print(f\"Q) {qa['Q']}:\\nA) {qa['A']}\")\n",
        "\n",
        "print(\"\\nAccumulated Key Concepts:\")\n",
        "for concept in all_key_concepts:\n",
        "    print(f\"{concept['id']}. {concept['term']} - {concept['definition']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3iOz08YObVy",
        "outputId": "ba751086-4fb6-4359-ead6-8ffb57659ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated Guiding Questions and Answers:\n",
            "Q) What is the main focus of text mining?:\n",
            "A) The main focus of text mining is to turn text data into actionable knowledge.\n",
            "Q) What is the Expected Overlap of Words in Context method?:\n",
            "A) It is a method that represents each context with a word vector based on the probability of a word in the context and measures similarity using the product of the probabilities.\n",
            "Q) What is the difference between text mining and text analytics?:\n",
            "A) Text mining and text analytics are used interchangeably, but there is a subtle difference in emphasis. Text mining focuses on the process, while text analytics emphasizes the result or the problem at hand.\n",
            "2. Q2: What is the main goal of text mining and text analytics?\n",
            "   A2: The main goal is to turn text data into high-quality information or actionable knowledge.\n",
            "3. Q3: What is the difference between high-quality information and actionable knowledge?\n",
            "   A3: High-quality information refers to concise information that is easier for humans to digest than raw text data. Actionable knowledge, on the other hand, is knowledge that can be used to make decisions or take actions.\n",
            "4. Q4: How does text retrieval relate to text mining?\n",
            "   A4: Text retrieval is an essential component in many text mining systems. It helps find relevant information from a large amount of text data, which can then be used for text mining.\n",
            "5. Q5: What is the relationship between text mining and text retrieval?\n",
            "   A5: Text retrieval can serve as a preprocessor for text mining, helping to identify the most relevant text data for solving a particular problem. Text retrieval is also needed for knowledge provenance and verification in text mining.\n",
            "Q) What are the two types of word relations?:\n",
            "A) The two types of word relations are paradigmatic and syntagmatic relations.\n",
            "2. Q2: How do words have a paradigmatic relation?\n",
            "A2: Words have a paradigmatic relation if they can be substituted for each other without affecting the understanding of the sentence, indicating that they are in the same semantic or syntactic class.\n",
            "3. Q3: What is a syntagmatic relation?\n",
            "A3: A syntagmatic relation is when two words can be combined with each other in a sentence because they are semantically related.\n",
            "4. Q4: What is the difference between paradigmatic and syntagmatic relations?\n",
            "A4: Paradigmatic relations focus on words that can be substituted for each other, while syntagmatic relations focus on words that can be combined with each other.\n",
            "5. Q5: How can word associations be useful in NLP tasks?\n",
            "A5: Word associations can improve the accuracy of NLP tasks, assist in grammar learning, and enhance text retrieval and mining applications.\n",
            "Q) What are paradigmatic and syntagmatic relations?:\n",
            "A) Paradigmatic relations are relationships between words that are typically associated with each other, such as synonyms or antonyms. Syntagmatic relations refer to the way words are used together in a sentence or within a specific context.\n",
            "2. Q2: How are paradigmatic and syntagmatic relations related?\n",
            "   A2: Paradigmatic and syntagmatic relations are closely related because words that are paradigmatically related often have a syntagmatic relation with the same word. In other words, words that are associated with each other in terms of meaning (paradigmatic relation) also tend to be used together in similar contexts (syntagmatic relation).\n",
            "Q) What is the definition of paradigmatically related words?:\n",
            "A) Words are considered paradigmatically related if they share a similar context and occur in similar positions in text.\n",
            "Q) What are the basic steps a computer needs to go through in order to understand a sentence?:\n",
            "A) The computer needs to know the words, segment the words, determine the syntactical categories of the words, analyze the relationships between the words, and map the phrases and structures into real-world entities.\n",
            "Q) What are the different levels of text representation?:\n",
            "A) The different levels of text representations mentioned are stream text, word-based representation, syntactic structures, entities and relations, and logical predicates.\n",
            "2. Q2: What are some analysis techniques enabled by each level of text representation?\n",
            "   A2: \n",
            "   - Q2.1: What analysis techniques can be used for stream text?\n",
            "     A2.1: Stream processing algorithms can be used for stream text.\n",
            "   - Q2.2: What analysis techniques can be used for word-based representation?\n",
            "     A2.2: Word relation analysis, topic analysis, sentiment analysis, thesaurus discovery, and topic and opinion-related applications.\n",
            "   - Q2.3: What analysis techniques can be used for syntactic structures?\n",
            "     A2.3: Syntactical graph analysis, graph mining algorithms, and stylistic analysis.\n",
            "   - Q2.4: What analysis techniques can be used for entities and relations?\n",
            "     A2.4: Knowledge graph analysis, information network analysis, and discovery of knowledge and opinions about real-world entities.\n",
            "   - Q2.5: What analysis techniques can be used for logical predicates?\n",
            "     A2.5: Large inference, integration of scattered knowledge, and making inferences using ontology.\n",
            "3. Q3: What are some example applications for each level of representation?\n",
            "   A3:\n",
            "   - Q3.1: What are some example applications for stream text representation?\n",
            "     A3.1: Text compression.\n",
            "   - Q3.2: What are some example applications for word-based representation?\n",
            "     A3.2: Thesaurus discovery, topic and opinion-related applications, understanding major topics in research literature, mining customer complaints from emails, and understanding consumers' opinions about products.\n",
            "   - Q3.3: What are some example applications for syntactic structures?\n",
            "     A3.3: Stylistic analysis and classification of text objects based on syntactic structures.\n",
            "   - Q3.4: What are some example applications for entities and relations?\n",
            "     A3.4: Discovery of knowledge and opinions about real-world entities and integration of information from various sources.\n",
            "   - Q3.5: What are some example applications for logical predicates?\n",
            "     A3.5: Knowledge assistants, making inferences about gene functions based on extracted information from literature.\n",
            "\n",
            "Accumulated Key Concepts:\n",
            "1. Text Mining - The process of extracting useful and actionable knowledge from text data.\n",
            "2. Observed World - The view or perspective of a human observer on the real world, which can be represented in text data.\n",
            "3. Content Mining - The process of mining the content of text data to extract high-quality information about a specific aspect of the observed world.\n",
            "4. Knowledge About the Observer - Using text data to infer properties of the person who observed the world, such as mood or sentiment.\n",
            "5. Predictive Analytics - The use of text mining and other techniques to predict the value of certain real-world variables.\n",
            "6. Non - Text Data - Data that is not in the form of text, such as numerical data or images, which can be used in conjunction with text data for analysis and prediction.\n",
            "7. Context - Sensitive Analysis - Analyzing text data in different contexts, such as time periods or locations, to gain interesting perspectives and insights.\n",
            "1. Expected Overlap of Words in Context - a method that represents contexts using word vectors based on word probabilities and measures similarity by the product of these probabilities.\n",
            "2. Sublinear Transformation - a transformation of term frequency, such as logarithmic or BM25, that reduces the emphasis on high-frequency terms and prevents them from dominating the similarity calculation.\n",
            "3. BM25 Transformation - a specific sublinear transformation commonly used in text retrieval that allows for a varying level of emphasis on term frequency by adjusting a parameter called k.\n",
            "4. IDF (Inverse Document Frequency) - a term weighting measure in text retrieval that assigns higher weights to words that occur less frequently in the document collection.\n",
            "5. Document Vector - a vector representation of a context or document that contains elements representing the weights or probabilities of each term, often calculated using transformations and term weighting measures.\n",
            "6. Syntagmatic Relations - word associations or relations between words based on their co-occurrence in a context, which can be discovered using the methods discussed in the lecture.\n",
            "1. Text mining - The process of converting text data into high-quality information or actionable knowledge.\n",
            "2. Text analytics - The same as text mining, emphasizing the result or problem at hand rather than the process.\n",
            "3. High - quality information - More concise information about a topic that is easier for humans to understand than raw text data.\n",
            "4. Actionable knowledge - Knowledge that can be used to make decisions or take actions.\n",
            "5. Text retrieval - Finding relevant information from a large amount of text data.\n",
            "6. Knowledge provenance - Verifying patterns or knowledge discovered from text mining by referring back to the original text data.\n",
            "7. Data mining - The process of turning data, including text data, into actionable knowledge.\n",
            "8. Mining algorithms - Algorithms used to mine different types of data, including text data. Specialized algorithms are often developed for mining text data.\n",
            "1. Paradigmatic relation - A word relation in which two words can be substituted for each other without affecting the understanding of the sentence, indicating they belong to the same semantic or syntactic class.\n",
            "2. Syntagmatic relation - A word relation in which two words can be combined with each other because they are semantically related.\n",
            "3. Context similarity - A measure of how similar the context of two words is, used to determine paradigmatic relations.\n",
            "4. Co - occurrence - The frequency with which two words occur together in a specific context, used to determine syntagmatic relations.\n",
            "1. Paradigmatic relation - A relationship between words that are typically associated or connected in terms of meaning, such as synonyms, antonyms, or words belonging to the same semantic field.\n",
            "2. Syntagmatic relation - The way words are used together in a sentence or within a specific context. It refers to the sequential and grammatical relationships between words.\n",
            "1. Paradigmatically related words - Words that share a similar context and occur in similar positions in text.\n",
            "2. Left context - Words that occur before a specific word.\n",
            "3. Right context - Words that occur after a specific word.\n",
            "4. Window of text - A defined length of text around a specific word.\n",
            "5. Similarity function - A function used to measure the similarity between two vectors representing context. In this case, the dot product of the vectors is used.\n",
            "6. Vector space model - A model that represents documents or context as vectors in a high dimensional space, where each dimension corresponds to a word in the vocabulary.\n",
            "7. EOWC (Extracted Overlap of Words in Context) - An approach to representing context as word vectors and defining similarity based on the expected overlap of words.\n",
            "1. Word matching - A technique used in text information retrieval that involves comparing the occurrence of specific words between a query and a document.\n",
            "2. Strong evidence - An indication in text information retrieval that suggests a high degree of relevance between a query and a document.\n",
            "3. Word frequency - The number of times a word appears in a text or document.\n",
            "1. Natural language processing - The field of computer science and artificial intelligence that focuses on enabling computers to understand and process human language.\n",
            "2. Lexical analysis - The process of tagging words with their syntactic categories, often referred to as part-of-speech tagging.\n",
            "3. Syntactical parsing - The process of determining the relationships between words in a sentence and creating a parse tree that represents the structure of the sentence.\n",
            "4. Semantics - The branch of linguistic and logic that deals with meaning, including mapping phrases and structures into real-world entities and connecting them to create meaning.\n",
            "5. World - level ambiguity - The ambiguity that arises when a word can have multiple meanings depending on the context, making it difficult for computers to determine the correct interpretation.\n",
            "6. Syntactic ambiguity - The ambiguity that arises when a sentence can have different interpretations in terms of its structure.\n",
            "7. Anaphora resolution - The process of resolving ambiguous references to pronouns or other entities, determining which entity a pronoun or phrase refers to.\n",
            "8. Presupposition - The assumption or knowledge that needs to be present for understanding certain language constructs, such as understanding that \"He has quit smoking\" implies that he smoked before.\n",
            "9. Inference - The process of deriving new information or conclusions from existing information or premises.\n",
            "10. Speech act analysis - The analysis of the speaker's intention and purpose in saying a sentence, including the actions being taken through language.\n",
            "11. Accuracy - A measure of how well a natural language processing task is performed, often expressed as a percentage indicating the proportion of correctly performed tasks.\n",
            "1. String of characters - A representation of text data where a sentence is stored as a sequence of characters.\n",
            "2. Sequence of words - A representation of text data where a sentence is divided into individual words.\n",
            "3. Part - of-speech tags - A representation of text data where each word in a sentence is labeled with its grammatical category.\n",
            "4. Syntactic structure - The arrangement and organization of words in a sentence according to a specific grammatical structure.\n",
            "5. Entities and relations - Recognizing and representing the entities (e.g., people, locations) and relations between them in a sentence.\n",
            "6. Logical conditions - The use of predicates and inference rules to derive logical conclusions from the text data.\n",
            "7. Speech acts - Understanding the intention and purpose behind a sentence, such as whether it is a statement, question, or request.\n",
            "1. Stream text - Text that can only be processed by stream processing algorithms.\n",
            "2. Word - based representation - Representing text using individual words as basic units, enabling various analysis techniques such as word relation analysis, topic analysis, and sentiment analysis.\n",
            "3. Syntactic structures - Adding structure to text analysis by incorporating syntactical graphs, enabling syntactical graph analysis, and stylistic analysis.\n",
            "4. Entities and relations - Adding entities and their relations to text analysis, enabling analysis techniques like knowledge graph analysis and discovery of knowledge and opinions about real-world entities.\n",
            "5. Logical predicates - Adding logical predicates to text analysis for large inference and integration of scattered knowledge, enabling applications like knowledge assistants and making inferences from extracted information.\n",
            "6. Text representation - Different ways of representing text, such as strings, words, syntactic structures, entity-relation graphs, and logical predicates, determining the type of mining algorithms that can be applied.\n",
            "7. Combination of representations - The idea of combining different levels of text representation to enable richer and more powerful analysis in real applications.\n",
            "8. Word - based representation techniques - Techniques that are general, robust, and widely used in various text mining applications due to their applicability to any natural language, minimal manual effort required, and effectiveness in representing semantics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For each question in the Overview Guiding Questions, have GPT generate answers using the top 3 documents as context"
      ],
      "metadata": {
        "id": "5ydPHvrXVdNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Your updated query_gpt4 function\n",
        "def query_gpt4(prompt):\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",  # Use the specific GPT-4 variant\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=4096  # Maximum output token limit\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Function to construct prompt with context and query GPT-4\n",
        "def ask_gpt_with_context(question, context_documents, document_names):\n",
        "    # Combine the question with context documents\n",
        "    prompt = f\"Question: {question}\\n\\n\"\n",
        "    prompt += \"Context:\\n\"\n",
        "    for doc, name in zip(context_documents, document_names):\n",
        "        prompt += f\"Document: {name}\\n{doc}\\n\\n\"\n",
        "\n",
        "    # Truncate the prompt if it's too long\n",
        "    max_length = 128000  # max context token limit for GPT-4\n",
        "    if len(prompt) > max_length:\n",
        "        prompt = prompt[-max_length:]\n",
        "\n",
        "    # Query GPT-4 and return the response\n",
        "    return query_gpt4(prompt)\n",
        "\n",
        "# Store the outputs in a list\n",
        "outputs = []\n",
        "\n",
        "# Process each guiding question\n",
        "for query in guiding_questions:\n",
        "    top_documents = find_top_documents(query)\n",
        "    top_doc_names = [doc_name for doc_name, _ in top_documents]\n",
        "    top_doc_contents = [documents[document_names.index(doc_name)] for doc_name in top_doc_names]\n",
        "    answer = ask_gpt_with_context(query, top_doc_contents, top_doc_names)\n",
        "\n",
        "    # Store the output\n",
        "    output = {\n",
        "        \"question\": query,\n",
        "        \"documents_used\": top_doc_names,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "    outputs.append(output)\n",
        "\n",
        "    # Print the output\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Documents used: {', '.join(top_doc_names)}\")\n",
        "    print(\"Answer:\", answer)\n",
        "    print(\"-------------------------------------------------------------------------------------------------------------\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-iHECDMSLh4",
        "outputId": "61797a87-f48d-4733-cbc2-fb26c9377920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What does a computer have to do in order to understand a natural language sentence?\n",
            "Documents used: 7_3_Natural_Language_Content_Analysis_Part_1.txt, 7_5_Text_Representation_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: A computer must perform several steps to understand a natural language sentence. These steps include:\n",
            "\n",
            "1. **Word Segmentation**: Identifying individual words in a sentence, which can usually be done by looking for spaces in languages like English.\n",
            "\n",
            "2. **Lexical Analysis**: Determining the syntactical categories (parts of speech) of each word, such as nouns, verbs, adjectives, etc. This step is also known as part-of-speech tagging.\n",
            "\n",
            "3. **Syntactical Parsing**: Analyzing the structure of the sentence to determine how words and phrases are related. This step involves identifying noun phrases, verb phrases, prepositional phrases, etc., and how they connect to convey meaning. The result is a parse tree that shows the sentence's syntactical structure.\n",
            "\n",
            "4. **Semantic Analysis**: Mapping phrases and sentence structures to concepts and entities in the real world. Computers must formally represent these entities by using symbols. For instance, representing that \"d1 is a dog\" or \"b1 is a boy\" and identifying actions as predicates like \"chasing(d1, b1, p1)\" where d1, b1, and p1 would refer to the dog, boy, and playground, respectively.\n",
            "\n",
            "5. **Inference**: Making logical deductions based on additional knowledge. For example, if there's a rule stating that being chased can make someone scared, the computer might infer that the boy in the sentence could be scared.\n",
            "\n",
            "6. **Pragmatic Analysis**: Determining the speaker's intent or the purpose of the sentence. This involves understanding the context and the implication of the speech act, such as making a request or offering a suggestion.\n",
            "\n",
            "These steps are part of Natural Language Processing (NLP) and are crucial for a computer to progress from merely recognizing characters and words to interpreting the semantics and pragmatics of a sentence. However, NLP is challenging because languages are designed for human efficiency and often rely on common sense and the ability to handle ambiguities—capabilities that computers do not naturally possess. As a result, while computers can perform these steps to a certain extent, they often struggle to fully understand natural language sentences as humans do, due to limitations in common sense reasoning and disambiguation.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is ambiguity?\n",
            "Documents used: 7_3_Natural_Language_Content_Analysis_Part_1.txt, 7_6_Text_Representation_Part_2.txt, 7_5_Text_Representation_Part_1.txt\n",
            "Answer: Ambiguity in natural language refers to words, phrases, or sentences that have multiple possible meanings or interpretations. In the context provided, ambiguity is discussed as a significant challenge for natural language processing (NLP) because computers do not have common sense knowledge that humans do, which allows us to disambiguate words based on the context. As a result, when a word can have several meanings, a computer may struggle to determine the correct one for a given situation. Ambiguity may occur at various levels, including:\n",
            "\n",
            "1. Lexical or word-level ambiguity: A word has multiple meanings (e.g., \"root\" can refer to a part of a plant or a mathematical concept).\n",
            "\n",
            "2. Syntactic ambiguity: A sentence can be parsed in different ways, leading to different interpretations (e.g., \"I saw the man with the telescope\" could mean either that \"I\" used a telescope to see the man or the man had a telescope).\n",
            "\n",
            "3. Anaphora resolution: It is unclear to which antecedent a pronoun refers (e.g., \"John persuaded Bill to buy a TV for himself\" could mean the TV is for John or Bill).\n",
            "\n",
            "4. Presupposition: Making assumptions about prior knowledge (e.g., \"He has quit smoking\" presupposes he used to smoke).\n",
            "\n",
            "Ambiguity is described as the \"main killer\" in natural language processing because it introduces multiple choices at various steps of interpreting language, making it difficult for computers to make the right decisions without common sense reasoning, which they currently lack.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why is natural language processing (NLP) difficult for computers?\n",
            "Documents used: 7_3_Natural_Language_Content_Analysis_Part_1.txt, 7_4_Natural_Language_Content_Analysis_Part_2.txt, 7_5_Text_Representation_Part_1.txt\n",
            "Answer: Natural Language Processing (NLP) is difficult for computers because it is designed for efficient human communication, which involves complexities that are challenging for machines to replicate or understand. Some of the key difficulties include:\n",
            "\n",
            "1. **Ambiguity**: Natural language is inherently ambiguous. A single word can have multiple meanings, and sentences can be interpreted in different ways depending on context. Computers lack the ability to easily disambiguate these meanings without additional information.\n",
            "\n",
            "2. **Common Sense Knowledge**: Humans use a vast amount of unspoken common sense knowledge when interpreting language, which is not explicitly stated in the communication. Computers do not possess this kind of background knowledge, making it hard for them to infer unstated information or understand implied meanings.\n",
            "\n",
            "3. **Complex Grammar and Syntax**: Human languages have complex grammatical structures and syntax rules that can be tough for computers to parse. Understanding the relationships between words in a sentence (such as which nouns are connected to which verbs) requires parsing sentences accurately, which can be error-prone for machines.\n",
            "\n",
            "4. **Variety of Expressions**: There are many ways to express the same idea in natural language, including idioms, metaphors, and variations in dialect or individual expression. Capturing all these nuances is challenging for computers.\n",
            "\n",
            "5. **Contextual Understanding**: Language is context-dependent. The meaning of a sentence can change dramatically depending on the speaker's intent, the situation, and the prior conversation. Computers need mechanisms to track and integrate this contextual information to understand language.\n",
            "\n",
            "6. **Anaphora and Presupposition Resolution**: Words like \"he\" or \"it\" refer back to earlier mentioned entities, and sentences can contain presuppositions that require outside knowledge to understand. Resolving these references and presuppositions is a complex task for computers.\n",
            "\n",
            "7. **Pragmatics**: Beyond just understanding the literal meaning of sentences, human language users grasp the pragmatic aspects, such as the speaker's intent or the socially appropriate response in conversation. This level of understanding is still beyond the capabilities of most NLP systems.\n",
            "\n",
            "Because of these challenges, even tasks considered simple like part-of-speech tagging cannot achieve 100% accuracy. Deeper understanding, which requires integrating all levels of language analysis from syntax to pragmatics with common sense reasoning, remains an area of active research and development. Current approaches in NLP rely on statistical models, machine learning, and human collaboration for handling text data, often opting for shallow but scalable methods over deeper, more nuanced analyses that are currently less feasible at a large scale.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is bag-of-words representation?\n",
            "Documents used: 7_5_Text_Representation_Part_1.txt, 7_6_Text_Representation_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt\n",
            "Answer: Bag-of-words representation is a simplistic yet effective way of representing text data in the field of natural language processing. It involves treating text as an unordered collection of words, disregarding grammar and word order but keeping multiplicity. \n",
            "\n",
            "The main idea is to create a \"bag\" filled with distinct words found in the text, and then count the frequency of each word's appearance. These word frequencies can be used as features for various machine learning algorithms.\n",
            "\n",
            "For example, considering the sentence:\n",
            "- \"The cat sat on the mat.\"\n",
            "\n",
            "A bag-of-words representation (assuming binary presence or absence rather than frequency) might look like:\n",
            "- {the, cat, sat, on, mat}\n",
            "\n",
            "If we consider frequency, it might be represented as:\n",
            "- {the: 2, cat: 1, sat: 1, on: 1, mat: 1}\n",
            "\n",
            "Note that the words \"the\" and \"on\" are used only once in this context despite the former appearing twice in the original sentence, as the bag-of-words model does not keep track of word positions or syntax; it only cares about the words used and their counts in the document.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "Documents used: 7_6_Text_Representation_Part_2.txt, 7_5_Text_Representation_Part_1.txt, 7_4_Natural_Language_Content_Analysis_Part_2.txt\n",
            "Answer: The word-based representation is considered more robust than representations derived from syntactic and semantic analysis of text for several reasons:\n",
            "\n",
            "1. Generality and Applicability: Word-based representation is applicable to any natural language text, making it very general. This approach doesn't rely on the detailed grammar of a language, which can vary significantly between languages and can be difficult to model accurately.\n",
            "\n",
            "2. Simplicity: It requires less manual effort or sometimes no manual effort at all. Therefore, you can apply it directly to any application without the need for extensive pre-processing or manual annotation.\n",
            "\n",
            "3. Surprisingly Powerful: Despite being relatively simple, word-based techniques are surprisingly powerful and effective for many applications. This is because words are the basic units of human communication in natural language, and they carry substantial meaning that can be levered in text analysis, such as determining topics, sentiment, etc.\n",
            "\n",
            "4. Less Fragile: Word-based representation does not depend on the successful parsing of sentences into their syntactical structure, which can be error-prone and require sophisticated algorithms, especially in languages where word boundaries are not obvious (like in Chinese).\n",
            "\n",
            "5. Shallow Analysis: The course described in the transcript focuses on statistical methods that are generally shallow but robust analyses. In contrast, deep syntactic or semantic analyses tend to be less general because they require more assumptions about the text structure, are more domain-specific, and can fail when confronted with sentences that deviate from the modeled patterns.\n",
            "\n",
            "6. Human Interpretability: Even though word-based approaches may not capture all the nuances of language, what is extracted from word-level analysis can often be interpreted by humans who can then guide computers to perform more accurate analyses.\n",
            "   \n",
            "In summary, word-based representation is preferred for its general applicability to natural languages, requiring less manual effort and being robust enough for a wide range of practical text mining applications.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is a paradigmatic relation?\n",
            "Documents used: 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt\n",
            "Answer: A paradigmatic relation is a type of word association where two words can substitute for each other in a similar context without affecting the overall meaning of a sentence. These words are typically from the same semantic or syntactic class. \n",
            "\n",
            "The idea is that if you have two words, such as \"cat\" and \"dog,\" and by replacing one with the other the sentence still remains valid and meaningful, then they are said to have a paradigmatic relation. This concept is often used in linguistic analysis to understand how words in a language are related and can be replaced within sentences while still making sense. \n",
            "\n",
            "The paradigmatic relation contrasts with the syntagmatic relation, another fundamental word association where words are semantically related and typically combine with each other in a sentence (like \"cat\" and \"sit\"), but cannot be substituted for one another without altering the meaning or grammaticality of the sentence.\n",
            "\n",
            "Discovering paradigmatic relations is useful for various linguistic tasks and applications, such as expanding queries in information retrieval, understanding the structure of language, and learning grammar by categorizing words into syntactic classes.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is a syntagmatic relation?\n",
            "Documents used: 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt\n",
            "Answer: A syntagmatic relation in the context of linguistics and language studies refers to the combination or co-occurrence of words in a sequence within a sentence or a larger discourse. In other words, words that have a syntagmatic relation are those that can meaningfully and grammatically be put together to form a sequence in language use. These words are semantically and functionally related, and their relationship is typically defined by their adjacency or co-occurrence patterns within sentences.\n",
            "\n",
            "For example, in the sentence \"The quick brown fox jumps over the lazy dog,\" the words \"quick\" and \"brown\" have a syntagmatic relationship because they are adjacent and together modify the noun \"fox.\" Similarly, \"fox\" and \"jumps\" have a syntagmatic relationship because they co-occur in a subject-verb grammatical structure.\n",
            "\n",
            "In contrast to paradigmatic relations—where words can be substituted for one another without losing the meaning of the sentence—syntagmatic relations are about how words connect and function together within a sentence to convey meaning. Syntagmatic relations are essential for understanding the syntax and structure of language, as they show how words combine to form phrases and sentences.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is the general idea for discovering paradigmatic relations from text?\n",
            "Documents used: 7_7_Word_Association_Mining_And_Analysis.txt, 7_2_Overview_Text_Mining_And_Analytics_Part_2.txt, 7_1_Overview_Text_Mining_And_Analytics_Part_1.txt\n",
            "Answer: The general idea for discovering paradigmatic relations from text involves mining word associations by examining the contexts in which words occur. If two words can be substituted for each other without significantly changing the meaning of the sentence, they are said to have a paradigmatic relation. This indicates that they belong to the same semantic or syntactic class.\n",
            "\n",
            "To discover these relationships, one can employ methods such as:\n",
            "\n",
            "1. Observing Similar Contexts: By examining the words that occur in similar contexts to the target words, we can infer that words with similar contexts might be paradigmatically related. For example, the contexts in which 'cat' and 'dog' appear are likely to be similar because they both belong to the class of animals.\n",
            "\n",
            "2. Context Similarity Computation: Words are represented by their contexts, and the similarity between these contexts is computed. Words that have high context similarity are assumed to have paradigmatic relationships.\n",
            "\n",
            "3. Examining Correlated Occurrences: For syntagmatic relations, which are about words that can be combined in a sentence, we look at correlated occurrences. This involves identifying words that co-occur with a given word across various contexts. The goal is to find patterns of co-occurrence that suggest a syntagmatic relationship.\n",
            "\n",
            "By using these methods, text mining aims to discover hidden structures and relationships within language, enabling applications such as query expansion in text retrieval, the construction of browsable topic maps, and the analysis and summarization of opinions in reviews or discussions.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is the general idea for discovering syntagmatic relations from text?\n",
            "Documents used: 7_7_Word_Association_Mining_And_Analysis.txt, 7_2_Overview_Text_Mining_And_Analytics_Part_2.txt, 7_1_Overview_Text_Mining_And_Analytics_Part_1.txt\n",
            "Answer: The general idea for discovering syntagmatic relations from text is based on exploring the correlated occurrence of words within a given context. This context could be a sentence, paragraph, or even a whole document. The intuition is that if two words often co-occur within the same context, they have a semantic relationship and can combine meaningfully in a sentence.\n",
            "\n",
            "To discover these syntagmatic relations, the process involves looking at instances where a target word appears, and then identifying other words that frequently appear alongside it. By evaluating the co-occurrence of words relative to their individual occurrences, one can infer which words are semantically related and tend to be combined in the language.\n",
            "\n",
            "For instance, if you look at occurrences of the word \"eats,\" you would identify which words typically occur to the left (subjects like \"cat\" or \"dog\") and to the right (objects like \"meat\" or \"fish\") of it. If \"eats\" and \"meat\" often appear together more than with other unrelated words (like \"text\"), then it's assumed they have a syntagmatic relation. This would indicate a semantic association where one can meaningfully say, for example, \"The cat eats meat.\"\n",
            "\n",
            "In the documents provided in the context, these syntagmatic relations are contrasted with paradigmatic relations, which are about substitution. Words with paradigmatic relations can substitute for one another without altering the essential meaning or validity of a sentence, like \"Monday\" for \"Tuesday,\" or \"cat\" for \"dog.\" Syntagmatic relations, on the other hand, are about words that combine with each other to create meaningful phrases, even if they cannot substitute for one another, like \"cat\" and \"sit,\" or \"car\" and \"drive.\"\n",
            "\n",
            "Overall, the discovery of syntagmatic relations involves statistical analysis of word occurrences within text to reveal patterns of word combination and semantic association, which is fundamental for understanding language structure and use. This knowledge can then be applied to various Natural Language Processing (NLP) tasks and improve the accuracy of applications such as search engines, information retrieval, and opinion analysis.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "Documents used: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: Term Frequency Transformation (TF Transformation) is important when computing the similarity of context to address two specific problems inherent in using raw frequency counts of words in context.\n",
            "\n",
            "1. Emphasis on High-Frequency Terms: Raw frequency counts tend to favor matching one frequent term very well over matching many distinct terms, causing an overemphasis on terms that occur frequently. This means that a single commonly occurring term could overshadow the importance of matching multiple, potentially more meaningful, but less frequent terms. Using TF Transformation, such as logarithmic scaling or BM25 transformation, subdues the influence of very high-frequency counts and ensures that a favorable match is due to the similarity across a broader set of terms.\n",
            "\n",
            "2. Equal Treatment of All Words: Another issue is that raw frequency counts treat every word equally. Thus, common words like \"the\" would contribute equally to the similarity measure as content-specific words like \"eats.\" However, content words typically carry more significance and uniqueness than function words like \"the.\" By using techniques such as Inverse Document Frequency (IDF) weighting in combination with TF Transformation, the model can give more weight to rare terms, thus rewarding matches on less frequent, content-rich words over common ones.\n",
            "\n",
            "In summary, TF Transformation helps to balance the influence of term frequency in similarity computation by penalizing high-frequency terms and allowing for a more even distribution of significance across the words that make up the context. This leads to more accurate and meaningful similarity measures that reflect the actual semantic and contextual similarities between documents, contexts, or word associations.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: How does BM25 Term Frequency transformation work?\n",
            "Documents used: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_1_Overview_Text_Mining_And_Analytics_Part_1.txt\n",
            "Answer: BM25, also known as Okapi BM25, is a ranking function used by search engines to estimate the relevance of documents to a given search query. The term frequency (TF) transformation in BM25 is designed to address the limitations of the simple term frequency (count of how often a term appears in a document) by implementing a sublinear scaling of term frequency. This ensures that matching very frequent terms does not dominate the scoring of documents, and it also helps to reduce the importance of matching terms that are not particularly informative or are too common.\n",
            "\n",
            "Here's a step-by-step explanation of how the BM25 TF transformation works:\n",
            "\n",
            "1. **Raw Term Frequency**: Start with the raw term frequency (f), which is the number of times a term appears in a document.\n",
            "\n",
            "2. **Sublinear Scaling**: Apply a sublinear transformation to the raw term frequency. The most commonly used sublinear scaling is the logarithm. However, BM25 uses a specific formula which can be written as:\n",
            "\n",
            "   TF = (k + 1) * f / (f + k)\n",
            "\n",
            "   where:\n",
            "   - f is the raw frequency of the term in the document\n",
            "   - k is a tuning parameter that calibrates the term frequency scaling. The k parameter controls how quickly the importance of additional term frequency diminishes. When k is low, the curve is very steep, and additional occurrences of a term quickly become less important. When k is high, the curve is flatter, meaning additional term occurrences continue to have more influence on the score.\n",
            "\n",
            "3. **Saturation**: The TF value is bound to a maximum value, which is the parameter k + 1. This saturation effect ensures that terms cannot have an infinitely increasing influence on document relevance as their frequency increases.\n",
            "\n",
            "4. **Normalization**: The term frequency is often normalized by the length of the document or by applying a penalty to longer documents. This penalty can be controlled using another parameter (b), although it's not part of the TF calculation itself; it's factored into the overall BM25 score. The parameter b ranges from 0 to 1, where 0 means no length normalization and 1 means full normalization based on the average document length.\n",
            "\n",
            "The main goal of the BM25 TF component is to provide a balance where all terms contribute to the relevance score but prevent any single term from having an excessive impact. This is crucial because a perfect match on a common term could result in a very high score despite the matching document not being as relevant. The BM25 TF transformation, along with inverse document frequency (IDF) weighting, creates a more sophisticated and effective scoring mechanism for information retrieval tasks.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "Documents used: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: Inverse Document Frequency (IDF) weighting is important when computing similarity of context because it helps address two major problems encountered with simpler methods like Expected Overlap of Words in Context (EOWC). These problems are:\n",
            "\n",
            "1. **Overemphasis on Frequent Terms**: Without IDF, a similarity measure may favor the matching of one frequent term excessively over the matching of more unique terms. This could skew the similarity computation, as the match might be driven by common words rather than distinctive terms that really capture the context.\n",
            "\n",
            "2. **Equal Weighting of All Words**: In a simple vector space model or similar methods, common words such as \"the\" carry the same weight as more content-specific words like \"eats.\" This is not ideal because common words are not as informative and do not contribute significantly to the distinctiveness of the context.\n",
            "\n",
            "By applying IDF weighting, the model assigns more importance to rare terms in a collection of documents (or contexts in this case). Words that appear in many documents receive a lower IDF weight since their occurrence is not very informative. In contrast, rare words (those that appear in few documents) get a higher IDF weight, reflecting their importance in distinguishing between different documents or contexts.\n",
            "\n",
            "This is particularly useful for information retrieval and text mining, including paradigmatic relation discovery, because weighing the importance of terms based on their frequency across documents allows for a more nuanced similarity measure that focuses on meaningful word matches rather than being dominated by common words.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a multiple choice quiz based on the above output questions and answers"
      ],
      "metadata": {
        "id": "fR0NRk2vV5sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create summaries for each Q&A pair\n",
        "summaries = []\n",
        "for output in outputs:\n",
        "    summary = f\"Topic: {output['question']}\\nKey Points: {output['answer']}\\n\"\n",
        "    summaries.append(summary)\n",
        "\n",
        "# Step 2: Combine the summaries into a single prompt\n",
        "quiz_prompt = \"Based on the following summaries, create a multiple-choice quiz to test knowledge on these topics:\\n\\n\"\n",
        "quiz_prompt += \"\\n\".join(summaries)\n",
        "quiz_prompt += \"\\n\\nCreate a multiple-choice quiz:\"\n",
        "\n",
        "# Step 3: Use the query_gpt4 function to create the quiz\n",
        "quiz_questions = query_gpt4(quiz_prompt)\n",
        "\n",
        "# Print the generated quiz\n",
        "print(quiz_questions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54koGs_nVn_B",
        "outputId": "31bbf02b-9b04-4f4a-ce53-50bac25ee2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Quiz: Understanding Natural Language Processing and Related Concepts**\n",
            "\n",
            "1. What is the first step a computer must perform to understand a natural language sentence?\n",
            "   A. Semantic Analysis\n",
            "   B. Syntactical Parsing\n",
            "   C. Word Segmentation\n",
            "   D. Lexical Analysis\n",
            "\n",
            "2. What is lexical ambiguity in language processing?\n",
            "   A. When a pronoun refers to more than one possible antecedent.\n",
            "   B. When a word has multiple syntactic parses.\n",
            "   C. When a word has multiple meanings.\n",
            "   D. When a sentence carries an unstated presupposition.\n",
            "\n",
            "3. Which of the following is NOT a challenge for Natural Language Processing (NLP)?\n",
            "   A. The variety of expressions in human language.\n",
            "   B. The simplicity of grammatical structures.\n",
            "   C. Disambiguating pronouns and presuppositions.\n",
            "   D. Understanding the pragmatic aspects of sentences.\n",
            "\n",
            "4. What is the main idea behind the bag-of-words representation in text mining?\n",
            "   A. To preserve the grammatical structure of sentences.\n",
            "   B. To capture the exact order of words.\n",
            "   C. To consider words as an unordered collection and count their frequencies.\n",
            "   D. To create a parse tree that represents the sentence structure.\n",
            "\n",
            "5. Which of the following reasons contributes to the robustness of word-based representation over syntactic and semantic analysis?\n",
            "   A. It requires detailed grammar of the language.\n",
            "   B. It bases the analysis on the successful parsing of sentences into syntactic structures.\n",
            "   C. It is general, simple, and surprisingly effective for many applications.\n",
            "   D. It is dependent on common sense reasoning, which computers naturally possess.\n",
            "\n",
            "6. What defines a paradigmatic relation between two words?\n",
            "   A. Words that co-occur and form grammatical sequences.\n",
            "   B. Words that have opposite meanings.\n",
            "   C. Words that can substitute for each other in a sentence without affecting the overall meaning.\n",
            "   D. Words that follow each other in a specific syntactic order.\n",
            "\n",
            "7. When looking for syntagmatic relations in text, we should focus on:\n",
            "   A. Substituting words without altering the sentence meaning.\n",
            "   B. Frequency of a single word in different documents.\n",
            "   C. The combination of words that coherently occur in sequences.\n",
            "   D. The antecedents of given pronouns.\n",
            "\n",
            "8. Which method is used to discover paradigmatic relations from text?\n",
            "   A. Counting the frequency of each word.\n",
            "   B. Observing words that co-occur frequently in similar contexts.\n",
            "   C. Maintaining the exact order of words in a sentence.\n",
            "   D. Assigning numerical values to the grammatical rules of a language.\n",
            "\n",
            "9. What issue does Term Frequency Transformation address when computing similarity of context?\n",
            "   A. It eliminates the need for semantic analysis.\n",
            "   B. It penalizes high-frequency terms to prevent them from dominating the document relevance.\n",
            "   C. It helps introduce grammatical complexity into the evaluation.\n",
            "   D. It emphasizes the importance of common words like \"the\" or \"and.\"\n",
            "\n",
            "10. How does BM25 Term Frequency transformation specifically work?\n",
            "   A. It assigns higher scores to documents with longer text.\n",
            "   B. It uses sublinear scaling of term frequency to prevent single terms from overshadowing the relevance score.\n",
            "   C. It completely disregards common words in computing document relevance.\n",
            "   D. It favors the matching of frequent terms over unique terms.\n",
            "\n",
            "11. Why is Inverse Document Frequency (IDF) weighting used when computing the similarity of context?\n",
            "   A. To ensure that every word is treated equally regardless of frequency.\n",
            "   B. To decrease the weight of rare terms as they are less informative.\n",
            "   C. To assign more importance to rare terms and decrease weight for common terms.\n",
            "   D. To only focus on the grammatical correctness of each word's usage.\n",
            "\n",
            "**Answers:**\n",
            "1. C\n",
            "2. C\n",
            "3. B\n",
            "4. C\n",
            "5. C\n",
            "6. C\n",
            "7. C\n",
            "8. B\n",
            "9. B\n",
            "10. B\n",
            "11. C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT"
      ],
      "metadata": {
        "id": "PMkvrvvgf_Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to preprocess and lemmatize text\n",
        "def preprocess_and_lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Lowercasing and removing special characters\n",
        "    words = re.sub(r'\\W+', ' ', text.lower()).split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words if word not in stop_words])\n",
        "\n",
        "# Preprocess and store documents\n",
        "preprocessed_documents = [preprocess_and_lemmatize(doc) for doc in documents]\n",
        "\n",
        "# Create a TfidfVectorizer object with custom parameters\n",
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.5, min_df=2)\n",
        "\n",
        "# Convert the documents to a matrix of TF-IDF features\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "\n",
        "# Optional: Apply dimensionality reduction\n",
        "svd = TruncatedSVD(n_components=100)\n",
        "tfidf_matrix_reduced = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Enhanced function to find the top matching documents\n",
        "def find_top_documents(query, top_n=3):\n",
        "    query_processed = preprocess_and_lemmatize(query)\n",
        "    query_tfidf = vectorizer.transform([query_processed])\n",
        "    # If dimensionality reduction is applied\n",
        "    query_tfidf_reduced = svd.transform(query_tfidf)\n",
        "    cosine_similarities = cosine_similarity(query_tfidf_reduced, tfidf_matrix_reduced).flatten()\n",
        "    top_indices = np.argsort(cosine_similarities)[-top_n:][::-1]\n",
        "    return [(document_names[i], cosine_similarities[i]) for i in top_indices]\n",
        "\n",
        "# Example usage\n",
        "for query in queries:\n",
        "    top_documents = find_top_documents(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    for doc, score in top_documents:\n",
        "        print(f\"Matching document: {doc} with score {score:.4f}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT1IFIACgAvl",
        "outputId": "ebfe790f-e415-417a-8105-4a6568096c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What does a computer have to do in order to understand a natural language sentence?\n",
            "Matching document: 7_4_Natural_Language_Content_Analysis_Part_2.txt with score 0.9841\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.9804\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.3874\n",
            "\n",
            "\n",
            "Query: What is ambiguity?\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.0000\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.0000\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.0000\n",
            "\n",
            "\n",
            "Query: Why is natural language processing (NLP) difficult for computers?\n",
            "Matching document: 7_4_Natural_Language_Content_Analysis_Part_2.txt with score 0.9845\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.9725\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.3748\n",
            "\n",
            "\n",
            "Query: What is bag-of-words representation?\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9732\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9684\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9531\n",
            "\n",
            "\n",
            "Query: Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.9871\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.9690\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.4891\n",
            "\n",
            "\n",
            "Query: What is a paradigmatic relation?\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9965\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9845\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9679\n",
            "\n",
            "\n",
            "Query: What is a syntagmatic relation?\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9942\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9868\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9629\n",
            "\n",
            "\n",
            "Query: What is the general idea for discovering paradigmatic relations from text?\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9975\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9830\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9709\n",
            "\n",
            "\n",
            "Query: What is the general idea for discovering syntagmatic relations from text?\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9966\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9846\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9685\n",
            "\n",
            "\n",
            "Query: Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9924\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9898\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9548\n",
            "\n",
            "\n",
            "Query: How does BM25 Term Frequency transformation work?\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9955\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9705\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9343\n",
            "\n",
            "\n",
            "Query: Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9972\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9794\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9637\n",
            "\n",
            "\n",
            "Query: Part of speech tagging\n",
            "Matching document: 7_4_Natural_Language_Content_Analysis_Part_2.txt with score 0.9873\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.9690\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.3578\n",
            "\n",
            "\n",
            "Query: Syntactic analysis\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.9873\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.9736\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.3661\n",
            "\n",
            "\n",
            "Query: Semantic analysis\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.9669\n",
            "Matching document: 7_4_Natural_Language_Content_Analysis_Part_2.txt with score 0.8837\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.6272\n",
            "\n",
            "\n",
            "Query: Ambiguity\n",
            "Matching document: 7_6_Text_Representation_Part_2.txt with score 0.0000\n",
            "Matching document: 7_5_Text_Representation_Part_1.txt with score 0.0000\n",
            "Matching document: 7_3_Natural_Language_Content_Analysis_Part_1.txt with score 0.0000\n",
            "\n",
            "\n",
            "Query: Text representation, especially bag-of-words representation\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9271\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.8788\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.8662\n",
            "\n",
            "\n",
            "Query: Context of a word; context similarity\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9951\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9665\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9649\n",
            "\n",
            "\n",
            "Query: Paradigmatic relation\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9965\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9845\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9679\n",
            "\n",
            "\n",
            "Query: Syntagmatic relation\n",
            "Matching document: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt with score 0.9942\n",
            "Matching document: 7_7_Word_Association_Mining_And_Analysis.txt with score 0.9868\n",
            "Matching document: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt with score 0.9629\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backup_outputs = outputs"
      ],
      "metadata": {
        "id": "7bzC3TeZh5X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backup_outputs"
      ],
      "metadata": {
        "id": "o9fevmwth8oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This One is faster since it executes in parallel, but beware as it racks up the price quickly"
      ],
      "metadata": {
        "id": "T-ZiqeRCjDZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import concurrent.futures\n",
        "import openai\n",
        "\n",
        "def query_gpt4(prompt):\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=4096\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def ask_gpt_with_context(question, context_documents, document_names):\n",
        "    prompt = f\"Question: {question}\\n\\nContext:\\n\"\n",
        "    for doc, name in zip(context_documents, document_names):\n",
        "        prompt += f\"Document: {name}\\n{doc}\\n\\n\"\n",
        "\n",
        "    max_length = 128000\n",
        "    if len(prompt) > max_length:\n",
        "        prompt = prompt[-max_length:]\n",
        "\n",
        "    return query_gpt4(prompt)\n",
        "\n",
        "def process_question(query):\n",
        "    top_documents = find_top_documents(query)\n",
        "    top_doc_names = [doc_name for doc_name, _ in top_documents]\n",
        "    top_doc_contents = [documents[document_names.index(doc_name)] for doc_name in top_doc_names]\n",
        "    answer = ask_gpt_with_context(query, top_doc_contents, top_doc_names)\n",
        "\n",
        "    output = {\n",
        "        \"question\": query,\n",
        "        \"documents_used\": top_doc_names,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "    return output\n",
        "\n",
        "# Using ThreadPoolExecutor for parallel processing\n",
        "outputs2 = []\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Submit each query to the executor\n",
        "    future_to_query = {executor.submit(process_question, query): query for query in guiding_questions}\n",
        "\n",
        "    # As each future completes, process its result\n",
        "    for future in concurrent.futures.as_completed(future_to_query):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            output = future.result()\n",
        "            outputs2.append(output)\n",
        "            print(f\"Question: {query}\")\n",
        "            print(f\"Documents used: {', '.join(output['documents_used'])}\")\n",
        "            print(\"Answer:\", output['answer'])\n",
        "            print(\"-------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
        "        except Exception as exc:\n",
        "            print(f\"{query} generated an exception: {exc}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtbP6PfciOiR",
        "outputId": "25f12e4f-96ff-4e85-e06f-1ed839a64677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is a paradigmatic relation?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: A paradigmatic relation is a type of word association where two words are considered to be paradigmatically related if they can be substituted for each other without significantly altering the meaning of the sentence. This relates to them being in the same semantic or syntactic class.\n",
            "\n",
            "In summary:\n",
            "- **Paradigmatic Relation**: Two words A and B have a paradigmatic relation if they can be substituted for each other in a sentence, usually reflecting that they belong to the same semantic or syntactic class. This substitutability indicates that they occur in similar contexts within the text. For example, \"cat\" and \"dog\" can both be used in the sentence \"The ___ sleeps on the couch,\" indicating a paradigmatic relation between the words as they are both household animals that can perform the action of sleeping.\n",
            "\n",
            "The concept of paradigmatic relations can be applied not only to similar words but also to other sequences in language, including phrases or other syntactic structures. Recognizing these relations is beneficial for multiple natural language processing tasks, as it enhances the understanding of semantic structures and relationships in a language, thereby improving tasks like search retrieval, text summarization, grammar learning, and others.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is bag-of-words representation?\n",
            "Documents used: 7_7_Word_Association_Mining_And_Analysis.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: Bag-of-words is a representation used in natural language processing where a text (such as a sentence or document) is represented as an unordered collection of words, disregarding grammar and even word order but keeping multiplicity. The model involves two steps: (1) a vocabulary of known words is constructed, and (2) a measure of the presence of known words is taken. It's called a \"bag\" of words because any information about the order or structure of words is discarded; the model only tells us whether a known word occurs in the document, not where in the document it occurs.\n",
            "\n",
            "In the context of the provided text, the concept of a bag of words is referred to while discussing how to formally represent the context in which a word appears for the purpose of paradigmatic relation discovery. \n",
            "\n",
            "Here, the idea is to take the context surrounding a word, such as the word \"cat\" in various sentences, and then extract all other words occurring near \"cat\" to form a pseudo-document or context. This context representation can be captured by taking a window of words around \"cat,\" perhaps including the words before and after it. By looking at these contexts and computing the similarity between them, it's possible to identify paradigmatic relations—words that share similar contexts and can potentially be substituted for one another in sentences without losing meaning. This similarity is usually computed using vectors in a high-dimensional space where each dimension corresponds to a word in the vocabulary. \n",
            "\n",
            "In short, bag-of-words is a simple way to extract features from text to use in machine learning algorithms, which in the context of the provided lecture, is utilized to identify relationships between words based on their usage in similar contexts.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "Documents used: 7_5_Text_Representation_Part_1.txt, 7_6_Text_Representation_Part_2.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt\n",
            "Answer: Word-based representation is considered more robust than representations derived from syntactic and semantic analysis for several reasons:\n",
            "\n",
            "1. **Generality and Applicability**: The word-based approach is very general and applies to any natural language without requiring specialized knowledge of syntax or semantics specific to that language. This makes it easier to implement and use across various languages and contexts.\n",
            "\n",
            "2. **Basic Communication Units**: Words are the most basic units of human communication in natural language, and much of the meaning in text can be derived from the words themselves. Identifying and analyzing words can provide a lot of valuable insights, such as topic discovery, sentiment analysis, and word frequency analysis.\n",
            "\n",
            "3. **Lower Complexity**: Unlike syntactic and semantic analyses, word-based representation does not require deep parsing or understanding of the sentence structure and meaning. This reduces complexity and the potential for errors, as syntactic parsing and semantic interpretation can often result in mistakes due to language ambiguity.\n",
            "\n",
            "4. **Robustness to Errors**: The word-based method is less susceptible to errors caused by the nuances of language, such as ambiguous word senses or complicated syntactic structures. It focuses on the presence and frequency of words, which is a more stable feature of text data.\n",
            "\n",
            "5. **No Need for Extensive Manual Effort**: The process of word segmentation and analysis usually requires less manual effort, and in many cases, can be automated through simple algorithms that separate text based on delimiters like spaces or punctuation.\n",
            "\n",
            "6. **Effective for a Range of Applications**: Despite being a relatively shallow form of text representation, word-based approaches are surprisingly powerful and effective for many practical applications in text mining and natural language processing.\n",
            "\n",
            "While more advanced representations that include syntactic and semantic information can offer deeper insights and enable more sophisticated applications, the word-based approach provides a good balance of robustness, simplicity, and utility, making it widely applicable and reliable in a variety of text mining scenarios.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is ambiguity?\n",
            "Documents used: 7_6_Text_Representation_Part_2.txt, 7_5_Text_Representation_Part_1.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt\n",
            "Answer: Ambiguity in natural language refers to the existence of words or sentences that can be interpreted in multiple ways due to their context or structure. Ambiguity can manifest in different forms, as mentioned in the provided documents:\n",
            "\n",
            "1. **Word-level ambiguity**: A single word can have multiple meanings or serve different grammatical functions in different contexts. For instance, \"design\" can be a noun (the design of a product) or a verb (to design a product), and \"root\" can refer to the mathematical concept or a part of a plant.\n",
            "\n",
            "2. **Syntactic ambiguity**: A sentence or phrase can be structured in such a way that it has multiple possible interpretations. For example, \"natural language processing\" can be understood as the processing of natural language, or that the process of language processing is natural.\n",
            "   \n",
            "   Another classic example is, \"I saw the man with a telescope,\" which could mean that the observer used a telescope to see the man, or that the man had a telescope.\n",
            "\n",
            "3. **Anaphora resolution ambiguity**: Pronouns or other anaphoric expressions can refer to multiple antecedents, making it unclear which one is intended. In the sentence \"John persuaded Bill to buy a TV for himself,\" it's ambiguous whether \"himself\" refers to John or Bill.\n",
            "\n",
            "4. **Presupposition**: Some sentences carry implicit assumptions that the listener or reader is expected to understand. For example, \"He has quit smoking\" presupposes that the person used to smoke.\n",
            "\n",
            "Ambiguity poses significant challenges for natural language understanding and processing, as computers often lack the common sense, background knowledge, and contextual awareness needed to accurately interpret human language. This inherent ambiguity is a key reason why it's difficult for computers to achieve perfect comprehension and why natural language processing techniques often can't do everything correctly. Addressing ambiguity often requires sophisticated algorithms and, in many cases, human involvement to disambiguate and make decisions based on context.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What does a computer have to do in order to understand a natural language sentence?\n",
            "Documents used: 7_4_Natural_Language_Content_Analysis_Part_2.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt, 7_5_Text_Representation_Part_1.txt\n",
            "Answer: To understand a natural language sentence, a computer must go through several steps involving multiple layers of natural language processing (NLP):\n",
            "\n",
            "1. **Word Segmentation (Tokenization)**: The computer must first identify the individual words in a sentence. This involves separating the text into tokens, usually by recognizing spaces and punctuation. Word segmentation is relatively straightforward in languages like English but can be challenging in languages without clear word boundaries, such as Chinese.\n",
            "\n",
            "2. **Lexical Analysis (Part-of-Speech Tagging)**: After identifying the words, the computer needs to understand their syntactic categories. This means categorizing each word as a noun, verb, adjective, etc., which is known as part-of-speech tagging.\n",
            "\n",
            "3. **Syntactic Parsing**: The next step is to determine the grammatical structure of the sentence. This involves identifying phrases (e.g., noun phrases, prepositional phrases) and how they connect to convey meaning. Syntactic parsing results in a parse tree that outlines the sentence structure.\n",
            "\n",
            "4. **Semantic Analysis**: The computer must then map the syntactic structures to real-world concepts represented in its knowledge base. For example, it recognizes \"dog\" as an animal and \"boy\" as a person. It identifies entities, predicates, and their relationships. Semantic roles and predicate-argument structures help the computer understand who did what to whom.\n",
            "\n",
            "5. **Inference and Common Sense Reasoning**: To derive deeper meanings, the computer may have to infer additional information based on rules and its knowledge base. This could involve recognizing implied facts or drawing conclusions.\n",
            "\n",
            "6. **Pragmatic Analysis (Speech Act Analysis)**: Finally, the computer analyzes the purpose or intent behind the sentence, considering the context in which it was said. This could involve recognizing a sentence as a request, command, question, etc.\n",
            "\n",
            "These steps involve increasingly complex levels of analysis, from shallow (e.g., tokenization) to deep (e.g., pragmatic analysis). However, deeper NLP is challenging due to the nuances of natural language, ambiguities, and the need for extensive common sense knowledge. As a result, practical applications often rely on more robust and general shallow NLP techniques. These are generally statistical methods that can be applied to a wide variety of text data. For tasks requiring deeper understanding, human annotation and supervised machine learning techniques are used in combination with statistical methods to improve performance.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why is natural language processing (NLP) difficult for computers?\n",
            "Documents used: 7_4_Natural_Language_Content_Analysis_Part_2.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt, 7_5_Text_Representation_Part_1.txt\n",
            "Answer: Natural Language Processing (NLP) is difficult for computers for several reasons:\n",
            "\n",
            "1. **Part of Speech Ambiguity**: The same word can serve as different parts of speech in different contexts, and computers may struggle to tag them correctly. For example, \"off\" in \"he turned off the highway\" versus \"he turned off the fan\" carries slightly different implications.\n",
            "\n",
            "2. **Syntactic Ambiguity**: Sentences can be parsed in different ways, leading to different interpretations. For instance, \"A man saw a boy with a telescope\" could mean the man used a telescope to see the boy or the boy had a telescope.\n",
            "\n",
            "3. **Semantic Complexity**: Deep semantic understanding, such as defining the exact meaning of \"own\" in \"John owns a restaurant,\" is challenging because of the nuanced meanings words can have in different contexts.\n",
            "\n",
            "4. **Common Sense Knowledge**: NLP requires an understanding of common knowledge and inferences that humans inherently possess. Computers lack this common sense knowledge, making it hard for them to fully understand natural language.\n",
            "\n",
            "5. **Variety and Ambiguity in Language**: Natural language is full of ambiguities and relies on shared common sense knowledge. This includes homonyms, words with multiple meanings, syntactic structures with multiple interpretations (e.g., \"natural language processing\" can mean \"processing that is natural for language\" or \"the processing of natural language\"), anaphora resolution (determining what \"himself\" refers to in \"John persuaded Bill to buy a TV for himself\"), and presuppositions (understanding that \"He has quit smoking\" implies he smoked before).\n",
            "\n",
            "6. **Need for Inference**: Fully understanding natural language often requires inference, which remains a challenging area in artificial intelligence.\n",
            "\n",
            "7. **Pragmatic Analysis**: Determining the purpose behind a sentence or the intent of a speaker (speech act analysis), is also complex for computers.\n",
            "\n",
            "The difficulty of NLP is compounded by the fact that language is designed to be efficient for human communication, omitting explicit details that a computer might require for understanding. NLP tasks such as part-of-speech tagging, parsing, and entity recognition can be done with reasonably high accuracy, but they are not perfect and become less robust as the level of language understanding deepens.\n",
            "\n",
            "Due to these complexities, practical applications of NLP often use a combination of shallow, statistically-based techniques and deeper analysis that may require human annotation to guide computers through machine learning. This balance allows for robust general application while still benefiting from deeper understanding for specific important tasks.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is a syntagmatic relation?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: A syntagmatic relation is a linguistic relation between words that interact with each other within the same syntactic structure or sequence. It refers to how words are combined or sequenced to form phrases, clauses, and sentences. Specifically, two words have a syntagmatic relation if they can be combined with each other in a sentence; that is, these words are semantically related and can be put together to convey meaning. This contrasts with paradigmatic relations, where two words can be substituted for each other and belong to the same semantic or syntactic class.\n",
            "\n",
            "For example, in the syntagmatic pair \"cat sits,\" \"cat\" and \"sit\" are related because a cat can sit somewhere. In another syntagmatic pair, \"car drives,\" \"car\" and \"drive\" can be combined to express an action that a car performs. Contrast this with a paradigmatic relationship where \"cat\" and \"dog\" are related because they can often be swapped in a sentence without affecting the sentence's validity, as they both belong to the class of animals.\n",
            "\n",
            "In summary, syntagmatic relations deal with the co-occurrence or combination of words in the structure of a sentence, while paradigmatic relations deal with the substitutability of words within similar contexts.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: We want to do Inverse Document Frequency (IDF) weighting when computing similarity of context because treating all words equally in the context might not provide an accurate indication of similarity. Particularly, common words like 'the', 'is', 'and', etc., can occur across many different contexts with high frequency, but their presence doesn't necessarily signal a strong contextual relationship between words.\n",
            "\n",
            "IDF weighting addresses this by assigning a lower weight to words that are frequent across all contexts (documents) and a higher weight to rarer words. Thus, when computing the similarity between the contexts of two words (e.g., 'cat' and 'dog'), matching on more distinctive (rarer) words is given more importance than matching on common words. This approach aims to improve the robustness and precision of the similarity measure between words by emphasizing the importance of less common, more informative words that contribute to the unique aspects of each context.\n",
            "\n",
            "Therefore, applying IDF weighting helps to prevent frequent but less informative words from dominating the similarity measure and ensures that the resulting similarity more accurately reflects the true shared context between words, which is essential for tasks like paradigmatic relation discovery.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: Term Frequency Transformation in the context of computing similarity between word contexts is applied to address specific issues when representing the context of words and measuring their similarity. The reasons for applying Term Frequency Transformation are as follows:\n",
            "\n",
            "1. Mitigate the impact of very frequent terms: Without transformation, the raw term frequency can dominate the similarity measure. A high frequency of a single term can disproportionately influence the similarity score, potentially leading to the conclusion that two contexts are similar based solely on this one term's prevalence.\n",
            "\n",
            "2. Treat rare terms more significantly: By transforming term frequencies, the method can better account for the importance of rare terms, which are more informative. Raw term frequency treats every term equally, which can be misleading, as the presence of common words (e.g., 'the') is less indicative of context similarity than less frequent words that carry more meaning (e.g., 'eats').\n",
            "\n",
            "3. Normalize term weights: Term Frequency Transformation can help in ensuring that term weights contribute more evenly to the similarity measure, avoiding bias toward terms that may occur very frequently, but may not be as informative regarding the true relationship between words.\n",
            "\n",
            "4. Sub-linear scaling: Transformations (e.g., taking logarithms or using BM25 transformation) apply sub-linear scaling to term frequencies. This scaling reduces the impact of extreme values, making the representation more robust to variances in term frequency.\n",
            "\n",
            "5. Increase robustness and discriminative power: By modifying term frequencies appropriately through transformations like TF-IDF (Term Frequency-Inverse Document Frequency) or BM25, we can enhance the methodology's ability to discern truly paradigmatically related words from those that may just coincidentally appear in similar contexts. \n",
            "\n",
            "In summary, Term Frequency Transformation is utilized when computing similarity between word contexts to provide a more balanced and informative representation of contexts, reducing the influence of very frequent terms and giving more weight to rare, but informative terms. This results in a more accurate and reliable measure of context similarity when discovering word associations, such as paradigmatic relations.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is the general idea for discovering syntagmatic relations from text?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: The general idea for discovering syntagmatic relations from text is to explore the correlated occurrences of words within specific contexts. Syntagmatic relations exist between words that can be combined with each other in a sentence, meaning that these words are semantically related.\n",
            "\n",
            "To discover syntagmatic relations, one needs to count how frequently two words occur together within a specified context and compare this co-occurrence rate to their individual occurrence rates. When two words have a high co-occurrence rate but relatively low individual occurrence rates, they are assumed to have a syntagmatic relation since they tend to appear together rather than alone.\n",
            "\n",
            "The crux of discovering syntagmatic relations lies in capturing the correlation between the occurrences of words. By identifying words that typically co-occur (e.g., \"cat\" and \"sits\" because a cat can sit), we can make inferences about their syntagmatic relations. Conversely, words that do not frequently co-occur (like \"eats\" and \"text\") are less likely to have a meaningful syntagmatic relation.\n",
            "\n",
            "In summary, the key steps to discover syntagmatic relations are:\n",
            "\n",
            "1. Define the context within which to explore word associations (e.g., a sentence, a paragraph, or a document).\n",
            "2. Count the co-occurrence frequency of two words within the context.\n",
            "3. Consider the individual occurrence frequency of each word.\n",
            "4. Compare the co-occurrence rate to individual occurrence rates to determine if a syntagmatic relation exists.\n",
            "5. Identify pairs of words that have a high co-occurrence rate but low individual occurrence rates, as these are more likely to have a syntagmatic relation.\n",
            "\n",
            "This approach can also be integrated with the discovery of paradigmatic relations, where the focus is on computing the similarity of contexts of two words. Discovering words with high co-occurrences but relatively low individual occurrences in the text can help establish both syntagmatic and associated paradigmatic relations.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is the general idea for discovering paradigmatic relations from text?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: The general idea for discovering paradigmatic relations from text involves identifying words that share similar contexts within the text. The approach is based on the observation that paradigmatically related words tend to occur in similar positions and are often interchangeable without altering the meaning of the sentence significantly. \n",
            "\n",
            "Here are the key steps involved in discovering paradigmatic relations:\n",
            "\n",
            "1. **Representing Context:** The context of a word is represented as the set of surrounding words that occur with it in the text.\n",
            "\n",
            "2. **Context Views:** Various views of the context can be considered, such as the words immediately to the left (Left1 context), immediately to the right (Right1 context), or all words within a certain window size around the target word (Window context).\n",
            "\n",
            "3. **Bag of Words:** The context is often represented as a bag of words, forming a pseudo-document that can reflect the word's typical usage and the other words it is associated with.\n",
            "\n",
            "4. **Similarity Computation:** The similarity between the contexts of two words is computed, and a high similarity score suggests a paradigmatic relation between them.\n",
            "\n",
            "5. **Vector Space Modeling:** The context can be represented as vectors in a high-dimensional space, with each dimension corresponding to a word in the vocabulary.\n",
            "\n",
            "6. **Dot Product and Probability:** The dot product of two word vectors can be interpreted as the probability that two randomly picked words from their contexts are identical, providing a measure of context similarity.\n",
            "\n",
            "7. **Addressing Issues with Frequency and Common Words:** Approaches like Expected Overlap of Words in Context (EOWC) may favor matching one frequent term over more distinct terms and treat every word equally, which can be problematic. To overcome these issues, adjustments such as sublinear transformation of term frequency and weighting schemes like Inverse Document Frequency (IDF) are applied to balance the significance of frequent and rare terms.\n",
            "\n",
            "8. **Retrieval Heuristics:** Heuristics from information retrieval, such as TF-IDF and the BM25 weighting scheme, can help address the overemphasis on high-frequency terms and the equal treatment of common words.\n",
            "\n",
            "9. **Extraction of Relations:** By computing such adjusted similarity measures, potentially paradigmatically related word pairs can be extracted. Additionally, these methods can sometimes yield insights into syntagmatic relations as a by-product by highlighting words with strong associations in the context of the target word.\n",
            "\n",
            "The conceptual framework for discovering these word relations is grounded in representing context accurately and using similarity measures that reflect the likelihood of interchangeability or co-occurrence in similar contexts. These methods can be applied to a wide range of text analysis tasks and can be very useful for tasks such as query expansion in information retrieval, automatic thesaurus construction, and linguistic analysis.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: How does BM25 Term Frequency transformation work?\n",
            "Documents used: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: BM25 (Best Matching 25) is an algorithm used in information retrieval for ranking documents based on the query terms appearing in each document, regardless of their proximity within the document. It is part of the family of probabilistic information retrieval models and is derived from the probabilistic relevance framework. It extends the Term Frequency-Inverse Document Frequency (TF-IDF) model by incorporating two parameters, k1 and b, which control the non-linear term frequency normalization (saturation) and the length normalization, respectively.\n",
            "\n",
            "The term frequency transformation within BM25 is designed to address the problem of term frequency saturation. In traditional models like TF-IDF, term frequency can lead to the dominance of terms with higher frequencies in documents. This can skew the model's performance towards term frequency rather than the discriminative power of terms.\n",
            "\n",
            "The BM25 term frequency transformation works as follows:\n",
            "\n",
            "1. **Sublinear Term Frequency Scaling**: Instead of using the raw frequency count (f) of a term in a document, BM25 applies a transformation that grows sublinearly with f. Given a term frequency f, the transformed frequency (TF) is calculated using the formula:\n",
            "\n",
            "\\[\n",
            "\\text{TF} = \\frac{(k + 1) \\cdot f}{f + k \\cdot \\left(1 - b + b \\cdot \\frac{l_d}{l_{avg}}\\right)}\n",
            "\\]\n",
            "\n",
            "Here:\n",
            "- \\(k\\) is the saturation parameter, usually set to a value such as 1.2 or 2.0.\n",
            "- \\(b\\) is the length normalization parameter, typically set to 0.75.\n",
            "- \\(l_d\\) is the length of the document currently being scored.\n",
            "- \\(l_{avg}\\) is the average document length in the corpus.\n",
            "- This formula ensures that as the term frequency increases, the additional contribution of each occurrence diminishes. There is a diminishing return to the increasing frequency: beyond a certain point, increasing the frequency of the term does not significantly impact the score.\n",
            "\n",
            "2. **Length Normalization**: The term b is used to control the influence of document length on the term frequency. Smaller documents would naturally have fewer occurrences of a term, so without this normalization, longer documents could be at an advantage. The \\(1 - b + b \\cdot \\frac{l_d}{l_{avg}}\\) part adjusts the saturation based on the document's length relative to the average document length.\n",
            "\n",
            "In summary, BM25's term frequency transformation normalizes both term frequency and document length to provide a more balanced representation of term importance than simple raw frequencies would. This approach reduces the problem of overemphasis on term frequency and moves towards a balance between term frequency and term rarity (distinctiveness), thus providing more effective document ranking for information retrieval tasks.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "CPU times: user 1.66 s, sys: 106 ms, total: 1.76 s\n",
            "Wall time: 44.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faster and cheaper with 3.5k turbo"
      ],
      "metadata": {
        "id": "UT1zQP2nk0X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import concurrent.futures\n",
        "import openai\n",
        "\n",
        "def query_gpt4(prompt):\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=4096\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def ask_gpt_with_context(question, context_documents, document_names):\n",
        "    prompt = f\"Question: {question}\\n\\nContext:\\n\"\n",
        "    for doc, name in zip(context_documents, document_names):\n",
        "        prompt += f\"Document: {name}\\n{doc}\\n\\n\"\n",
        "\n",
        "    max_length = 16000\n",
        "    if len(prompt) > max_length:\n",
        "        prompt = prompt[-max_length:]\n",
        "\n",
        "    return query_gpt4(prompt)\n",
        "\n",
        "def process_question(query):\n",
        "    top_documents = find_top_documents(query)\n",
        "    top_doc_names = [doc_name for doc_name, _ in top_documents]\n",
        "    top_doc_contents = [documents[document_names.index(doc_name)] for doc_name in top_doc_names]\n",
        "    answer = ask_gpt_with_context(query, top_doc_contents, top_doc_names)\n",
        "\n",
        "    output = {\n",
        "        \"question\": query,\n",
        "        \"documents_used\": top_doc_names,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "\n",
        "    return output\n",
        "\n",
        "# Using ThreadPoolExecutor for parallel processing\n",
        "outputs3 = []\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Submit each query to the executor\n",
        "    future_to_query = {executor.submit(process_question, query): query for query in guiding_questions}\n",
        "\n",
        "    # As each future completes, process its result\n",
        "    for future in concurrent.futures.as_completed(future_to_query):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            output = future.result()\n",
        "            outputs3.append(output)\n",
        "            print(f\"Question: {query}\")\n",
        "            print(f\"Documents used: {', '.join(output['documents_used'])}\")\n",
        "            print(\"Answer:\", output['answer'])\n",
        "            print(\"-------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
        "        except Exception as exc:\n",
        "            print(f\"{query} generated an exception: {exc}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSe5Q2DRkguy",
        "outputId": "79504056-a2ce-4678-8116-e07a3bc133f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is ambiguity?\n",
            "Documents used: 7_6_Text_Representation_Part_2.txt, 7_5_Text_Representation_Part_1.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt\n",
            "Answer: Ambiguity is the presence of multiple possible meanings or interpretations within a given context. In the context of natural language processing and text mining, ambiguity refers to the difficulty in accurately interpreting and understanding language due to the multiple possible meanings of words, sentences, and phrases. The presence of ambiguity makes it challenging for computers to process and analyze text data accurately.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is bag-of-words representation?\n",
            "Documents used: 7_7_Word_Association_Mining_And_Analysis.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: The bag-of-words representation is a method of representing text data where each document is represented as a vector of word counts. In the context of paradigmatcal relation discovery, the bag-of-words representation is used to collect the context of a candidate word to form a pseudo document. The context is typically represented as a bag of words, and then the similarity of the corresponding context documents of two candidate words is computed to determine their paradigmatic relationship. This representation allows for the computation of similarity functions and the discovery of word associations.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why is this word-based representation more robust than representations derived from syntactic and semantic analysis of text?\n",
            "Documents used: 7_5_Text_Representation_Part_1.txt, 7_6_Text_Representation_Part_2.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt\n",
            "Answer: The word-based representation is more robust than representations derived from syntactic and semantic analysis of text because it is more general and applicable to any natural language. It is also relatively more straightforward and less prone to errors, as it does not require as much manual effort. Additionally, word-based representation and the techniques enabled by such representation can be combined with many other sophisticated approaches, allowing for a richer and more powerful analysis. This level of representation may not provide a full understanding of the semantics of the text, but it is still very effective for many applications and is widely used in various text mining applications.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why is natural language processing (NLP) difficult for computers?\n",
            "Documents used: 7_4_Natural_Language_Content_Analysis_Part_2.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt, 7_5_Text_Representation_Part_1.txt\n",
            "Answer: Natural language processing (NLP) is difficult for computers because human communication is designed to be efficient, which means it relies on common sense knowledge and includes ambiguities that we can easily disambiguate. However, computers lack this common sense knowledge, which makes it difficult for them to understand natural language. Additionally, there are multiple challenges in NLP such as word-level ambiguity, syntactic ambiguity, anaphora resolution, and presupposition, which make it hard for computers to precisely understand and analyze natural language. As a result, even the state-of-the-art NLP techniques cannot fully understand and process natural language, leading to the need for a combination of shallow and deep analysis along with human input for accuracy and effectiveness.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is a paradigmatic relation?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: A paradigmatic relation is a word association in which two words are considered related if they share a similar context, meaning they occur in similar positions in text. This is a fundamental relation in language, and it can be discovered by computing the similarity of the contexts of two words. This can be achieved using various methods, including representing the context as a word vector and defining the similarity function using approaches like expected overlap of words in context, sublinear transformation of term frequency, and inverse document frequency (IDF) term weighting. These methods help address the challenges of overemphasizing frequent terms and treating all words equally, allowing for the discovery of paradigmatic relations. Additionally, syntagmatic relations can also be discovered using similar techniques as a by-product of discovering paradigmatic relations.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What does a computer have to do in order to understand a natural language sentence?\n",
            "Documents used: 7_4_Natural_Language_Content_Analysis_Part_2.txt, 7_3_Natural_Language_Content_Analysis_Part_1.txt, 7_5_Text_Representation_Part_1.txt\n",
            "Answer: In order to understand a natural language sentence, a computer has to go through several steps. First, the computer needs to segment the words in the sentence and then identify the syntactical categories of these words, known as part-of-speech tagging. After that, the computer needs to figure out the relationship between the words, creating a syntactical analysis or parsing of the natural language sentence. Additionally, the computer needs to map these phrases and structures into real-world entities and understand the meaning of the sentence through formal representation of the semantics. Finally, the computer may need to make inferences and analyze the purpose of the sentence through speech act analysis or pragmatic analysis. However, even with these steps, it is difficult for a computer to fully understand natural language due to factors like word-level and syntactic ambiguity, anaphora resolution, and presupposition. The current state of the art in natural language processing techniques cannot perfectly handle all these challenges, making precise understanding of natural language difficult for computers. Therefore, a combination of statistical and shallow analysis methods along with human input is often used to analyze text data more effectively.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is a syntagmatic relation?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: A syntagmatic relation is a relation between two words that can be combined with each other in a sentence, indicating that the two words are semantically related and tend to co-occur together in a context. This relation captures the correlation between the occurrences of the two words, and words with high co-occurrences but relatively low individual occurrences are considered to have syntagmatic relations. The syntagmatic relation is a fundamental word association that can be applied to any items in a language, and it can be used to capture the basic relations between units in arbitrary sequences. It is closely related to the paradigmatic relation, and the two relations can be discovered in a joint manner by leveraging word associations.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why do we want to do Term Frequency Transformation when computing similarity of context?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: The term frequency transformation is important when computing the similarity of context because it allows us to represent the context of words in a way that emphasizes important words and avoids favoring matching one frequent term over matching more distinct terms. By transforming the raw frequency count of a term into a weight that reflects its importance in the context, we can address the problem of overemphasizing a frequency term and treat every word equally. This helps in computing the similarity of context and discovering paradigmatic relations effectively. Additionally, term frequency transformation also allows us to penalize popular terms and reward matching a rare word, which is essential for discovering syntagmatic relations. Overall, by using term frequency transformation, we can improve the accuracy of word association mining and analysis and effectively capture both paradigmatic and syntagmatic relations in text data.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is the general idea for discovering syntagmatic relations from text?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: The general idea for discovering syntagmatic relations from text involves collecting the context of a candidate word to form a pseudo document, typically represented as a bag of words. The next step is to compute the similarity of the corresponding context documents of two candidate words. Subsequently, the highly similar word pairs can be treated as having syntagmatic relations. These are the words that tend to co-occur together in similar contexts, indicating a semantic relationship.\n",
            "\n",
            "In summary, the main idea for discovering syntagmatic relations is to measure the correlation of word occurrences in context and use this information to identify words that are strongly associated with each other. This can be accomplished alongside the discovery of paradigmatic relations, as the two types of relations are closely related and can be discovered in a joint manner by leveraging such word associations. Different approaches, including using text retrieval models such as BM25 and IDF weighting, are effective for discovering both syntagmatic and paradigmatic relations from text data.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: What is the general idea for discovering paradigmatic relations from text?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt\n",
            "Answer: The general idea for discovering paradigmatic relations from text is to collect the context of a candidate word to form a pseudo document, represented as a bag of words, and then compute the similarity of the corresponding context documents of two candidate words. High context similarity indicates paradigmatic relations between the words. Different approaches, such as using text retrieval models like BM25 and IDF weighting, are used to design effective similarity functions for computing paradigmatic relations. Additionally, the discovery of paradigmatic relations can also lead to the discovery of syntagmatic relations as a by-product.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: How does BM25 Term Frequency transformation work?\n",
            "Documents used: 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: BM25 Term Frequency transformation works by using a sublinear transformation of term frequency to address the problem of overemphasizing frequent terms and penalizing popular terms. This transformation helps to prevent dominant terms from overshadowing the importance of other terms in the context. The transformation function entails the use of a parameter, k, and another parameter, b, to control the upper bound and the extent of linear transformation. This method has been shown to be very effective for text retrieval and makes sense in the context of paradigmatic relation discovery.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Question: Why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "Documents used: 7_8_Paradigmatic_Relation_Discovery_Part_1.txt, 7_9_Paradigmatic_Relation_Discovery_Part_2.txt, 7_7_Word_Association_Mining_And_Analysis.txt\n",
            "Answer: In the context of discovering word associations, why do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?\n",
            "\n",
            "IDF weighting is important when computing similarity of context because it helps address two major problems in the Expected Overlap of Words in Context (EOWC) method. First, it addresses the problem of favoring one frequent term over matching more distinct terms by penalizing high frequency terms and putting more weight on rare terms. Second, it addresses the problem of treating every word equally by rewarding rare terms and penalizing common words. This allows for a more accurate and effective measure of similarity between word contexts, which is crucial in discovering word associations. Additionally, IDF weighting helps in discovering both paradigmatic and syntagmatic relations, making it an important component in the overall process of word association mining and analysis.\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "CPU times: user 1.93 s, sys: 83.2 ms, total: 2.02 s\n",
            "Wall time: 13.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Might want to look into fine tuning [OpenAI - Fine Tuning Models](https://platform.openai.com/docs/guides/fine-tuning)"
      ],
      "metadata": {
        "id": "DLlaTIJd2kY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import openai\n",
        "import re\n",
        "from pdfplumber import open as open_pdf\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open_pdf(pdf_path) as pdf:\n",
        "        text = \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
        "    return text\n",
        "\n",
        "def process_lecture_transcripts(folder_path):\n",
        "    lectures = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
        "                lectures.append(file.read())\n",
        "    return lectures\n",
        "\n",
        "def split_textbook_into_sections(textbook_content, section_pattern):\n",
        "    sections = re.split(section_pattern, textbook_content)\n",
        "    return [section.strip() for section in sections if section.strip()]\n",
        "\n",
        "def create_jsonl_data_lectures(lectures):\n",
        "    \"\"\"\n",
        "    Create JSONL data from lecture transcripts.\n",
        "\n",
        "    :param lectures: List of lecture transcripts.\n",
        "    :return: List of JSON objects for each lecture.\n",
        "    \"\"\"\n",
        "    jsonl_data_lectures = []\n",
        "    for lecture in lectures:\n",
        "        # Assume each lecture has a question or topic and a corresponding detailed explanation\n",
        "        # Split them into 'question' and 'answer'\n",
        "        # This is a placeholder; actual implementation will depend on the lecture format\n",
        "        question, answer = split_lecture_into_qa(lecture)  # Define this function based on your lecture format\n",
        "        jsonl_data_lectures.append({\n",
        "            \"prompt\": question,\n",
        "            \"completion\": answer\n",
        "        })\n",
        "    return jsonl_data_lectures\n",
        "\n",
        "def create_jsonl_data_textbook(textbook_sections):\n",
        "    \"\"\"\n",
        "    Create JSONL data from textbook sections.\n",
        "\n",
        "    :param textbook_sections: List of sections from the textbook.\n",
        "    :return: List of JSON objects for each section.\n",
        "    \"\"\"\n",
        "    jsonl_data_textbook = []\n",
        "    for section in textbook_sections:\n",
        "        # Assume each section has a heading or topic and detailed content\n",
        "        # Split them into 'topic' and 'content'\n",
        "        # This is a placeholder; actual implementation will depend on the textbook format\n",
        "        topic, content = split_section_into_topic_content(section)  # Define this function based on your textbook format\n",
        "        jsonl_data_textbook.append({\n",
        "            \"prompt\": topic,\n",
        "            \"completion\": content\n",
        "        })\n",
        "    return jsonl_data_textbook\n",
        "\n",
        "\n",
        "# Extract text from textbook PDF\n",
        "textbook_content = extract_text_from_pdf('/content/textbook.pdf')\n",
        "\n",
        "# Process lecture transcripts\n",
        "lectures = process_lecture_transcripts('/content/lecture_transcripts/')\n",
        "\n",
        "# Split textbook into sections\n",
        "section_pattern = r\"\\n(?:Chapter|Section) \\d+:\"  # Adjust the pattern based on your textbook\n",
        "textbook_sections = split_textbook_into_sections(textbook_content, section_pattern)\n",
        "\n",
        "# Create JSONL data for fine-tuning from lectures and textbook sections\n",
        "jsonl_data_lectures = create_jsonl_data_lectures(lectures)\n",
        "jsonl_data_textbook = create_jsonl_data_textbook(textbook_sections)\n",
        "\n",
        "# Combine both datasets\n",
        "jsonl_data = jsonl_data_lectures + jsonl_data_textbook\n",
        "\n",
        "# Save your data as a JSONL file\n",
        "with open('training_data.jsonl', 'w') as f:\n",
        "    for item in jsonl_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "# Upload the data using OpenAI API\n",
        "openai.File.create(file=open(\"training_data.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
        "\n",
        "# Create a fine-tuning job\n",
        "# Replace 'file-abc123' with your file ID obtained after uploading\n",
        "response = openai.FineTuning.create(\n",
        "    training_file=\"file-abc123\",\n",
        "    model=\"gpt-3.5-turbo\"\n",
        ")\n",
        "\n",
        "# The model name will be in the response\n",
        "fine_tuned_model_name = response['fine_tuned_model']\n",
        "print(\"Fine-tuned model name:\", fine_tuned_model_name)\n",
        "\n",
        "# Check response for details about the fine-tuning job\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "yGTygGz1km42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "  model=fine_tuned_model_name,  # Use the name of your fine-tuned model\n",
        "  prompt=\"Your prompt here\",\n",
        "  max_tokens=50\n",
        ")\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "EYoElX155dfz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}